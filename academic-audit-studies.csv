"","source","audit_type_condensed","authors","title","urls","journal_series_institution","year","keywords","abstract","source_detailed","audit_type","paper_id"
"1","ACL","Data Audit","Emiel van Miltenburg","How Do Image Description Systems Describe People? A Targeted Assessment of System Competence in the PEOPLE-domain","https://aclanthology.org/2020.lantern-1.4/","ACL/EMNLP",2020,NA,"Evaluations of image description systems are typically domain-general: generated descriptions for the held-out test images are either compared to a set of reference descriptions (using automated metrics), or rated by human judges on one or more Likert scales (for fluency, overall quality, and other quality criteria). While useful, these evaluations do not tell us anything about the kinds of image descriptions that systems are able to produce. Or, phrased differently, these evaluations do not tell us anything about the cognitive capabilities of image description systems. This paper proposes a different kind of assessment, that is able to quantify the extent to which these systems are able to describe humans. This assessment is based on a manual characterisation (a context-free grammar) of English entity labels in the PEOPLE domain, to determine the range of possible outputs. We examined 9 systems to see what kinds of labels they actually use. We found that these systems only use a small subset of at most 13 different kinds of modifiers (e.g. tall and short modify HEIGHT, sad and happy modify MOOD), but 27 kinds of modifiers are never used. Future research could study these semantic dimensions in more detail.","[URL Match | Institution Match] ACL","Data Audit",386
"2","ACL","Data Audit","Alexandra Luccioni, Joseph Viviano","What’s in the Box? An Analysis of Undesirable Content in the Common Crawl Corpus","https://aclanthology.org/2021.acl-short.24/","ACL/EMNLP",2021,NA,"Whereas much of the success of the current generation of neural language models has been driven by increasingly large training corpora, relatively little research has been dedicated to analyzing these massive sources of textual data. In this exploratory analysis, we delve deeper into the Common Crawl, a colossal web corpus that is extensively used for training language models. We find that it contains a significant amount of undesirable content, including hate speech and sexually explicit content, even after filtering procedures. We discuss the potential impacts of this content on language models and conclude with future research directions and a more mindful approach to corpus collection and analysis.","[URL Match | Institution Match] ACL","Data Audit",377
"3","ACL","Data Audit","Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, Hanna Wallach","Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets","https://aclanthology.org/2021.acl-long.81/","ACL/EMNLP",2021,NA,"Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system’s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens—originating from the social sciences—to inventory a range of pitfalls that threaten these benchmarks’ validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.","[URL Match | Institution Match] ACL","Data Audit",385
"4","ACL","Case Study","Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, Adina Williams","On the Machine Learning of Ethical Judgments from Natural Language","https://aclanthology.org/2022.naacl-main.56.pdf","ACL/EMNLP",2022,NA,"Ethics is one of the longest standing intellectual
endeavors of humanity. In recent years, the
fields of AI and NLP have attempted to address
ethical issues of harmful outcomes in machine
learning systems that are made to interface
with humans. One recent approach in this vein
is the construction of NLP morality models that
can take in arbitrary text and output a moral
judgment about the situation described. In this
work, we offer a critique of such NLP methods
for automating ethical decision-making.
Through an audit of recent work on computa-
tional approaches for predicting morality, we
examine the broader issues that arise from such
efforts. We conclude with a discussion of how
machine ethics could usefully proceed in NLP,
by focusing on current and near-future uses
of technology, in a way that centers around
transparency, democratic values, and allows
for straightforward accountability.","[URL Match | Institution Match] ACL","Case Study",336
"5","ACL","Case Study","Fraser, Kiritchenko, Balkir","Does Moral Code Have a Moral Code?","https://aclanthology.org/2022.trustnlp-1.3.pdf","ACL/EMNLP",2022,NA,"In an effort to guarantee that machine learning
model outputs conform with human moral val-
ues, recent work has begun exploring the pos-
sibility of explicitly training models to learn
the difference between right and wrong. This is
typically done in a bottom-up fashion, by expos-
ing the model to different scenarios, annotated
with human moral judgements. One question,
however, is whether the trained models actually
learn any consistent, higher-level ethical princi-
ples from these datasets – and if so, what? Here,
we probe the Allen AI Delphi model with a set
of standardized morality questionnaires, and
find that, despite some inconsistencies, Delphi
tends to mirror the moral principles associated
with the demographic groups involved in the
annotation process. We question whether this
is desirable and discuss how we might move
forward with this knowledge.","[URL Match | Institution Match] ACL","Case Study",337
"6","ACL","Data Audit","I. Caswell, J. Kreutzer, L. Wang, A. Wahab, D. van Esch, N. Ulzii-Orshikh, A. Tapo, N. Subramani, A. Sokolov, C. Sikasote et al.,","Quality at a glance: An audit of web-crawled multilingual datasets","https://aclanthology.org/2022.tacl-1.4/","ACL/EMNLP",2022,NA,"With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, Web-mined text datasets covering hundreds of languages. We manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource corpora have systematic issues: At least 15 corpora have no usable text, and a significant fraction contains less than 50% sentences of acceptable quality. In addition, many are mislabeled or use nonstandard/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and improve multilingual corpora and discuss potential risks that come with low-quality data releases.","[URL Match | Institution Match] ACL","Data Audit",384
"7","ACMDL - Accountability","Ecosystem","Abdul A,Vermeulen J,Wang D,Lim BY,Kankanhalli M","Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda","https://doi.org/10.1145/3173574.3174156;http://dx.doi.org/10.1145/3173574.3174156","Association for Computing Machinery",2018,"intelligibility, interpretable machine learning, explainable artificial intelli-gence, explanations","Advances in artificial intelligence, sensors and big data management have far-reaching societal impacts. As these systems augment our everyday lives, it becomes increasing-ly important for people to understand them and remain in control. We investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of 289 core papers on explanations and explaina-ble systems, as well as 12,412 citing papers. Using topic modeling, co-occurrence and network analysis, we mapped the research space from diverse domains, such as algorith-mic accountability, interpretable machine learning, context-awareness, cognitive psychology, and software learnability. We reveal fading and burgeoning trends in explainable systems, and identify domains that are closely connected or mostly isolated. The time is ripe for the HCI community to ensure that the powerful new autonomous systems have intelligible interfaces built-in. From our results, we propose several implications and directions for future research to-wards this goal.","ACMDL - Accountability","Ecosystem",22
"8","ACMDL - Accountability","Ecosystem","Veale M,Van Kleek M,Binns R","Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making","https://doi.org/10.1145/3173574.3174014;http://dx.doi.org/10.1145/3173574.3174014","Association for Computing Machinery",2018,"public administration, predictive policing, decision-support, algorithmic bias, algorithmic accountability","Calls for heightened consideration of fairness and accountability in algorithmically-informed public decisions-like taxation, justice, and child protection-are now commonplace. How might designers support such human values? We interviewed 27 public sector machine learning practitioners across 5 OECD countries regarding challenges understanding and imbuing public values into their work. The results suggest a disconnect between organisational and institutional realities, constraints and needs, and those addressed by current research into usable, transparent and 'discrimination-aware' machine learning-absences likely to undermine practical initiatives unless addressed. We see design opportunities in this disconnect, such as in supporting the tracking of concept drift in secondary data sources, and in building usable transparency tools to identify risks and incorporate domain knowledge, aimed both at managers and at the 'street-level bureaucrats' on the frontlines of public service. We conclude by outlining ethical challenges and future directions for collaboration in these high-stakes applications.","ACMDL - Accountability","Ecosystem",23
"9","ACMDL - Accountability","Case Study","Bahri L,Carminati B,Ferrari E,Bianco A","Enhanced Audit Strategies for Collaborative and Accountable Data Sharing in Social Networks","https://doi.org/10.1145/3134439;http://dx.doi.org/10.1145/3134439","Association for Computing Machinery",2018,"decentralized social networks, Apriori access control, accountability","Data sharing and access control management is one of the issues still hindering the development of decentralized online social networks (DOSNs), which are now gaining more research attention with the recent developments in P2P computing, such as the secure public ledger–based protocols (Blockchains) for monetary systems. In a previous work, we proposed an initial audit–based model for access control in DOSNs. In this article, we focus on enhancing the audit strategies and the privacy issues emerging from records kept for audit purposes. We propose enhanced audit and collaboration strategies, for which experimental results, on a real online social network graph with simulated sharing behavior, show an improvement in the detection rate of bad behavior of more than 50% compared to the basic model. We also provide an analysis of the related privacy issues and discuss possible privacy-preserving alternatives.","ACMDL - Accountability","Case Study",57
"10","ACMDL - Accountability","Data Audit","Chowdhury N","An IoT and Blockchain-Based Approach for Ensuring Transparency and Accountability in Regulatory Compliance","https://doi.org/10.1145/3341162.3349320;http://dx.doi.org/10.1145/3341162.3349320","Association for Computing Machinery",2019,"trust, regulatory compliance, blockchain, internet of things","Regulatory compliance is an essential exercise in the modern societies confirming safety and prevention of harm to consumers. Despite many efforts from international and national quality control authorities, transparency and accountability in regulatory compliance remain a challenging technical-legal problem sitting atop a heavy reliance on trust. This paper presents a theoretical model of regulatory compliance aiming at improving accountability for systems and data audit and introduces a higher degree of transparency in management and quality control. It explores the technical aspects of two emerging technologies the Internet of Things (IoT) and Blockchain, and using a common use-case in practice shows how to better align these technologies with legal concerns and trust in regulatory compliance.","ACMDL - Accountability","Data Audit",10
"11","ACMDL - Accountability","Ecosystem","Spiliotopoulos D,Margaris D,Vassilakis C","Citizen Engagement for Transparent and Accountable Policy Modelling","https://doi.org/10.1145/3297662.3365813;http://dx.doi.org/10.1145/3297662.3365813","Association for Computing Machinery",2019,"Accountability, e-Government, Transparency, Policy Modelling, Natural Language Processing, Mobile Public Services, Legislation, Citizen Engagement","This work presents a platform for linked legislative data to engage citizens in transparent and effective democracies. With a focus on scaling up participatory approaches from local to national level, the approach extends well established and open source tools and technologies, to build mobile monitoring and analysis tools that increase transparency of law-making and implementation to citizens. This is achieved by combining open data and open services with user and citizen generated content, in order to address citizen's needs in the context of open government. Data and feeds from trusted sources are interconnected with new and re-purposed data feeds generated by users via the social web to form a meaningful, searchable, customizable, reusable and open data-focused personalised mobile public service approach. The framework exploits the social aspects of open data, as well as the training of users, citizens and public servants to be able to understand and demand useful public open data, as well as facilitate the opening of more data.","ACMDL - Accountability","Ecosystem",14
"12","ACMDL - Accountability","Ecosystem","Brown A,Chouldechova A,Putnam-Hornstein E,Tobin A,Vaithianathan R","Toward Algorithmic Accountability in Public Services: A Qualitative Study of Affected Community Perspectives on Algorithmic Decision-Making in Child Welfare Services","https://doi.org/10.1145/3290605.3300271;http://dx.doi.org/10.1145/3290605.3300271","Association for Computing Machinery",2019,"algorithmic bias, algorithmic accountability, participatory design, decision-support, child welfare services, automated decision systems","Algorithmic decision-making systems are increasingly being adopted by government public service agencies. Researchers, policy experts, and civil rights groups have all voiced concerns that such systems are being deployed without adequate consideration of potential harms, disparate impacts, and public accountability practices. Yet little is known about the concerns of those most likely to be affected by these systems. We report on workshops conducted to learn about the concerns of affected communities in the context of child welfare services. The workshops involved 83 study participants including families involved in the child welfare system, employees of child welfare agencies, and service providers. Our findings indicate that general distrust in the existing system contributes significantly to low comfort in algorithmic decision-making. We identify strategies for improving comfort through greater transparency and improved communication strategies. We discuss the implications of our study for accountable algorithm design for child welfare applications.","ACMDL - Accountability","Ecosystem",27
"13","ACMDL - Accountability","Meta-Commentary","Ustun B,Spangher A,Liu Y","Actionable Recourse in Linear Classification","https://doi.org/10.1145/3287560.3287566;http://dx.doi.org/10.1145/3287560.3287566","Association for Computing Machinery",2019,"accountability, audit, classification, integer programming, recourse, credit scoring","Classification models are often used to make decisions that affect humans: whether to approve a loan application, extend a job offer, or provide insurance. In such applications, individuals should have the ability to change the decision of the model. When a person is denied a loan by a credit scoring model, for example, they should be able to change the input variables of the model in a way that will guarantee approval. Otherwise, this person will be denied the loan so long as the model is deployed, and -- more importantly --will lack agency over a decision that affects their livelihood.In this paper, we propose to evaluate a linear classification model in terms of recourse, which we define as the ability of a person to change the decision of the model through actionable input variables (e.g., income vs. age or marital status). We present an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a target population; and (ii) generate a list of actionable changes for a person to obtain a desired outcome. We discuss how our tools can inform different stakeholders by using them to audit recourse for credit scoring models built with real-world datasets. Our results illustrate how recourse can be significantly affected by common modeling practices, and motivate the need to evaluate recourse in algorithmic decision-making.","ACMDL - Accountability","Tool",50
"14","ACMDL - Accountability","Case Study","Grasso I,Russell D,Matthews A,Matthews J,Record NR","Applying Algorithmic Accountability Frameworks with Domain-Specific Codes of Ethics: A Case Study in Ecosystem Forecasting for Shellfish Toxicity in the Gulf of Maine","https://doi.org/10.1145/3412815.3416897;http://dx.doi.org/10.1145/3412815.3416897","Association for Computing Machinery",2020,"ecology, ethics, algorithmic accountability, forecasting","Ecological forecasts are used to inform decisions that can havesignificant impacts on the lives of individuals and on the healthof ecosystems. These forecasts, or models, embody the ethics oftheir creators as well as many seemingly arbitrary implementationchoices made along the way. They can contain implementationerrors as well as reflect patterns of bias learned when ingestingdatasets derived from past biased decision making. Principles andframeworks for algorithmic accountability allow a wide range ofstakeholders to place the results of models and software systemsinto context. We demonstrate how the combination of algorithmicaccountability frameworks and domain-specific codes of ethics helpanswer calls to uphold fairness and human values, specifically indomains that utilize machine learning algorithms. This helps avoidmany of the unintended consequences that can result from deploy-ing ""black box"" systems to solve complex problems. In this paper,we discuss our experience applying algorithmic accountability prin-ciples and frameworks to ecosystem forecasting, focusing on a casestudy forecasting shellfish toxicity in the Gulf of Maine. We adaptexisting frameworks such as Datasheets for Datasets and ModelCards for Model Reporting from their original focus on personallyidentifiable private data to include public datasets, such as thoseoften used in ecosystem forecasting applications, to audit the casestudy. We show how high level algorithmic accountability frame-works and domain level codes of ethics compliment each other,incentivizing more transparency, accountability, and fairness inautomated decision-making systems.","ACMDL - Accountability","Case Study",29
"15","ACMDL - Accountability","Ecosystem","Watson H,Moju-Igbene E,Kumari A,Das S","""We Hold Each Other Accountable"": Unpacking How Social Groups Approach Cybersecurity and Privacy Together","https://doi.org/10.1145/3313831.3376605;http://dx.doi.org/10.1145/3313831.3376605","Association for Computing Machinery",2020,"qualitative methods, groups, interviews, security, privacy, social cybersecurity","Digital resources are often collectively owned and shared by small social groups (e.g., friends sharing Netflix accounts, roommates sharing game consoles, families sharing WhatsApp groups). Yet, little is known about (i) how these groups jointly navigate cybersecurity and privacy (S&P) decisions for shared resources, (ii) how shared experiences influence individual S&P attitudes and behaviors, and (iii) how well existing S&P controls map onto group needs. We conducted group interviews and a supplemental diary study with nine social groups (n=34) of varying relationship types. We identified why, how and what resources groups shared, their jointly construed threat models, and how these factors influenced group strategies for securing shared resources. We also identified missed opportunities for cooperation and stewardship among group members that could have led to improved S&P behaviors, and found that existing S&P controls often fail to meet the needs of these small social groups.","ACMDL - Accountability","Ecosystem",30
"16","ACMDL - Accountability","Case Study","Javadi SA,Cloete R,Cobbe J,Lee MS,Singh J","Monitoring Misuse for Accountable 'Artificial Intelligence as a Service'","https://doi.org/10.1145/3375627.3375873;http://dx.doi.org/10.1145/3375627.3375873","Association for Computing Machinery",2020,"machine learning, aiaas, accountability, law, compliance, cloud computing, monitoring, mlaas, misuse, audit, artificial intelligence","AI is increasingly being offered 'as a service' (AIaaS). This entails service providers offering customers access to pre-built AI models and services, for tasks such as object recognition, text translation, text-to-voice conversion, and facial recognition, to name a few. The offerings enable customers to easily integrate a range of powerful AI-driven capabilities into their applications. Customers access these models through the provider's APIs, sending particular data to which models are applied, the results of which returned.However, there are many situations in which the use of AI can be problematic. AIaaS services typically represent generic functionality, available 'at a click'. Providers may therefore, for reasons of reputation or responsibility, seek to ensure that the AIaaS services they offer are being used by customers for 'appropriate' purposes.This paper introduces and explores the concept whereby AIaaS providers uncover situations of possible service misuse by their customers. Illustrated through topical examples, we consider the technical usage patterns that could signal situations warranting scrutiny, and raise some of the legal and technical challenges of monitoring for misuse. In all, by introducing this concept, we indicate a potential area for further inquiry from a range of perspectives.","ACMDL - Accountability","Case Study",47
"17","ACMDL - Accountability","Case Study","Sharma S,Kumar Kar A,Gupta MP","Unpacking Digital Accountability: Ensuring Efficient and Answerable e-Governance Service Delivery","https://doi.org/10.1145/3494193.3494229;http://dx.doi.org/10.1145/3494193.3494229","Association for Computing Machinery",2021,"Digital Accountability, e-governance, BWM, TISM, MICMAC","Governments around the world are investing many resources to develop ubiquitous governance networks and information systems; this is seen as the advent of Digital Era Governance (DEG). Government websites or service platforms are replacing the physical offices of government department and in-person state-citizen interactions. It is a transformative shift in not only the workings of the State but also a change in the overall governance ecosystem. Citizens' role as co-creators and participants in policy and initiative design is increasing, and there is a general push for collaborative e-governance.With this study, we attempt to explore how governments can ensure accountability by design and policy, i.e., Digital Accountability in these websites or platforms that are going to act as a one-stop shop for citizens regarding government services. We have taken a multi-method approach to explore the various constituent factors that help build up accountability in any e-government process or websites delivering web services. Best Worst Method (BWM) is employed to find the relative weights of these factors in the chosen context. These are validated using qualitative techniques of Total Interpretative structural modelling, and the Matrix of Cross Impact Multiplications Applied to Classification (TISM-MICMAC). We have also attempted to explore the contextual relationships between these factors and how do they help conceptualise and operationalise digital accountability for e-governance.","ACMDL - Accountability","Case Study",3
"18","ACMDL - Accountability","Case Study","Shen Z","The Regulatory Path of Big-Data Price Discrimination-Based on Economic Characteristics and Legal Accountability","https://doi.org/10.1145/3474944.3474954;http://dx.doi.org/10.1145/3474944.3474954","Association for Computing Machinery",2021,"differential treatment, platform economy, big-data price discrimination, anti-monopoly, fair trading","Big data has not been effectively governed after it has received social attention since 2018. This paper aims to explore a more appropriate and feasible regulatory path by exploring the reasons why big-data price discrimination has failed to obtain legal regulation, as well as the economic, technological, and market structure causes of big-data price discrimination. In the argument, it is found that there are legal problems such as doubtful illegality, regulatory gaps that belong to the intersection of multiple laws, and difficulty in obtaining evidence. And because of the profit-seeking nature of the platform economy, the unconsciousness of algorithms and the uniqueness of the platform market structure, the boundaries between big-data price discrimination and reasonable marketing strategies are difficult to decide. It is not possible to totally prohibit big-data price discrimination and hinder the development of the new business model of the platform economy. However, when the platform's profit-seeking behavior infringes on the market order and consumer rights, big-data price discrimination is legally accountable. On one side, big-data price discrimination may infringe consumers’ fair trading rights, and the judgment criteria should be determined based on cost and profit rate. On the other side, in the anti-monopoly law, the boundary between reasonable transaction habits and legally accountable differential treatment should be distinguished for big-data price discrimination.","ACMDL - Accountability","Case Study",5
"19","ACMDL - Accountability","Ecosystem","Getto G,Flanagan S,Labriola J","Designing Boater Advocacy: A Lean UX Mobile App Project to Increase Emergency Response Accountability","https://doi.org/10.1145/3472714.3473631;http://dx.doi.org/10.1145/3472714.3473631","Association for Computing Machinery",2021,"advocacy, user experience, mobile UX, methods","Based on an ongoing UX research project into the development of a grant-funded mobile safety application for recreational boaters, this paper details a workflow for aligning user advocacy with organizational accountability. Essentially, boating safety regulations have not been updated since the 1970s due to successful lobbying by boating manufacturers. Meanwhile, numerous boating safety concerns and associated incidents go unreported each year. User research into this context has taught us that recreational boaters most want a mobile application that will help them enjoy their boating trips while remaining safe. State agencies most want to locate boaters who are in distress without having to launch costly, and often ineffective, searches over large areas. Based on a review of best practices from amassed literature, as well as our own analysis of user interviews with 141 stakeholders obtained during an NSF grant devoted to this purpose, we will discuss a Lean UX workflow for mobile application development that balances user goals with organizational accountability.","ACMDL - Accountability","Ecosystem",55
"20","ACMDL - Accountability","Data Audit","Poirier L","Accountable Data: The Politics and Pragmatics of Disclosure Datasets","https://doi.org/10.1145/3531146.3533201;http://dx.doi.org/10.1145/3531146.3533201","Association for Computing Machinery",2022,"data provenance, accountability, infrastructure, disclosure","This paper attends specifically to what I call “disclosure datasets” - tabular datasets produced in accordance with laws requiring various kinds of disclosure. For the purposes of this paper, the most significant defining feature of disclosure datasets is that they aggregate information produced and reported by the same institutions they are meant to hold accountable. Through a series of case studies of disclosure datasets in the United States, I specifically draw attention to two concerns with disclosure datasets: First, for disclosure datasets, there is often political and social mobilization around the definitions that determine reporting thresholds, which in turn implicates what observations end up in the dataset. Changes in reporting thresholds can be traced along changes in political party power as the aims to promote accountability through mandated disclosure often get pitted against the aims to reduce regulatory burden. Second, for disclosure datasets, the observational unit – what is ultimately being counted in the data – is often not a person, institution, or action but instead a form that the reporting institution is required by law to fill out. Forms infrastructure the information that ends up in the dataset in notable ways. This work contributes to recent calls to promote the transparency and accountability of data science work through improved inquiry into and documentation of the social lineages of source datasets. The analysis of disclosure datasets presented in this paper poses important questions regarding what ultimately gets documented in the data, along with the representativeness and usefulness of these accountability mechanisms.","ACMDL - Accountability","Data Audit",6
"21","ACMDL - Accountability","Meta-Commentary","Ramesh D,Kameswaran V,Wang D,Sambasivan N","How Platform-User Power Relations Shape Algorithmic Accountability: A Case Study of Instant Loan Platforms and Financially Stressed Users in India","https://doi.org/10.1145/3531146.3533237;http://dx.doi.org/10.1145/3531146.3533237","Association for Computing Machinery",2022,"algorithmic fairness, socio-technical systems, algorithmic accountability, instant loans, human-ai interaction","Accountability, a requisite for responsible AI, can be facilitated through transparency mechanisms such as audits and explainability. However, prior work suggests that the success of these mechanisms may be limited to Global North contexts; understanding the limitations of current interventions in varied socio-political conditions is crucial to help policymakers facilitate wider accountability. To do so, we examined the mediation of accountability in the existing interactions between vulnerable users and a ‘high-risk’ AI system in a Global South setting. We report on a qualitative study with 29 financially-stressed users of instant loan platforms in India. We found that users experienced intense feelings of indebtedness for the ‘boon’ of instant loans, and perceived huge obligations towards loan platforms. Users fulfilled obligations by accepting harsh terms and conditions, over-sharing sensitive data, and paying high fees to unknown and unverified lenders. Users demonstrated a dependence on loan platforms by persisting with such behaviors despite risks of harms such as abuse, recurring debts, discrimination, privacy harms, and self-harm to them. Instead of being enraged with loan platforms, users assumed responsibility for their negative experiences, thus releasing the high-powered loan platforms from accountability obligations. We argue that accountability is shaped by platform-user power relations, and urge caution to policymakers in adopting a purely technical approach to fostering algorithmic accountability. Instead, we call for situated interventions that enhance agency of users, enable meaningful transparency, reconfigure designer-user relations, and prompt a critical reflection in practitioners towards wider accountability. We conclude with implications for responsibly deploying AI in FinTech applications in India and beyond.","ACMDL - Accountability","Critique",7
"22","ACMDL - Accountability","Case Study","Cloete R,Norval C,Singh J","Auditable Augmented/Mixed/Virtual Reality: The Practicalities of Mobile System Transparency","https://doi.org/10.1145/3495001;http://dx.doi.org/10.1145/3495001","Association for Computing Machinery",2022,"accountability, mobile/pervasive systems, audit, Android, transparency, audit tooling, augmented/mixed/virtual reality","Virtual, Augmented and Mixed Reality (XR) technologies are becoming increasingly pervasive. However, the contextual nature of XR, and its tight coupling of the digital and physical environments, brings real propensity for loss and harm.This means that auditability---the ability to inspect how a system operates---will be crucial for dealing with incidents as they occur, by providing the information enabling rectification, repair and recourse. However, supporting audit in XR brings considerations, as the process of capturing audit data itself has implications and challenges, both for the application (e.g., overheads) and more broadly.This paper explores the practicalities of auditing XR systems, characterises the tensions between audit and other considerations, and argues the need for flexible tools enabling the management of such. In doing so, we introduce Droiditor, a configurable open-source Android toolkit that enables the runtime capture of audit-relevant data from mobile applications. We use Droiditor as a means to indicate some potential implications of audit data capture, demonstrate how greater configurability can assist in managing audit-related concerns, and discuss the potential considerations that result. Given the societal demands for more transparent and accountable systems, our broader aim is to draw attention to auditability, highlighting tangible ways forward and areas for future work.","ACMDL - Accountability","Case Study",58
"23","ACMDL - Accountability","Meta-Commentary","Liu H,Wang Y,Fan W,Liu X,Li Y,Jain S,Liu Y,Jain AK,Tang J","Trustworthy AI: A Computational Perspective","https://doi.org/10.1145/3546872;http://dx.doi.org/10.1145/3546872","ACM Trans. Intell. Syst. Technol.",2022,"artificial intelligence, robustness, environmental well-being, privacy, accountability, fairness, explainability","In the past few decades, artificial intelligence (AI) technology has experienced swift developments, changing everyone’s daily life and profoundly altering the course of human society. The intention behind developing AI was and is to benefit humans by reducing labor, increasing everyday conveniences, and promoting social good. However, recent research and AI applications indicate that AI can cause unintentional harm to humans by, for example, making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against a group or groups. Consequently, trustworthy AI has recently garnered increased attention regarding the need to avoid the adverse effects that AI could bring to people, so people can fully trust and live in harmony with AI technologies. A tremendous amount of research on trustworthy AI has been conducted and witnessed in recent years. In this survey, we present a comprehensive appraisal of trustworthy AI from a computational perspective to help readers understand the latest technologies for achieving trustworthy AI. Trustworthy AI is a large and complex subject, involving various dimensions. In this work, we focus on six of the most crucial dimensions in achieving trustworthy AI: (i) Safety & Robustness, (ii) Nondiscrimination & Fairness, (iii) Explainability, (iv) Privacy, (v) Accountability & Auditability, and (vi) Environmental Well-being. For each dimension, we review the recent related technologies according to a taxonomy and summarize their applications in real-world systems. We also discuss the accordant and conflicting interactions among different dimensions and discuss potential aspects for trustworthy AI to investigate in the future.","ACMDL - Accountability","Meta-Commentary",59
"24","ACMDL - Accountability","Meta-Commentary","Orphanou K,Otterbacher J,Kleanthous S,Batsuren K,Giunchiglia F,Bogina V,Tal AS,Hartman A,Kuflik T","Mitigating Bias in Algorithmic Systems - A Fish-Eye View","https://doi.org/10.1145/3527152;http://dx.doi.org/10.1145/3527152","ACM Comput. Surv.",2022,"transparency, fairness, Algorithmic bias, social bias, explainability","Mitigating bias in algorithmic systems is a critical issue drawing attention across communities within the information and computer sciences. Given the complexity of the problem and the involvement of multiple stakeholders – including developers, end users and third-parties – there is a need to understand the landscape of the sources of bias, and the solutions being proposed to address them, from a broad, cross-domain perspective. This survey provides a “fish-eye view,” examining approaches across four areas of research. The literature describes three steps toward a comprehensive treatment – bias detection, fairness management and explainability management – and underscores the need to work from within the system as well as from the perspective of stakeholders in the broader context.","ACMDL - Accountability","Meta-Commentary",60
"25","AIES","Case Study","Kyle et al.","Always Lurking: Understanding and Mitigating Bias in Online Human Trafficking Detection","https://dl.acm.org/doi/10.1145/3278721.3278782","AIES",2018,"Human Trafficking; Web Crawling; Bias Mitigation; Clustering, Text Classification","Web-based human trafficking activity has increased in recent years but it remains sparsely dispersed among escort advertisements and difficult to identify due to its often-latent nature. The use of intelligent systems to detect trafficking can thus have a direct impact on investigative resource allocation and decision-making, and, more broadly, help curb a widespread social problem. Trafficking detection involves assigning a normalized score to a set of escort advertisements crawled from the Web -- a higher score indicates a greater risk of trafficking-related (involuntary) activities. In this paper, we define and study the problem of trafficking detection and present a trafficking detection pipeline architecture developed over three years of research within the DARPA Memex program. Drawing on multi-institutional data, systems, and experiences collected during this time, we also conduct post hoc bias analyses and present a bias mitigation plan. Our findings show that, while automatic trafficking detection is an important application of AI for social good, it also provides cautionary lessons for deploying predictive machine learning algorithms without appropriate de-biasing. This ultimately led to integration of an interpretable solution into a search system that contains over 100 million advertisements and is used by over 200 law enforcement agencies to investigate leads.","ACMDL - AIES","Case Study",440
"26","AIES","Meta-Commentary","Tan et al.","Distill-and-Compare: Auditing Black-Box Models Using Transparent Model Distillation","https://dl.acm.org/doi/10.1145/3278721.3278725","AIES",2018,"Interpretability; Black-box models; Distillation; Fairness","Black-box risk scoring models permeate our lives, yet are typically proprietary or opaque. We propose Distill-and-Compare, an approach to audit such models without probing the black-box model API or pre-defining features to audit. To gain insight into black-box models, we treat them as teachers, training transparent student models to mimic the risk scores assigned by the black-box models. We compare the mimic model trained with distillation to a second, un-distilled transparent model trained on ground truth outcomes, and use differences between the two models to gain insight into the black-box model. We demonstrate the approach on four data sets: COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club. We also propose a statistical test to determine if a data set is missing key features used to train the black-box model. Our test finds that the ProPublica data is likely missing key feature(s) used in COMPAS.","ACMDL - AIES","Method",478
"27","AIES","Meta-Commentary","Raz et al.","Face Mis-ID: An Interactive Pedagogical Tool Demonstrating Disparate Accuracy Rates in Facial Recognition","https://dl.acm.org/doi/10.1145/3461702.3462627","AIES",2018,"facial recognition, surveillance; algorithmic bias; participatory design; educational tools; literacy; legibility; non-specialist understanding; interactive demo","This paper reports on the making of an interactive demo to illustrate algorithmic bias in facial recognition. Facial recognition technology has been demonstrated to be more likely to misidentify women and minoritized people. This risk, among others, has elevated facial recognition into policy discussions across the country, where many jurisdictions have already passed bans on its use. Whereas scholarship on the disparate impacts of algorithmic systems is growing, general public awareness of this set of problems is limited in part by the illegibility of machine learning systems to non-specialists. Inspired by discussions with community organizers advocating for tech fairness issues, we created the Face Mis-ID Demo to reveal the algorithmic functions behind facial recognition technology and to demonstrate its risks to policymakers and members of the community. In this paper, we share the design process behind this interactive demo, its form and function, and the design decisions that honed its accessibility, toward its use for improving legibility of algorithmic systems and awareness of the sources of their disparate impacts.","ACMDL - AIES","Tool",479
"28","AIES","Case Study","Matthews et al.","The Right To Confront Your Accusers: Opening the Black Box of Forensic DNA Software","https://dl.acm.org/doi/10.1145/3306618.3314279","AIES",2019,"algorithmic accountability; criminal justice software; probabilistic genotyping software; Forensic Statistical Tool (FST)","The results of forensic DNA software systems are regularly introduced as compelling evidence in criminal trials, but requests by defendants to evaluate how these results are generated are often denied. Furthermore, there is mounting evidence of problems such as failures to disclose substantial changes in methodology to oversight bodies and substantial differences in the results generated by different software systems. In a society that purports to guarantee defendants the right to face their accusers and confront the evidence against them, what then is the role of black-box forensic software systems in moral decision making in criminal justice? In this paper, we examine the case of the Forensic Statistical Tool (FST), a forensic DNA system developed in 2010 by New York City's Office of Chief Medical Examiner (OCME). For over 5 years, expert witness review requested by defense teams was denied, even under protective order, while the system was used in over 1300 criminal cases. When the first expert review was finally permitted in 2016, many problems were identified including an undisclosed function capable of dropping evidence that could be beneficial to the defense. Overall, the findings were so substantial that a motion to release the full source code of FST publicly was granted. In this paper, we quantify the impact of this undisclosed function on samples from OCME's own validation study and discuss the potential impact on individual defendants. Specifically, we find that 104 of the 439 samples (23.7%) triggered the undisclosed data-dropping behavior and that the change skewed results toward false inclusion for individuals whose DNA was not present in an evidence sample. Beyond this, we consider what changes in the criminal justice system could prevent problems like this from going unresolved in the future.","ACMDL - AIES","Case Study",458
"29","AIES","Case Study","Bryant, Howard","A Comparative Analysis of Emotion-Detecting AI Systems with Respect to Algorithm Performance and Dataset Diversity","https://dl.acm.org/doi/pdf/10.1145/3306618.3314284","AIES",2019,NA,"In recent news, organizations have been considering the use of facial and emotion recognition for applications involving youth such as tackling surveillance and security in schools. However, the majority of efforts on facial emotion recognition research have focused on adults. Children, particularly in their early years, have been shown to express emotions quite differently than adults. Thus, before such algorithms are deployed in environments that impact the wellbeing and circumstance of youth, a careful examination should be made on their accuracy with respect to appropriateness for this target demographic. In this work, we utilize several datasets that contain facial expressions of children linked to their emotional state to evaluate eight different commercial emotion classification systems. We compare the ground truth labels provided by the respective datasets to the labels given with the highest confidence by the classification systems and assess the results in terms of matching score (TPR), positive predictive value, and failure to compute rate. Overall results show that the emotion recognition systems displayed subpar performance on the datasets of children's expressions compared to prior work with adult datasets and initial human ratings. We then identify limitations associated with automated recognition of emotions in children and provide suggestions on directions with enhancing recognition accuracy through data diversification, dataset accountability, and algorithmic regulation.","ACMDL - AIES","Case Study",459
"30","AIES","Case Study","Raji, Buolamwini","Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products","https://dl.acm.org/doi/10.1145/3306618.3314244","AIES",2019,"Ethics, Machine Learning, Artificial Intelligence, Facial Recognition, Commercial Applications, Fairness, Computer Vision","Although algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, we struggle to understand the real-world impact of these audits, as scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the Gender Shades study, 2) presents new performance metrics from targeted companies IBM, Microsoft and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, 3) provides performance results on PPB by non-target companies Amazon and Kairos and, 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within 7 months of the original audit, we find that all three targets released new API versions. All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup, that underwent a 17.7% - 30.4% reduction in error between audit periods. Minimizing these disparities led to a 5.72% to 8.3% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66% and 6.60% overall, and error rates of 31.37% and 22.50% for the darker female subgroup, respectively.","ACMDL - AIES","Case Study",460
"31","AIES","Case Study","Matthews et al.","When Trusted Black Boxes Don't Agree: Incentivizing Iterative Improvement and Accountability in Critical Software Systems","https://dl.acm.org/doi/10.1145/3375627.3375807","AIES",2020,"algorithmic accountability, criminal justice software, probabilistic genotyping, software verification, disparate impact","Software increasingly plays a key role in regulated areas like housing, hiring, and credit, as well as major public functions such as criminal justice and elections. It is easy for there to be unintended defects with a large impact on the lives of individuals and society as a whole. Preventing, finding, and fixing software defects is a key focus of both industrial software development efforts as well as academic research in software engineering. In this paper, we discuss flaws in the larger socio-technical decision-making processes in which critical black-box software systems are developed, deployed, and trusted. We use criminal justice software, specifically probabilistic genotyping (PG) software, as a concrete example. We describe how PG software systems, designed to do the same job, produce different results. We highlight the under-appreciated impact of changes in key parameters and the disparate impact that one such parameter can have on different racial/ethnic groups. We propose concrete changes to the socio-technical decision-making processes surrounding the use of PG software that could be used to incentivize iterative improvements in the accuracy, fairness, reliability, and accountability of these systems.","ACMDL - AIES","Case Study",453
"32","AIES","Case Study","Slack et al.","Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods","https://dl.acm.org/doi/10.1145/3375627.3375830","AIES",2020,"black box explanations, model interpretability, bias detection, adversarial attacks","As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.","ACMDL - AIES","Case Study",454
"33","AIES","Case Study","Dulhanty, Wong","Investigating the Impact of Inclusion in Face Recognition Training Data on Individual Face Identification","https://dl.acm.org/doi/10.1145/3375627.3375875","AIES",2020,"face recognition, neural networks, privacy, informed consent","Modern face recognition systems leverage datasets containing images of hundreds of thousands of specific individuals' faces to train deep convolutional neural networks to learn an embedding space that maps an arbitrary individual's face to a vector representation of their identity. The performance of a face recognition system in face verification (1:1) and face identification (1:N) tasks is directly related to the ability of an embedding space to discriminate between identities. Recently, there has been significant public scrutiny into the source and privacy implications of large-scale face recognition training datasets such as MS-Celeb-1M and MegaFace, as many people are uncomfortable with their face being used to train dual-use technologies that can enable mass surveillance. However, the impact of an individual's inclusion in training data on a derived system's ability to recognize them has not previously been studied. In this work, we audit ArcFace, a state-of-the-art, open source face recognition system, in a large-scale face identification experiment with more than one million distractor images. We find a Rank-1 face identification accuracy of 79.71% for individuals present in the model's training data and an accuracy of 75.73% for those not present. This modest difference in accuracy demonstrates that face recognition systems using deep learning work better for individuals they are trained on, which has serious privacy implications when one considers all major open source face recognition training datasets do not obtain informed consent from individuals during their collection.","ACMDL - AIES","Case Study",455
"34","AIES","Case Study","Clavell et al.","Auditing Algorithms: On Lessons Learned and the Risks of Data Minimization","https://dl.acm.org/doi/10.1145/3375627.3375852","AIES",2020,"GDPR; algorithms; AI, recommender systems; data ethics; bias","In this paper, we present the Algorithmic Audit (AA) of REM!X, a personalized well-being recommendation app developed by Telefónica Innovación Alpha. The main goal of the AA was to identify and mitigate algorithmic biases in the recommendation system that could lead to the discrimination of protected groups. The audit was conducted through a qualitative methodology that included five focus groups with developers and a digital ethnography relying on users comments reported in the Google Play Store. To minimize the collection of personal information, as required by best practice and the GDPR [1], the REM!X app did not collect gender, age, race, religion, or other protected attributes from its users. This limited the algorithmic assessment and the ability to control for different algorithmic biases. Indirect evidence was thus used as a partial mitigation for the lack of data on protected attributes, and allowed the AA to identify four domains where bias and discrimination were still possible, even without direct personal identifiers. Our analysis provides important insights into how general data ethics principles such as data minimization, fairness, non-discrimination and transparency can be operationalized via algorithmic auditing, their potential and limitations, and how the collaboration between developers and algorithmic auditors can lead to better technologies","ACMDL - AIES","Case Study",456
"35","AIES","Case Study","Zhou, Sheng, Howley","Assessing Post-hoc Explainability of the BKT Algorithm","https://dl.acm.org/doi/10.1145/3375627.3375856","AIES",2020,"explainable AI, post-hoc explanations, interpretability of algorithms, communicating algorithmic systems, evaluation of xAI systems","As machine intelligence is increasingly incorporated into educational technologies, it becomes imperative for instructors and students to understand the potential flaws of the algorithms on which their systems rely. This paper describes the design and implementation of an interactive post-hoc explanation of the Bayesian Knowledge Tracing algorithm which is implemented in learning analytics systems used across the United States. After a user-centered design process to smooth out interaction design difficulties, we ran a controlled experiment to evaluate whether the interactive or static version of the explainable led to increased learning. Our results reveal that learning about an algorithm through an explainable depends on users' educational background. For other contexts, designers of post-hoc explainables must consider their users' educational background to best determine how to empower more informed decision-making with AI-enhanced systems.","ACMDL - AIES","Case Study",457
"36","AIES","Data Audit","Kazimzade, Miceli","Biased Priorities, Biased Outcomes: Three Recommendations for Ethics-oriented Data Annotation Practices","https://dl.acm.org/doi/10.1145/3375627.3375809","AIES",2020,"Bias; data; annotation; classification; ethics; power; transparency; priority; profit","In this paper, we analyze the relation between data-related biases and practices of data annotation, by placing them in the context of market economy. We understand annotation as a praxis related to the sensemaking of data and investigate annotation practices for vision models by focusing on the values that are prioritized by industrial decision-makers and practitioners. The quality of data is critical for machine learning models as it holds the power to (mis-)represent the population it is intended to analyze. For autonomous systems to be able to make sense of the world, humans first need to make sense of the data these systems will be trained on. This paper addresses this issue, guided by the following research questions: Which goals are prioritized by decision-makers at the data annotation stage? How do these priorities correlate with data-related bias issues? Focusing on work practices and their context, our research goal aims at understanding the logics driving companies and their impact on the performed annotations. The study follows a qualitative design and is based on 24 interviews with relevant actors and extensive participatory observations, including several weeks of fieldwork at two companies dedicated to data annotation for vision models in Buenos Aires, Argentina and Sofia, Bulgaria. The prevalence of market-oriented values over socially responsible approaches is argued based on three corporate priorities that inform work practices in this field and directly shape the annotations performed: profit (short deadlines connected to the strive for profit are prioritized over alternative approaches that could prevent biased outcomes), standardization (the strive for standardized and, in many cases, reductive or biased annotations to make data fit the products and revenue plans of clients), and opacity (related to client's power to impose their criteria on the annotations that are performed. Criteria that most of the times remain opaque due to corporate confidentiality). Finally, we introduce three elements, aiming at developing ethics-oriented practices of data annotation, that could help prevent biased outcomes: transparency (regarding the documentation of data transformations, including information on responsibilities and criteria for decision-making.), education (training on the potential harms caused by AI and its ethical implications, that could help data annotators and related roles adopt a more critical approach towards the interpretation and labeling of data), and regulations (clear guidelines for ethical AI developed at the governmental level and applied both in private and public organizations).","ACMDL - AIES","Data Audit",467
"37","AIES","Meta-Commentary","Raji et al.","Saving Face: Investigating the Ethical Concerns of Facial Recognition Auditing","https://dl.acm.org/doi/10.1145/3375627.3375820","AIES",2020,NA,"Although essential to revealing biased performance, well intentioned attempts at algorithmic auditing can have effects that may harm the very populations these measures are meant to protect. This concern is even more salient while auditing biometric systems such as facial recognition, where the data is sensitive and the technology is often used in ethically questionable manners. We demonstrate a set of fiveethical concerns in the particular case of auditing commercial facial processing technology, highlighting additional design considerations and ethical tensions the auditor needs to be aware of so as not exacerbate or complement the harms propagated by the audited system. We go further to provide tangible illustrations of these concerns, and conclude by reflecting on what these concerns mean for the role of the algorithmic audit and the fundamental product limitations they reveal.","ACMDL - AIES","Meta-Commentary",473
"38","AIES","Meta-Commentary","Dobbe, Gilbert, Mintz","Hard Choices in Artificial Intelligence: Addressing Normative Uncertainty through Sociotechnical Commitments","https://dl.acm.org/doi/10.1145/3375627.3375861","AIES",2020,NA,"The implementation of AI systems has led to new forms of harm in various sensitive social domains. We analyze these as problems How to address these harms remains at the center of controversial debate. In this paper, we discuss the inherent normative uncertainty and political debates surrounding the safety of AI systems.of vagueness to illustrate the shortcomings of current technical approaches in the AI Safety literature, crystallized in three dilemmas that remain in the design, training and deployment of AI systems. We argue that resolving normative uncertainty to render a system 'safe' requires a sociotechnical orientation that combines quantitative and qualitative methods and that assigns design and decision power across affected stakeholders to navigate these dilemmas through distinct channels for dissent. We propose a set of sociotechnical commitments and related virtues to set a bar for declaring an AI system 'human-compatible', implicating broader interdisciplinary design approaches.","ACMDL - AIES","Method",477
"39","AIES","Case Study","Engelmann et al.","Blacklists and Redlists in the Chinese Social Credit System: Diversity, Flexibility, and Comprehensiveness","https://dl.acm.org/doi/10.1145/3461702.3462535","AIES",2021,"China’s Social Credit Systems; Reputation Systems; Digital SocioTechnical Systems; China","The Chinese Social Credit System (SCS) is a novel digital socio-technical credit system. The SCS aims to regulate societal behavior by reputational and material devices. Scholarship on the SCS has offered a variety of legal and theoretical perspectives. However, little is known about its actual implementation. Here, we provide the first comprehensive empirical study of digital blacklists (listing ""bad"" behavior) and redlists (listing ""good"" behavior) in the Chinese SCS. Based on a unique data set of reputational blacklists and redlists in 30 Chinese provincial-level administrative divisions (ADs), we show the diversity, flexibility, and comprehensiveness of the SCS listing infrastructure. First, our results demonstrate that the Chinese SCS unfolds in a highly diversified manner: we find differences in accessibility, interface design and credit information across provincial-level SCS blacklists and redlists. Second, SCS listings are flexible. During the COVID-19 outbreak, we observe a swift addition of blacklists and redlists that helps strengthen the compliance with coronavirus-related norms and regulations. Third, the SCS listing infrastructure is comprehensive. Overall, we identify 273 blacklists and 154 redlists across provincial-level ADs. Our blacklist and redlist taxonomy highlights that the SCS listing infrastructure prioritizes law enforcement and industry regulations. We also identify redlists that reward political and moral behavior. Our study substantiates the enormous scale and diversity of the Chinese SCS and puts the debate on its reach and societal impact on firmer ground. Finally, we initiate a discussion on the ethical dimensions of data-driven research on the SCS.","ACMDL - AIES","Case Study",444
"40","AIES","Case Study","Guo, Caliskan","Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases","https://dl.acm.org/doi/10.1145/3461702.3462536","AIES",2021,"AI ethics; bias; intersectionality; language models; social psychology; word embeddings","With the starting point that implicit human biases are reflected in the statistical regularities of language, it is possible to measure biases in English static word embeddings. State-of-the-art neural language models generate dynamic word embeddings dependent on the context in which the word appears. Current methods measure pre-defined social and intersectional biases that occur in contexts defined by sentence templates. Dispensing with templates, we introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. GPT-2 contains the smallest magnitude of overall bias followed by GPT, BERT, and then ELMo, negatively correlating with the contextualization levels of the models.","ACMDL - AIES","Case Study",445
"41","AIES","Case Study","Sheng, Yao, Goel","Surveilling Surveillance: Estimating the Prevalence of Surveillance Cameras with Street View Data","https://dl.acm.org/doi/10.1145/3461702.3462525","AIES",2021,"Computer vision, privacy, urban computing","The use of video surveillance in public spaces--both by government agencies and by private citizens--has attracted considerable attention in recent years, particularly in light of rapid advances in face-recognition technology. But it has been difficult to systematically measure the prevalence and placement of cameras, hampering efforts to assess the implications of surveillance on privacy and public safety. Here we present a novel approach for estimating the spatial distribution of surveillance cameras: applying computer vision algorithms to large-scale street view image data. Specifically, we build a camera detection model and apply it to 1.6 million street view images sampled from 10 large U.S. cities and 6 other major cities around the world, with positive model detections verified by human experts. After adjusting for the estimated recall of our model, and accounting for the spatial coverage of our sampled images, we are able to estimate the density of surveillance cameras visible from the road. Across the 16 cities we consider, the estimated number of surveillance cameras per linear kilometer ranges from 0.1 (in Seattle) to 0.9 (in Seoul). In a detailed analysis of the 10 U.S. cities, we find that cameras are concentrated in commercial, industrial, and mixed zones, and in neighborhoods with higher shares of non-white residents---a pattern that persists even after adjusting for land use. These results help inform ongoing discussions on the use of surveillance technology, including its potential disparate impacts on communities of color.","ACMDL - AIES","Case Study",446
"42","AIES","Case Study","Barlas et al.","Person, Human, Neither: The Dehumanization Potential of Automated Image Tagging","https://dl.acm.org/doi/10.1145/3461702.3462567","AIES",2021,"image tagging algorithms; dehumanization; science, technology, and society studies; critical computing","Following the literature on dehumanization via technology, we audit six proprietary image tagging algorithms (ITAs) for their potential to perpetuate dehumanization. We examine the ITAs' outputs on a controlled dataset of images depicting a diverse group of people for tags that indicate the presence of a human in the image. Through an analysis of the (mis)use of these tags, we find that there are some individuals whose 'humanness' is not recognized by an ITA, and that these individuals are often from marginalized social groups. Finally, we compare these findings with the use of the 'face' tag, which can be used for surveillance, revealing that people's faces are often recognized by an ITA even when their 'humanness' is not. Overall, we highlight the subtle ways in which ITAs may inflict widespread, disparate harm, and emphasize the importance of considering the social context of the resulting application.","ACMDL - AIES","Case Study",447
"43","AIES","Case Study","Fabris et al.","Algorithmic Audit of Italian Car Insurance: Evidence of Unfairness in Access and Pricing","https://dl.acm.org/doi/10.1145/3461702.3462569","AIES",2021,"Algorithmic Audit; Algorithmic Fairness; Car Insurance; Fairness Through Unawareness","We conduct an audit of pricing algorithms employed by companies in the Italian car insurance industry, primarily by gathering quotes through a popular comparison website. While acknowledging the complexity of the industry, we find evidence of several problematic practices. We show that birthplace and gender have a direct and sizeable impact on the prices quoted to drivers, despite national and international regulations against their use. Birthplace, in particular, is used quite frequently to the disadvantage of foreign-born drivers and drivers born in certain Italian cities. In extreme cases, a driver born in Laos may be charged 1,000 more than a driver born in Milan, all else being equal. For a subset of our sample, we collect quotes directly on a company website, where the direct influence of gender and birthplace is confirmed. Finally, we find that drivers with riskier profiles tend to see fewer quotes in the aggregator result pages, substantiating concerns of differential treatment raised in the past by Italian insurance regulators.","ACMDL - AIES","Case Study",448
"44","AIES","Case Study","Grzelak, Brandao","The Dangers of Drowsiness Detection: Differential Performance, Downstream Impact, and Misuses","https://dl.acm.org/doi/10.1145/3461702.3462593","AIES",2021,"drowsiness detection, bias, disparate impact, fairness, technology misuses, surveillance","Drowsiness and fatigue are important factors in driving safety and work performance. This has motivated academic research into detecting drowsiness, and sparked interest in the deployment of related products in the insurance and work-productivity sectors. In this paper we elaborate on the potential dangers of using such algorithms. We first report on an audit of performance bias across subject gender and ethnicity, identifying which groups would be disparately harmed by the deployment of a state-of-the-art drowsiness detection algorithm. We discuss some of the sources of the bias, such as the lack of robustness of facial analysis algorithms to face occlusions, facial hair, or skin tone. We then identify potential downstream harms of this performance bias, as well as potential misuses of drowsiness detection technology---focusing on driving safety and experience, insurance cream-skimming and coverage-avoidance, worker surveillance, and job precarity.","ACMDL - AIES","Case Study",449
"45","AIES","Case Study","Hanley et al.","Computer Vision and Conflicting Values: Describing People with Automated Alt Text","https://dl.acm.org/doi/10.1145/3461702.3462620","AIES",2021,"accessibility, alt text, visual impairments, computer vision, identity, gender, race, disability, Facebook, museums, policy","Scholars have recently drawn attention to a range of controversial issues posed by the use of computer vision for automatically generating descriptions of people in images. Despite these concerns, automated image description has become an important tool to ensure equitable access to information for blind and low vision people. In this paper, we investigate the ethical dilemmas faced by companies that have adopted the use of computer vision for producing alt text: textual descriptions of images for blind and low vision people. We use Facebook's automatic alt text tool as our primary case study. First, we analyze the policies that Facebook has adopted with respect to identity categories, such as race, gender, age, etc., and the company's decisions about whether to present these terms in alt text. We then describe an alternative---and manual---approach practiced in the museum community, focusing on how museums determine what to include in alt text descriptions of cultural artifacts. We compare these policies, using notable points of contrast to develop an analytic framework that characterizes the particular apprehensions behind these policy choices. We conclude by considering two strategies that seem to sidestep some of these concerns, finding that there are no easy ways to avoid the normative dilemmas posed by the use of computer vision to automate alt text.","ACMDL - AIES","Case Study",450
"46","AIES","Case Study","Heidenreick, Williams","The Earth Is Flat and the Sun Is Not a Star: The Susceptibility of GPT-2 to Universal Adversarial Triggers","https://dl.acm.org/doi/10.1145/3461702.3462578","AIES",2021,"Natural Language Processing; Adversarial Attacks; Bias; Language Modeling","This work considers universal adversarial triggers, a method of adversarially disrupting natural language models, and questions if it is possible to use such triggers to affect both the topic and stance of conditional text generation models. In considering four ""controversial"" topics, this work demonstrates success at identifying triggers that cause the GPT-2 model to produce text about targeted topics as well as influence the stance the text takes towards the topic. We show that, while the more fringe topics are more challenging to identify triggers for, they do appear to more effectively discriminate aspects like stance. We view this both as an indication of the dangerous potential for controllability and, perhaps, a reflection of the nature of the disconnect between conflicting views on these topics, something that future work could use to question the nature of filter bubbles and if they are reflected within models trained on internet content. In demonstrating the feasibility and ease of such an attack, this work seeks to raise the awareness that neural language models are susceptible to this influence--even if the model is already deployed and adversaries lack internal model access--and advocates the immediate safeguarding against this type of adversarial attack in order to prevent potential harm to human users.","ACMDL - AIES","Case Study",451
"47","AIES","Case Study","Kim et al.","Age Bias in Emotion Detection: An Analysis of Facial Emotion Recognition Performance on Young, Middle-Aged, and Older Adults","https://dl.acm.org/doi/10.1145/3461702.3462609","AIES",2021,"ethical computing, age bias, facial emotion recognition, technology audit, artificial intelligence","The growing potential for facial emotion recognition (FER) technology has encouraged expedited development at the cost of rigorous validation. Many of its use-cases may also impact the diverse global community as FER becomes embedded into domains ranging from education to security to healthcare. Yet, prior work has highlighted that FER can exhibit both gender and racial biases like other facial analysis techniques. As a result, bias-mitigation research efforts have mainly focused on tackling gender and racial disparities, while other demographic related biases, such as age, have seen less progress. This work seeks to examine the performance of state of the art commercial FER technology on expressive images of men and women from three distinct age groups. We utilize four different commercial FER systems in a black box methodology to evaluate how six emotions - anger, disgust, fear, happiness, neutrality, and sadness - are correctly detected by age group. We further investigate how algorithmic changes over the last year have affected system performance. Our results found that all four commercial FER systems most accurately perceived emotion in images of young adults and least accurately in images of older adults. This trend was observed for analyses conducted in 2019 and 2020. However, little to no gender disparities were observed in either year. While older adults may not have been the initial target consumer of FER technology, statistics show the demographic is quickly growing more keen to applications that use such systems. Our results demonstrate the importance of considering various demographic subgroups during FER system validation and the need for inclusive, intersectional algorithmic developmental practices.","ACMDL - AIES","Case Study",452
"48","AIES","Case Study","Abid, Abubakar and Farooqi, Maheen and Zou, James","Persistent Anti-Muslim Bias in Large Language Models","https://dl.acm.org/doi/10.1145/3461702.3462624","AIES",2021,"machine learning; language models; bias; stereotypes; ethics","It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups.","ACMDL - AIES","Case Study",462
"49","AIES","Case Study","Pandey A,Caliskan A","Disparate Impact of Artificial Intelligence Bias in Ridehailing Economy's Price Discrimination Algorithms","https://dl.acm.org/doi/10.1145/3461702.3462561","AIES",2021,"AI ethics; algorithmic bias; disparate impact; geolocation; prediction; price discrimination","Ridehailing applications that collect mobility data from individuals to inform smart city planning predict each trip's fare pricing with automated algorithms that rely on artificial intelligence (AI). This type of AI algorithm, namely a price discrimination algorithm, is widely used in the industry's black box systems for dynamic individualized pricing. Lacking transparency, studying such AI systems for fairness and disparate impact has not been possible without access to data used in generating the outcomes of price discrimination algorithms. Recently, in an effort to enhance transparency in city planning, the city of Chicago regulation mandated that transportation providers publish anonymized data on ridehailing. As a result, we present the first large-scale measurement of the disparate impact of price discrimination algorithms used by ridehailing applications.The application of random effects models from the meta-analysis literature combines the city-level effects of AI bias on fare pricing from census tract attributes, aggregated from the American Community Survey. An analysis of 100 million ridehailing samples from the city of Chicago indicates a significant disparate impact in fare pricing of neighborhoods due to AI bias learned from ridehailing utilization patterns associated with demographic attributes. Neighborhoods with larger non-white populations, higher poverty levels, younger residents, and high education levels are significantly associated with higher fare prices, with combined effect sizes, measured in Cohen's d, of -0.32, -0.28, 0.69, and 0.24 for each demographic, respectively. Further, our methods hold promise for identifying and addressing the sources of disparate impact in AI algorithms learning from datasets that contain U.S. geolocations.","ACMDL - AIES","Case Study",463
"50","AIES","Data Audit","Chen et al.","Gender Bias and Under-Representation in Natural Language Processing Across Human Languages","https://dl.acm.org/doi/10.1145/3461702.3462530","AIES",2021,"bias, gender bias, natural language processing","Natural Language Processing (NLP) systems are at the heart of many critical automated decision-making systems making crucial recommendations about our future world. However, these systems reflect a wide range of biases, from gender bias to a bias in which voices they represent. In this paper, a team including speakers of 9 languages - Chinese, Spanish, English, Arabic, German, French, Farsi, Urdu, and Wolof - reports and analyzes measurements of gender bias in the Wikipedia corpora for these 9 languages. In the process, we also document how our work exposes crucial gaps in the NLP-pipeline for many languages. Despite substantial investments in multilingual support, the modern NLP-pipeline still systematically and dramatically under-represents the majority of human voices in the NLP-guided decisions that are shaping our collective future. We develop extensions to profession-level and corpus-level gender bias metric calculations originally designed for English and apply them to 8 other languages, including languages like Spanish, Arabic, German, French and Urdu that have grammatically gendered nouns including different feminine, masculine and neuter profession words. We compare these gender bias measurements across the Wikipedia corpora in different languages as well as across some corpora of more traditional literature.","ACMDL - AIES","Data Audit",464
"51","AIES","Data Audit","Fogliato et al.","On the Validity of Arrest as a Proxy for Offense: Race and the Likelihood of Arrest for Violent Crimes","https://dl.acm.org/doi/10.1145/3461702.3462538","AIES",2021,"risk assessment instrument; crime; racial disparity; NIBRS","Re-offense risk is considered in decision-making at many stages of the criminal justice system, from pre-trial, to sentencing, to parole. To aid decision-makers in their assessments, institutions increasingly rely on algorithmic risk assessment instruments (RAIs). These tools assess the likelihood that an individual will be arrested for a new criminal offense within some time window following their release. However, since not all crimes result in arrest, RAIs do not directly assess the risk of re-offense. Furthermore, disparities in the likelihood of arrest can potentially lead to biases in the resulting risk scores. Several recent validations of RAIs have therefore focused on arrests for violent offenses, which are viewed as being more accurate and less biased reflections of offending behavior. In this paper, we investigate biases in violent arrest data by analysing racial disparities in the likelihood of arrest for White and Black violent offenders. We focus our study on 2007--2016 incident-level data of violent offenses from 16 US states as recorded in the National Incident Based Reporting System (NIBRS). Our analysis shows that the magnitude and direction of the racial disparities depend on various characteristics of the crimes. In addition, our investigation reveals large variations in arrest rates across geographical locations and offense types. We discuss the implications of the observed disconnect between re-arrest and re-offense in the context of RAIs and the challenges around the use of data from NIBRS to correct for the sampling bias.","ACMDL - AIES","Data Audit",465
"52","AIES","Data Audit","Park et al.","Understanding the Representation and Representativeness of Age in AI Data Sets","https://dl.acm.org/doi/10.1145/3461702.3462590","AIES",2021,"AI FATE; datasets; inclusion; representation; older adults; aging; accessibility","A diverse representation of different demographic groups in AI training data sets is important in ensuring that the models will work for a large range of users. To this end, recent efforts in AI fairness and inclusion have advocated for creating AI data sets that are well-balanced across race, gender, socioeconomic status, and disability status. In this paper, we contribute to this line of work by focusing on the representation of age by asking whether older adults are represented proportionally to the population at large in AI data sets. We examine publicly-available information about 92 face data sets to understand how they codify age as a case study to investigate how the subjects' ages are recorded and whether older generations are represented. We find that older adults are very under-represented; five data sets in the study that explicitly documented the closed age intervals of their subjects included older adults (defined as older than 65 years), while only one included oldest-old adults (defined as older than 85 years). Additionally, we find that only 24 of the data sets include any age-related information in their documentation or metadata, and that there is no consistent method followed across these data sets to collect and record the subjects' ages. We recognize the unique difficulties in creating representative data sets in terms of age, but raise it as an important dimension that researchers and engineers interested in inclusive AI should consider.","ACMDL - AIES","Data Audit",466
"53","AIES","Meta-Commentary","Kasirzadeh, Cliffor","Fairness and Data Protection Impact Assessments","https://dl.acm.org/doi/10.1145/3461702.3462528","AIES",2021,"Ethics of Artificial Intelligence; Regulation of Artificial Intelligence; Fairness Principle; Algorithmic Fairness; General Data Protection Regulation; Data Protection Impact Assessments","In this paper, we critically examine the effectiveness of the requirement to conduct a Data Protection Impact Assessment (DPIA) in Article 35 of the General Data Protection Regulation (GDPR) in light of fairness metrics. Through this analysis, we explore the role of the fairness principle as introduced in Article 5(1)(a) and its multifaceted interpretation in the obligation to conduct a DPIA. Our paper argues that although there is a significant theoretical role for the considerations of fairness in the DPIA process, an analysis of the various guidance documents issued by data protection authorities on the obligation to conduct a DPIA reveals that they rarely mention the fairness principle in practice. Our analysis questions this omission, and assesses the capacity of fairness metrics to be truly operationalized within DPIAs. We conclude by exploring the practical effectiveness of DPIA with particular reference to (1) technical challenges that have an impact on the usefulness of DPIAs irrespective of a controller's willingness to actively engage in the process, (2) the context dependent nature of the fairness principle, and (3) the key role played by data controllers in the determination of what is fair.","ACMDL - AIES","Meta-Commentary",469
"54","AIES","Meta-Commentary","Barocas et al.","Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs","https://dl.acm.org/doi/10.1145/3461702.3462610","AIES",2021,"evaluations, disaggregated evaluations, fairness, artificial intelligence, machine learning","Disaggregated evaluations of AI systems, in which system performance is assessed and reported separately for different groups of people, are conceptually simple. However, their design involves a variety of choices. Some of these choices influence the results that will be obtained, and thus the conclusions that can be drawn; others influence the impacts---both beneficial and harmful---that a disaggregated evaluation will have on people, including the people whose data is used to conduct the evaluation. We argue that a deeper understanding of these choices will enable researchers and practitioners to design careful and conclusive disaggregated evaluations. We also argue that better documentation of these choices, along with the underlying considerations and tradeoffs that have been made, will help others when interpreting an evaluation's results and conclusions.","ACMDL - AIES","Meta-Commentary",470
"55","AIES","Meta-Commentary","Loi, Spielkamp","Towards Accountability in the Use of Artificial Intelligence for Public Administrations","https://dl.acm.org/doi/10.1145/3461702.3462631","AIES",2021,"accountability; artificial intelligence; public administrations; AI guidelines","We argue that the phenomena of distributed responsibility, induced acceptance, and acceptance through ignorance constitute instances of imperfect delegation when tasks are delegated to computationally-driven systems. Imperfect delegation challenges human accountability. We hold that both direct public accountability via public transparency and indirect public accountability via transparency to auditors in public organizations can be both instrumentally ethically valuable and required as a matter of deontology from the principle of democratic self-government. We analyze the regulatory content of 16 guideline documents about the use of AI in the public sector, by mapping their requirements to those of our philosophical account of accountability, and conclude that while some guidelines refer processes that amount to auditing, it seems that the debate would benefit from more clarity about the nature of the entitlement of auditors and the goals of auditing, also in order to develop ethically meaningful standards with respect to which different forms of auditing can be evaluated and compared.","ACMDL - AIES","Meta-Commentary",471
"56","AIES","Meta-Commentary","Watkins et al.","Governing Algorithmic Systems with Impact Assessments: Six Observations","https://dl.acm.org/doi/10.1145/3461702.3462580","AIES",2021,"algorithmic impact assessment; impact; harm; accountability; governance","Algorithmic decision-making and decision-support systems (ADS) are gaining influence over how society distributes resources, administers justice, and provides access to opportunities. Yet collectively we do not adequately study how these systems affect people or document the actual or potential harms resulting from their integration with important social functions. This is a significant challenge for computational justice efforts of measuring and governing AI systems. Impact assessments are often used as instruments to create accountability relationships and grant some measure of agency and voice to communities affected by projects with environmental, financial, and human rights ramifications. Applying these tools-through Algorithmic Impact Assessments (AIA)-is a plausible way to establish accountability relationships for ADSs. At the same time, what an AIA would entail remains under-specified; they raise as many questions as they answer. Choices about the methods, scope, and purpose of AIAs structure the conditions of possibility for AI governance. In this paper, we present our research on the history of impact assessments across diverse domains, through a sociotechnical lens, to present six observations on how they co-constitute accountability. Decisions about what type of effects count as an impact; when impacts are assessed; whose interests are considered; who is invited to participate; who conducts the assessment; how assessments are made publicly available, and what the outputs of the assessment might be; all shape the forms of accountability that AIAs engender. Because AlAs are still an incipient governance strategy, approaching them as social constructions that do not require a single or universal approach offers a chance to produce interventions that emerge from careful deliberation.","ACMDL - AIES","Meta-Commentary",472
"57","AIES","Meta-Commentary","Yona, Ghorbani, Zou","Who's Responsible? Jointly Quantifying the Contribution of the Learning Algorithm and Data","https://dl.acm.org/doi/10.1145/3461702.3462574","AIES",2021,"accountability, machine learning, fairness, data valuation","A learning algorithm A trained on a dataset D is revealed to have poor performance on some subpopulation at test time. Where should the responsibility for this lay? It can be argued that the data is responsible, if for example training A on a more representative dataset D' would have improved the performance. But it can similarly be argued that A itself is at fault, if training a different variant A' on the same dataset D would have improved performance. As ML becomes widespread and such failure cases more common, these types of questions are proving to be far from hypothetical. With this motivation in mind, in this work we provide a rigorous formulation of the joint credit assignment problem between a learning algorithm A and a dataset D. We propose Extended Shapley as a principled framework for this problem, and experiment empirically with how it can be used to address questions of ML accountability.","ACMDL - AIES","Method",476
"58","AIES","Case Study","Papakyriakopoulos et al.","How Algorithms Shape the Distribution of Political Advertising: Case Studies of Facebook, Google, and TikTok","https://dl.acm.org/doi/10.1145/3514094.3534166","AIES",2022,"interpretability, political speech, algorithmic auditing, accountability, political advertising, algorithmic targeting, regulation","Online platforms play an increasingly important role in shaping democracy by influencing the distribution of political information to the electorate. In recent years, political campaigns have spent heavily on the platforms' algorithmic tools to target voters with online advertising. While the public interest in understanding how platforms perform the task of shaping the political discourse has never been higher, the efforts of the major platforms to make the necessary disclosures to understand their practices falls woefully short. In this study, we collect and analyze a dataset containing over 800,000 ads and 2.5 million videos about the 2020 U.S. presidential election from Facebook, Google, and TikTok. We conduct the first large scale data analysis of public data to critically evaluate how these platforms amplified or moderated the distribution of political advertisements. We conclude with recommendations for how to improve the disclosures so that the public can hold the platforms and political advertisers accountable.","ACMDL - AIES","Case Study",441
"59","AIES","Case Study","Sapiezynski et al.","Algorithms that ""Don't See Color"": Measuring Biases in Lookalike and Special Ad Audiences","https://dl.acm.org/doi/10.1145/3514094.3534135","AIES",2022,"online advertising, fairness, process fairness","Researchers and journalists have repeatedly shown that algorithms commonly used in domains such as credit, employment, healthcare, or criminal justice can have discriminatory effects. Some organizations have tried to mitigate these effects by simply removing sensitive features from an algorithm's inputs. In this paper, we explore the limits of this approach using a unique opportunity. In 2019, Facebook agreed to settle a lawsuit by removing certain sensitive features from inputs of an algorithm that identifies users similar to those provided by an advertiser for ad targeting, making both the modified and unmodified versions of the algorithm available to advertisers. We develop methodologies to measure biases along the lines of gender, age, and race in the audiences created by this modified algorithm, relative to the unmodified one. Our results provide experimental proof that merely removing demographic features from a real-world algorithmic system's inputs can fail to prevent biased outputs. As a result, organizations using algorithms to help mediate access to important life opportunities should consider other approaches to mitigating discriminatory effects.","ACMDL - AIES","Case Study",442
"60","AIES","Case Study","Wolfe, Caliskan","American == White in Multimodal Language-and-Image AI","https://dl.acm.org/doi/10.1145/3514094.3534136","AIES",2022,"bias in AI, multimodal models, visual semantics, racial bias","Three state-of-the-art language-and-image AI models, CLIP, SLIP, and BLIP, are evaluated for evidence of a bias previously observed in social and experimental psychology: equating American identity with being White. Embedding association tests (EATs) using standardized images of self-identified Asian, Black, Latina/o, and White individuals from the Chicago Face Database (CFD) reveal that White individuals are more associated with collective in-group words than are Asian, Black, or Latina/o individuals, with effect sizes >.4 for White vs. Asian comparisons across all models. In assessments of three core aspects of American identity reported by social psychologists, single-category EATs reveal that images of White individuals are more associated with patriotism and with being born in America, but that, consistent with prior findings in psychology, White individuals are associated with being less likely to treat people of all races and backgrounds equally. Additional tests reveal that the number of images of Black individuals returned by an image ranking task is more strongly correlated with state-level implicit bias scores for White individuals (Pearson's ρ=.63 in CLIP, ρ=.69 in BLIP) than are state demographics (ρ=.60), suggesting a relationship between regional prototypicality and implicit bias. Three downstream machine learning tasks demonstrate biases associating American with White. In a visual question answering task using BLIP, 97% of White individuals are identified as American, compared to only 3% of Asian individuals. When asked in what state the individual depicted lives in, the model responds China 53% of the time for Asian individuals, but always with an American state for White individuals. In an image captioning task, BLIP remarks upon the race of Asian individuals as much as 36% of the time, and the race of Black individuals as much as 18% of the time, but never remarks upon race for White individuals. Finally, when provided with an initialization image of individuals from the CFD and the text ""an American person,"" a synthetic image generator (VQGAN) using the text-based guidance of CLIP consistently lightens the skin tone of individuals of all races (by 35% for Black individuals, based on mean pixel brightness), and generates output images of White individuals with blonde hair. The results indicate that societal biases equating American identity with being White are learned by multimodal language-and-image AI, and that these biases propagate to downstream applications of such models.","ACMDL - AIES","Case Study",443
"61","AIES","Case Study","Rhea et al.","Resume Format, LinkedIn URLs and Other Unexpected Influences on AI Personality Prediction in Hiring: Results of an Audit","https://dl.acm.org/doi/10.1145/3514094.3534189","AIES",2022,"algorithm audit, validity, stability, reliability, hiring, personality","Automated hiring systems are among the fastest-developing of all high-stakes AI systems. Among these are algorithmic personality tests that use insights from psychometric testing, and promise to surface personality traits indicative of future success based on job seekers' resumes or social media profiles. We interrogate the reliability of such systems using stability of the outputs they produce, noting that reliability is a necessary, but not a sufficient, condition for validity. We develop a methodology for an external audit of stability of algorithmic personality tests, and instantiate this methodology in an audit of two systems, Humantic AI and Crystal. Rather than challenging or affirming the assumptions made in psychometric testing -- that personality traits are meaningful and measurable constructs, and that they are indicative of future success on the job -- we frame our methodology around testing the underlying assumptions made by the vendors of the algorithmic personality tests themselves.","ACMDL - AIES","Case Study",461
"62","AIES","Meta-Commentary","Raji et al.","Outsider Oversight: Designing a Third Party Audit Ecosystem for AI Governance","https://dl.acm.org/doi/10.1145/3514094.3534181","AIES",2022,"auditing, policy, algorithms, accountability, society","Much attention has focused on algorithmic audits and impact assessments to hold developers and users of algorithmic systems accountable. But existing algorithmic accountability policy approaches have neglected the lessons from non-algorithmic domains: notably, the importance of third parties. Our paper synthesizes lessons from other fields on how to craft effective systems of external oversight for algorithmic deployments. First, we discuss the challenges of third party oversight in the current AI landscape. Second, we survey audit systems across domains - e.g., financial, environmental, and health regulation - and show that the institutional design of such audits are far from monolithic. Finally, we survey the evidence base around these design components and spell out the implications for algorithmic auditing. We conclude that the turn toward audits alone is unlikely to achieve actual algorithmic accountability, and sustained focus on institutional design will be required for meaningful third party involvement.","ACMDL - AIES","Meta-Commentary",468
"63","AIES","Meta-Commentary","Barnett, Diakopoulos","Crowdsourcing Impacts: Exploring the Utility of Crowds for Anticipating Societal Impacts of Algorithmic Decision Making","https://dl.acm.org/doi/10.1145/3514094.3534145","AIES",2022,"algorithmic impact, anticipatory ethics, natural language processing, cognitive diversity, participatory foresight","With the increasing pervasiveness of algorithms across industry and government, a growing body of work has grappled with how to understand their societal impact and ethical implications. Various methods have been used at different stages of algorithm development to encourage researchers and designers to consider the potential societal impact of their research. An understudied yet promising area in this realm is using participatory foresight to anticipate these different societal impacts. We employ crowdsourcing as a means of participatory foresight to uncover four different types of impact areas based on a set of governmental algorithmic decision making tools: (1) perceived valence, (2) societal domains, (3) specific abstract impact types, and (4) ethical algorithm concerns. Our findings suggest that this method is effective at leveraging the cognitive diversity of the crowd to uncover a range of issues. We further analyze the complexities within the interaction of the impact areas identified to demonstrate how crowdsourcing can illuminate patterns around the connections between impacts. Ultimately this work establishes crowdsourcing as an effective means of anticipating algorithmic impact which complements other approaches towards assessing algorithms in society by leveraging participatory foresight and cognitive diversity.","ACMDL - AIES","Method",474
"64","AIES","Meta-Commentary","Yew, Hadfield-Menell","A Penalty Default Approach to Preemptive Harm Disclosure and Mitigation for AI Systems","https://dl.acm.org/doi/10.1145/3514094.3534130","AIES",2022,"artificial intelligence law, technology policy, computing and society","As AI industry matures, it is important to ensure that the organizations developing these systems have sufficient incentives to identify and mitigate risks and harm. Unfortunately, the profit motive is often misaligned with this goal. Successful work to identify or reduce risk rarely has direct tangible benefits. In this paper, we consider the use of regulatory penalty defaults as a way to counter these perverse incentives. A regulatory penalty default regime consists of two parts: a regulatory penalty default and a mechanism to bargain around the default. The regulatory penalty default induces private actors to research and mitigate potential harms in order to limit liability, making the benefits of risk mitigation tangible. The bargaining mechanism provides incentives for companies to go beyond achieving a prescriptive threshold of compliance in creating a compelling case for escape from the default. With a focus on the policy landscape in the United States, we propose and discuss potential regulatory penalty default regimes for AI systems. For each of our proposals, we also discuss accompanying regulatory pathways for the bargaining process. While regulatory penalty default regimes are not a panacea (we discuss several drawbacks of the proposed methods), they are an important tool to consider in the regulation of AI systems.","ACMDL - AIES","Method",475
"65","ACMDL - Assurance","Case Study","Sen, Amartya and Madria, Sanjay","Data Analysis of Cloud Security Alliance's Security, Trust & Assurance Registry","https://doi.org/10.1145/3154273.3154343","ICDCN '18",2018,"CSA STAR, Cloud Computing, Data Analyses, Risk Assessment","The security of clients' applications on the cloud platforms has been of great interest. Security concerns associated with cloud computing are improving in both the domains; security issues faced by cloud providers and security issues faced by clients. However, security concerns still remain in domains like cloud auditing and migrating application components to cloud to make the process more secure and cost-efficient. To an extent, this can be attributed to a lack of detailed information being publicly present about the cloud platforms and their security policies. A resolution in this regard can be found in Cloud Security Alliance's Security, Trust, and Assurance Registry (STAR) which documents the security controls provided by popular cloud computing offerings. In this paper, we perform some descriptive analysis on STAR data in an attempt to comprehend the information publicly presented by different cloud providers. It is to help clients in more effectively searching and analyzing the required security information they need for the decision making process for hosting their applications on cloud. Based on the analysis, we outline some augmentations that can be made to STAR as well as certain specific design improvements for a cloud migration risk assessment framework.","ACMDL - Assurance","Case Study",19
"66","ACMDL - Assurance","Meta-Commentary","Bogner, Justus and Fritzsch, Jonas and Wagner, Stefan and Zimmermann, Alfred","Limiting Technical Debt with Maintainability Assurance: An Industry Survey on Used Techniques and Differences with Service- and Microservice-Based Systems","https://doi.org/10.1145/3194164.3194166","TechDebt '18",2018,"industry, microservice-based systems, maintainability, service-based systems, survey, software quality control","Maintainability assurance techniques are used to control this quality attribute and limit the accumulation of potentially unknown technical debt. Since the industry state of practice and especially the handling of Service- and Microservice-Based Systems in this regard are not well covered in scientific literature, we created a survey to gather evidence for a) used processes, tools, and metrics in the industry, b) maintainability-related treatment of systems based on service-orientation, and c) influences on developer satisfaction w.r.t. maintainability. 60 software professionals responded to our online questionnaire. The results indicate that using explicit and systematic techniques has benefits for maintainability. The more sophisticated the applied methods the more satisfied participants were with the maintainability of their software while no link to a hindrance in productivity could be established. Other important findings were the absence of architecture-level evolvability control mechanisms as well as a significant neglect of service-oriented particularities for quality assurance. The results suggest that industry has to improve its quality control in these regards to avoid problems with long-living service-based software systems.","ACMDL - Assurance","Method",24
"67","ACMDL - Assurance","Case Study","Srinivasan, Madhusudan and Shahri, Morteza Pourreza and Kahanda, Indika and Kanewala, Upulee","Quality Assurance of Bioinformatics Software: A Case Study of Testing a Biomedical Text Processing Tool Using Metamorphic Testing","https://doi.org/10.1145/3193977.3193981","MET '18",2018,"biomedical natural language processing, software testing, metamorphic testing, bioinformatics","Bioinformatics software plays a very important role in making critical decisions within many areas including medicine and health care. However, most of the research is directed towards developing tools, and little time and effort is spent on testing the software to assure its quality. In testing, a test oracle is used to determine whether a test is passed or failed during testing, and unfortunately, for much of bioinformatics software, the exact expected outcomes are not well defined. Thus, the main challenge associated with conducting systematic testing on bioinformatics software is the oracle problem.Metamorphic testing (MT) is a technique used to test programs that face the oracle problem. MT uses metamorphic relations (MRs) to determine whether a test has passed or failed and specifies how the output should change according to a specific change made to the input. In this work, we use MT to test LingPipe, a tool for processing text using computational linguistics, often used in bioinformatics for bio-entity recognition from biomedical literature.First, we identify a set of MRs for testing any bio-entity recognition program. Then we develop a set of test cases that can be used to test LingPipe's bio-entity recognition functionality using these MRs. To evaluate the effectiveness of this testing process, we automatically generate a set of faulty versions of LingPipe. According to our analysis of the experimental results, we observe that our MRs can detect the majority of these faulty versions, which shows the utility of this testing technique for quality assurance of bioinformatics software.","ACMDL - Assurance","Case Study",40
"68","ACMDL - Assurance","Case Study","Andrus, McKane and Gilbert, Thomas K.","Towards a Just Theory of Measurement: A Principled Social Measurement Assurance Program for Machine Learning","https://doi.org/10.1145/3306618.3314275","AIES '19",2019,"machine learning, representational measurement, institutional decision-making, measurement assurance, justice, rawls, fairness, measurement","While formal definitions of fairness in machine learning (ML) have been proposed, its place within a broader institutional model of fair decision-making remains ambiguous. In this paper we interpret ML as a tool for revealing when and how measures fail to capture purported constructs of interest, augmenting a given institution's understanding of its own interventions and priorities. Rather than codifying ""fair"" principles into ML models directly, the use of ML can thus be understood as a form of quality assurance for existing institutions, exposing the epistemic fault lines of their own measurement practices. Drawing from Friedler et al's [2016] recent discussion of representational mappings and previous discussions on the ontology of measurement, we propose a social measurement assurance program (sMAP) in which ML encourages expert deliberation on a given decision-making procedure by examining unanticipated or previously unexamined covariates. As an example, we apply Rawlsian principles of fairness to sMAP and produce a provisional just theory of measurement that would guide the use of ML for achieving fairness in the case of child abuse in Allegheny County.","ACMDL - Assurance","Case Study",26
"69","ACMDL - Assurance","Case Study","Babikian, Aren A.","Automated Generation of Test Scenario Models for the System-Level Safety Assurance of Autonomous Vehicles","https://doi.org/10.1145/3417990.3419484","MODELS '20",2020,"automated test generation, graph model synthesis, verification & validation, system-level testing, autonomous vehicles","Autonomous vehicles controlled by advanced machine learning techniques are significantly gaining in popularity. However, the safety engineering practices currently used in such vehicles are not capable of justifying that AI techniques would prevent unsafe situations with a designated level of confidence and reliability. One related challenge is the perpetually changing environment that autonomous vehicles must interact with, which must be taken into consideration when deriving test suites for their safety assurance. As a result, a common approach for testing autonomous vehicles involves subjecting them to test scenarios and evaluating their system-level quality of service. As it stands, such system-level testing approaches do exist but only at a prototypical and conceptual level: these approaches cannot handle complex system-level traffic scenarios and related coverage criteria. I plan to address this challenge through my PhD studies by (1) defining situation coverage as an abstract coverage criteria for autonomous vehicle testing, (2) evaluating situation coverage of existing test suites obtained by off-the-shelf simulation tools, and (3) proposing a test suite generation approach that provides test scenarios with increasing situation coverage as output.","ACMDL - Assurance","Case Study",36
"70","ACMDL - Assurance","Case Study","Scheerer, Max and Klamroth, Jonas and Reussner, Ralf and Beckert, Bernhard","Towards Classes of Architectural Dependability Assurance for Machine-Learning-Based Systems","https://doi.org/10.1145/3387939.3388613","SEAMS '20",2020,"software quality, artificial intelligence, dependability, architectural-driven self-adaptation, machine learning","Advances in Machine Learning (ML) have brought previously hard to handle problems within arm's reach. However, this power comes at the cost of unassured reliability and lacking transparency. Overcoming this drawback is very hard due to the probabilistic nature of ML. Current approaches mainly tackle this problem by developing more robust learning procedures. Such algorithmic approaches, however, are limited to certain types of uncertainties and cannot deal with all of them, e.g., hardware failure. This paper discusses how this problem can be addressed at architectural rather than algorithmic level to assess systems dependability properties in early development stages. Moreover, we argue that Self-Adaptive Systems (SAS) are more suited to safeguard ML w.r.t. various uncertainties. As a step towards this we propose classes of dependability in which ML-based systems may be categorized and discuss which and how assurances can be made for each class.","ACMDL - Assurance","Case Study",52
"71","ACMDL - Assurance","Meta-Commentary","Borum, Holger Stadel and Seidl, Christoph and Sestoft, Peter","Co-Designing DSL Quality Assurance Measures for and with Non-Programming Experts","https://doi.org/10.1145/3486603.3486776","DSM 2021",2021,"domain-specific language, co-design","Domain-specific languages seek to provide domain guarantees that eliminate many errors allowed by general-purpose languages. Still, a domain-specific language requires additional quality assurance measures to ensure that specifications behave as intended by the users. However, some domains may have specific quality assurance measures (e.g., proofs, experiments, or case studies) with little tradition of using quality assurance measures customary to software engineering. We investigate the possibility of accommodating such domains by conducting a workshop with 11 prospective users of a domain-specific language named MAL for the pension industry. The workshop emphasised the need for supporting actuaries with new analytical tools for quality assurance and resulted in three designs: quantity monitors let users identify outlier behaviour, fragment debugging lets users debug with limited evaluative power, and debugging spreadsheets let users visualise, analyse, and remodel concrete calculations with an established domain tool. Based on our experiences, we hypothesise that co-design workshops are a viable approach for DSLs in a similar situation.","ACMDL - Assurance","Method",18
"72","ACMDL - Assurance","Ecosystem","Hofbauer, Stefan and Quirchmayr, Gerald","Assuring Long-Term Operational Resilience in a Pandemic: Lessons Learned from COVID-19","https://doi.org/10.1145/3468784.3470466","IAIT2021",2021,"COVID-19, risk management, operational resilience, KRI, controls, BCM","The COVID-19 pandemic has shown that some companies have been prepared for the pandemic in terms of crisis management, but other companies have not been prepared at all. The dependency of a company on third-party provider is even bigger in a pandemic situation. Operational resilience must be assured for third-party providers, who are supporting the company in delivering critical business processes. In a pandemic, the risk is much bigger that a third-party provider is having economical or employee-related issues, for example financial problems or loss of staff so that the provider will not be able to support the company on the same level as before the pandemic or cannot support the company at all. To assure operational resilience within a company, it is needed to first identify the critical IT assets and critical processes within the company. Only then it is possible to protect these IT assets and assure the business continuity of the critical business processes. Results described in this paper are based on practical experiences gained during the COVID-19 crisis.","ACMDL - Assurance","Ecosystem",54
"73","ACMDL - Assurance","Ecosystem","Bekemeier, Felix","Deceptive Assurance? A Conceptual View on Systemic Risk in Decentralized Finance (DeFi)","https://doi.org/10.1145/3510487.3510499","ICBTA 2021",2022,"Systemic risk, Blockchain technology, Decentralized Finance","The Decentralized Finance (DeFi) ecosystem has recently been touted as a potential replacement for the existing financial system, with the monetary equivalent in this ecosystem based on various token concepts and infrastructural protocols. However, questions remain regarding the systemic risk of this ecosystem, and closer examination reveals interesting parallels to the concept of systemic risk in established financial systems. There is a need for research to examine important additional dimensions in relation to DeFi. This paper addresses systemic risk in DeFi, presenting the first holistic research framework on the topic, as well as the first empirical indications in order to create foundations for further research.","ACMDL - Assurance","Ecosystem",4
"74","ACMDL - Assurance","Case Study","Borg, Markus and Bengtsson, Johan and \""Osterling, Harald and Hagelborn, Alexander and Gagner, Isabella and Tomaszewski, Piotr","Quality Assurance of Generative Dialog Models in an Evolving Conversational Agent Used for Swedish Language Practice","https://doi.org/10.1145/3522664.3528592","CAIN '22",2022,"conversational agent, requirements engineering, action research, generative dialog model, software testing, AI quality","Due to the migration megatrend, efficient and effective second-language acquisition is vital. One proposed solution involves AI-enabled conversational agents for person-centered interactive language practice. We present results from ongoing action research targeting quality assurance of proprietary generative dialog models trained for virtual job interviews. The action team elicited a set of 38 requirements for which we designed corresponding automated test cases for 15 of particular interest to the evolving solution. Our results show that six of the test case designs can detect meaningful differences between candidate models. While quality assurance of natural language processing applications is complex, we provide initial steps toward an automated framework for machine learning model selection in the context of an evolving conversational agent. Future work will focus on model selection in an MLOps setting.","ACMDL - Assurance","Case Study",20
"75","ACMDL - Audit","Case Study","Burel S","Gender Audit: Linguistic Approach to Gender Stereotypes in Online Communication","https://doi.org/10.1145/3196839.3196849;http://dx.doi.org/10.1145/3196839.3196849","GenderIT '18",2018,"sprache, gender, gender audit, language","Studies from psychology and linguistics show that gender is perceived stereotypically in oral and textual communication as a fixed connection with distinct meanings. The study wants to find out its materialization on word level in online communication and identify reliable patterns for a keyword-based gender audit.","ACMDL - Audit","Case Study",43
"76","ACMDL - Audit","Meta-Commentary","Keyes O,Hutson J,Durbin M","A Mulching Proposal: Analysing and Improving an Algorithmic System for Turning the Elderly into High-Nutrient Slurry","https://doi.org/10.1145/3290607.3310433;http://dx.doi.org/10.1145/3290607.3310433","CHI EA '19",2019,"dystopia, accountability, fairness, algorithmic analysis, computer vision, transparency, ethics, algorithmic critique","The ethical implications of algorithmic systems have been much discussed in both HCI and the broader community of those interested in technology design, development and policy. In this paper, we explore the application of one prominent ethical framework-Fairness, Accountability, and Transparency-to a proposed algorithm that resolves various societal issues around food security and population ageing. Using various standardised forms of algorithmic audit and evaluation, we drastically increase the algorithm's adherence to the FAT framework, resulting in a more ethical and beneficent system. We discuss how this might serve as a guide to other researchers or practitioners looking to ensure better ethical outcomes from algorithmic systems in their line of work.","ACMDL - Audit","Method",13
"77","ACMDL - Audit","Case Study","Awadhutkar P,Santhanam GR,Holland B,Kothari S","DISCOVER: Detecting Algorithmic Complexity Vulnerabilities","https://doi.org/10.1145/3338906.3341177;http://dx.doi.org/10.1145/3338906.3341177","ESEC/FSE 2019",2019,"Cybersecurity, Algorithmic Complexity Vulnerability, Software Analysis","Algorithmic Complexity Vulnerabilities (ACV) are a class of vulnerabilities that enable Denial of Service Attacks. ACVs stem from asymmetric consumption of resources due to complex loop termination logic, recursion, and/or resource intensive library APIs. Completely automated detection of ACVs is intractable and it calls for tools that assist human analysts. We present DISCOVER, a suite of tools that facilitates human-on-the-loop detection of ACVs. DISCOVER's workflow can be broken into three phases - (1) Automated characterization of loops, (2) Selection of suspicious loops, and (3) Interactive audit of selected loops. We demonstrate DISCOVER using a case study using a DARPA challenge app. DISCOVER supports analysis of Java source code and Java bytecode. We demonstrate it for Java bytecode.","ACMDL - Audit","Case Study",25
"78","ACMDL - Audit","Case Study","Lee CS,Du J,Guerzhoy M","Auditing the COMPAS Recidivism Risk Assessment Tool: Predictive Modelling and Algorithmic Fairness in CS1","https://doi.org/10.1145/3341525.3393998;http://dx.doi.org/10.1145/3341525.3393998","ITiCSE '20",2020,"algorithmic fairness, data science, predictive modelling, CS1","We present an assignment in which students apply predictive modelling to build a model that predicts re-arrest of criminal defendants using real data. Students assess the algorithmic fairness of a real-world criminal risk assessment tool (RAT), and reproduce results from an impactful story in ProPublica and a 2018 Science Advances paper. Students explore different measures of algorithmic fairness, and adjust the model they build to satisfy the false positive parity measure. Our target audience is students in Introduction to Data Science courses that do not require previous computing experience, as well as students in standard CS1 courses. We advocate for teaching predictive modelling in CS1. To facilitate the teaching of predictive modelling in CS1, we provide tutorials on predictive modelling and algorithmic fairness, in both Python and Java; we also provide a simplified ""Learning Machine"" API in those languages.Our approach enables teaching algorithmic fairness and predictive modelling more generally very early in the students' computing career. A companion website with all our teaching materials is available at https://PredictiveModellingEarly.github.io/.","ACMDL - Audit","Case Study",28
"79","ACMDL - Audit","Meta-Commentary","Escher N,Banovic N","Exposing Error in Poverty Management Technology: A Method for Auditing Government Benefits Screening Tools","https://doi.org/10.1145/3392874;http://dx.doi.org/10.1145/3392874","CSCW",2020,"algorithmic audit, e-government, automated decision systems","Public benefits programs help people afford necessities like food, housing, and healthcare. In the US, such programs are means-tested: applicants must complete long forms to prove financial distress before receiving aid. Online benefits screening tools provide a gloss of such forms, advising households about their eligibility prior to completing full applications. If incorrectly implemented, screening tools may discourage qualified households from applying for benefits. Unfortunately, errors in screening tools are difficult to detect because they surface one at a time and difficult to contest because unofficial determinations do not generate a paper trail. We introduce a method for auditing such tools in four steps: 1) generate test households, 2) automatically populate screening questions with household information and retrieve determinations, 3) translate eligibility guidelines into computer code to generate ground truth determinations, and 4) identify conflicting determinations to detect errors. We illustrated our method on a real screening tool with households modeled from census data. Our method exposed major errors with corresponding examples to reproduce them. Our work provides a necessary corrective to an already arduous benefits application process.","ACMDL - Audit","Method",61
"80","ACMDL - Audit","Case Study","Hussein E,Juneja P,Mitra T","Measuring Misinformation in Video Search Platforms: An Audit Study on YouTube","https://doi.org/10.1145/3392854;http://dx.doi.org/10.1145/3392854",NA,2020,"conspiracy theory, misinformation, misinformation audit, algorithmic audit, group fairness, search engines, information retrieval","Search engines are the primary gateways of information. Yet, they do not take into account the credibility of search results. There is a growing concern that YouTube, the second largest search engine and the most popular video-sharing platform, has been promoting and recommending misinformative content for certain search topics. In this study, we audit YouTube to verify those claims. Our audit experiments investigate whether personalization (based on age, gender, geolocation, or watch history) contributes to amplifying misinformation. After shortlisting five popular topics known to contain misinformative content and compiling associated search queries representing them, we conduct two sets of audits-Search-and Watch-misinformative audits. Our audits resulted in a dataset of more than 56K videos compiled to link stance (whether promoting misinformation or not) with the personalization attribute audited. Our videos correspond to three major YouTube components: search results, Up-Next, and Top 5 recommendations. We find that demographics, such as, gender, age, and geolocation do not have a significant effect on amplifying misinformation in returned search results for users with brand new accounts. On the other hand, once a user develops a watch history, these attributes do affect the extent of misinformation recommended to them. Further analyses reveal a filter bubble effect, both in the Top 5 and Up-Next recommendations for all topics, except vaccine controversies; for these topics, watching videos that promote misinformation leads to more misinformative video recommendations. In conclusion, YouTube still has a long way to go to mitigate misinformation on its platform.","ACMDL - Audit","Case Study",62
"81","ACMDL - Audit","Case Study","Bartley N,Abeliuk A,Ferrara E,Lerman K","Auditing Algorithmic Bias on Twitter","https://doi.org/10.1145/3447535.3462491;http://dx.doi.org/10.1145/3447535.3462491","WebSci '21",2021,"social networks, algorithmic bias, black-box recommender systems","Digital media platforms are reshaping our habits, how we access information, and how we interact with others. As a result, algorithms used by platforms, for example, to recommend content, play an increasingly important role in our access to information. Due to practical difficulties of accessing how platforms present content to their users, relatively little is known about how recommendation algorithms affect the information people receive. In this paper we implement a sock-puppet audit, a computational framework to audit black-box social media systems so as to quantify the impact of algorithmic curation on the information people see. We evaluate this framework by conducting a study on Twitter. We demonstrate that Twitter’s timeline curation algorithms skew the popularity and novelty of content people see and increase the inequality of their exposure to friends’ tweets. Our work provides evidence that algorithmic curation of content systematically distorts the information people see.","ACMDL - Audit","Case Study",1
"82","ACMDL - Audit","Case Study","Zwiebelmann Z,Henderson T","Data Portability as a Tool for Audit","https://doi.org/10.1145/3460418.3479343;http://dx.doi.org/10.1145/3460418.3479343","UbiComp '21",2021,"Data Portability, Audit, GDPR, Activity-Tracking","Pervasive systems are almost omnipresent in their collection and processing of personal data. Understanding what these systems are doing is essential for trust, and to ensure that data being collected are accurate. Auditing these systems can help to determine the accuracy of these data. Such audit may take place internally by systems designers, but external audit is important for accountability. In this paper we explore whether users can conduct their own external audit of the systems with which they interact. In particular, we use the Right to Data Portability afforded to data subjects through the General Data Protection Regulation. Using fitness trackers, we collect and upload running data to a set of data controllers. By using data portability to then obtain a copy of our data, we compare the data held by the controllers with our ground-truth data. We find some inaccuracies in the data, but also that audit can be impeded by insufficient explanations from data controllers.","ACMDL - Audit","Case Study",11
"83","ACMDL - Audit","Case Study","Juneja P,Mitra T","Auditing E-Commerce Platforms for Algorithmically Curated Vaccine Misinformation","https://doi.org/10.1145/3411764.3445250;http://dx.doi.org/10.1145/3411764.3445250","CHI '21",2021,"recommendations, vaccine misinformation, search engines, personalization, search results, algorithmic bias, algorithmic audits, health misinformation, e-commerce platforms","There is a growing concern that e-commerce platforms are amplifying vaccine-misinformation. To investigate, we conduct two-sets of algorithmic audits for vaccine misinformation on the search and recommendation algorithms of Amazon—world’s leading e-retailer. First, we systematically audit search-results belonging to vaccine-related search-queries without logging into the platform—unpersonalized audits. We find 10.47% of search-results promote misinformative health products. We also observe ranking-bias, with Amazon ranking misinformative search-results higher than debunking search-results. Next, we analyze the effects of personalization due to account-history, where history is built progressively by performing various real-world user-actions, such as clicking a product. We find evidence of filter-bubble effect in Amazon’s recommendations; accounts performing actions on misinformative products are presented with more misinformation compared to accounts performing actions on neutral and debunking products. Interestingly, once user clicks on a misinformative product, homepage recommendations become more contaminated compared to when user shows an intention to buy that product.","ACMDL - Audit","Case Study",31
"84","ACMDL - Audit","Case Study","Shen H,DeVos A,Eslami M,Holstein K","Everyday Algorithm Auditing: Understanding the Power of Everyday Users in Surfacing Harmful Algorithmic Behaviors","https://doi.org/10.1145/3479577;http://dx.doi.org/10.1145/3479577",NA,2021,"everyday algorithm auditing, auditing algorithms, algorithmic bias, fair machine learning, everyday users","A growing body of literature has proposed formal approaches to audit algorithmic systems for biased and harmful behaviors. While formal auditing approaches have been greatly impactful, they often suffer major blindspots, with critical issues surfacing only in the context of everyday use once systems are deployed. Recent years have seen many cases in which everyday users of algorithmic systems detect and raise awareness about harmful behaviors that they encounter in the course of their everyday interactions with these systems. However, to date little academic attention has been granted to these bottom-up, user-driven auditing processes. In this paper, we propose and explore the concept of everyday algorithm auditing, a process in which users detect, understand, and interrogate problematic machine behaviors via their day-to-day interactions with algorithmic systems. We argue that everyday users are powerful in surfacing problematic machine behaviors that may elude detection via more centrally-organized forms of auditing, regardless of users' knowledge about the underlying algorithms. We analyze several real-world cases of everyday algorithm auditing, drawing lessons from these cases for the design of future platforms and tools that facilitate such auditing behaviors. Finally, we discuss work that lies ahead, toward bridging the gaps between formal auditing approaches and the organic auditing behaviors that emerge in everyday use of algorithmic systems.","ACMDL - Audit","Case Study",64
"85","ACMDL - Audit","Case Study","Haak F,Schaer P","Auditing Search Query Suggestion Bias Through Recursive Algorithm Interrogation","https://doi.org/10.1145/3501247.3531567;http://dx.doi.org/10.1145/3501247.3531567","WebSci '22",2022,"Algorithm Audit, Search Query Suggestion, Bias, Web Search Engines","Despite their important role in online information search, search query suggestions have not been researched as much as most other aspects of search engines. Although reasons for this are multi-faceted, the sparseness of context and the limited data basis of up to ten suggestions per search query pose the most significant problem in identifying bias in search query suggestions. The most proven method to reduce sparseness and improve the validity of bias identification of search query suggestions so far is to consider suggestions from subsequent searches over time for the same query. This work presents a new, alternative approach to search query bias identification that includes less high-level suggestions to deepen the data basis of bias analyses. We employ recursive algorithm interrogation techniques and create suggestion trees that enable access to more subliminal search query suggestions. Based on these suggestions, we investigate topical group bias in person-related searches in the political domain.","ACMDL - Audit","Case Study",2
"86","ACMDL - Audit","Case Study","Black E,Elzayn H,Chouldechova A,Goldin J,Ho D","Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models","https://doi.org/10.1145/3531146.3533204;http://dx.doi.org/10.1145/3531146.3533204","FAccT '22",2022,NA,"This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity—appropriately accounting for relevant differences across individuals—which is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods—as opposed to simpler models—shapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.","ACMDL - Audit","Case Study",8
"87","ACMDL - Audit","Case Study","Bandy J","Inspecting Algorithmic Flows: Ethics, Transparency, and Accountability for Digital Mass Communication Platforms","https://doi.org/10.1145/3514094.3539538;http://dx.doi.org/10.1145/3514094.3539538","AIES '22",2022,"computers and society, algorithm auditing, platform accountability","With billions of users, algorithmic platforms like Facebook, Twitter, and TikTok are among the most influential communication technologies in the history of civilization. These platforms offer society a mixed bag of benefits and harms, but much remains ambiguous about their true dynamics, effects, and interworkings. As a step toward improved accountability, this document outlines a dissertation that inspects (i.e. audits) algorithmic platforms as mass communication flows. Broadly, I aim to characterize the temporal dynamics, content quality, and sustainability of four different platforms (Apple News, Google Ads, Twitter, and Facebook).","ACMDL - Audit","Case Study",33
"88","ACMDL - Audit","Ecosystem","DeVos A,Dhabalia A,Shen H,Holstein K,Eslami M","Toward User-Driven Algorithm Auditing: Investigating Users’ Strategies for Uncovering Harmful Algorithmic Behavior","https://doi.org/10.1145/3491102.3517441;http://dx.doi.org/10.1145/3491102.3517441","CHI '22",2022,"Algorithmic Harm, Search, Sensemaking, Auditing Algorithms, Fair Machine Learning, Qualitative Methods, User-Driven Algorithm Auditing, Collective Work, Algorithmic Bias","Recent work in HCI suggests that users can be powerful in surfacing harmful algorithmic behaviors that formal auditing approaches fail to detect. However, it is not well understood how users are often able to be so effective, nor how we might support more effective user-driven auditing. To investigate, we conducted a series of think-aloud interviews, diary studies, and workshops, exploring how users find and make sense of harmful behaviors in algorithmic systems, both individually and collectively. Based on our findings, we present a process model capturing the dynamics of and influences on users’ search and sensemaking behaviors. We find that 1) users’ search strategies and interpretations are heavily guided by their personal experiences with and exposures to societal bias; and 2) collective sensemaking amongst multiple users is invaluable in user-driven algorithm audits. We offer directions for the design of future methods and tools that can better support user-driven auditing.","ACMDL - Audit","Ecosystem",34
"89","ACMDL - Audit","Meta-Commentary","Park S,Kim S,Lim YS","Fairness Audit of Machine Learning Models with Confidential Computing","https://doi.org/10.1145/3485447.3512244;http://dx.doi.org/10.1145/3485447.3512244","WWW '22",2022,"Fairness, Algorithmic audit, Security and privacy, Confidential computing","Algorithmic discrimination is one of the significant concerns in applying machine learning models to a real-world system. Many researchers have focused on developing fair machine learning algorithms without discrimination based on legally protected attributes. However, the existing research has barely explored various security issues that can occur while evaluating model fairness and verifying fair models. In this study, we propose a fairness audit framework that assesses the fairness of ML algorithms while addressing potential security issues such as data privacy, model secrecy, and trustworthiness. To this end, our proposed framework utilizes confidential computing and builds a chain of trust through enclave attestation primitives combined with public scrutiny and state-of-the-art software-based security techniques, enabling fair ML models to be securely certified and clients to verify a certified one. Our micro-benchmarks on various ML models and real-world datasets show the feasibility of the fairness certification implemented with Intel SGX in practice. In addition, we analyze the impact of data poisoning, which is an additional threat during data collection for fairness auditing. Based on the analysis, we illustrate the theoretical curves of fairness gap and minimal group size and the empirical results of fairness certification on poisoned datasets.","ACMDL - Audit","Method",48
"90","ACMDL - Audit","Case Study","Seidelin C,Moreau T,Shklovski I,Holten Møller N","Auditing Risk Prediction of Long-Term Unemployment","https://doi.org/10.1145/3492827;http://dx.doi.org/10.1145/3492827",NA,2022,"public services, accountability, algorithm, job placement, audit","As more and more governments adopt algorithms to support bureaucratic decision-making processes, it becomes urgent to address issues of responsible use and accountability. We examine a contested public service algorithm used in Danish job placement for assessing an individual's risk of long-term unemployment. The study takes inspiration from cooperative audits and was carried out in dialogue with the Danish unemployment services agency. Our audit investigated the practical implementation of algorithms. We find (1) a divergence between the formal documentation and the model tuning code, (2) that the algorithmic model relies on subjectivity, namely the variable which focus on the individual's self-assessment of how long it will take before they get a job, (3) that the algorithm uses the variable ""origin"" to determine its predictions, and (4) that the documentation neglects to consider the implications of using variables indicating personal characteristics when predicting employment outcomes. We discuss the benefits and limitations of cooperative audits in a public sector context. We specifically focus on the importance of collaboration across different public actors when investigating the use of algorithms in the algorithmic society.","ACMDL - Audit","Case Study",63
"91","ACMDL - Bias","Case Study","Andriyansah, Redy and Bukhari, Syed Saqib and Jenckel, Martin and Dengel, Andreas","Using Balanced Training to Minimize Biased Classification","https://doi.org/10.1145/3352631.3352639","HIP '19",2019,"biased performance, balanced training, document zone","In this paper, we classify semantic zone in a document image and observe how a balanced training influences the classification performance. Unlike holistic document which normally distinguishes in content and structural layout, semantic zone introduces stronger inter-class ambiguity as it loses layout feature. Zone extraction from documents often results in unbalanced class distribution. Our experiment shows that training on such data leads to a biased classification. We classify semantic zone by using AlexNet which is a Convolutional Neural Network (CNN). It works on 3 corpora: University of Washington (UW) III, German historical document images (OCRD), and combination of both data sets. Because zone distribution is heavily unbalanced, we augment the data and balance the training distribution to prevent over expression by major classes. To maintain accuracy, we adopt transfer learning from larger document corpus (RVLCDIP). Besides deep learning, we also use heuristic approach to compare performance between balanced and unbalanced training. The result shows that balanced training can alleviate biased performance.","ACMDL - Bias","Case Study",45
"92","ACMDL - Bias","Case Study","Badjatiya, Pinkesh and Gupta, Manish and Varma, Vasudeva","Stereotypical Bias Removal for Hate Speech Detection Task Using Knowledge-Based Generalizations","https://doi.org/10.1145/3308558.3313504","WWW '19",2019,"bias removal, stereotypical bias, hate speech, knowledge-based generalization, bias detection, natural language processing","With the ever-increasing cases of hate spread on social media platforms, it is critical to design abuse detection mechanisms to pro-actively avoid and control such incidents. While there exist methods for hate speech detection, they stereotype words and hence suffer from inherently biased training. Bias removal has been traditionally studied for structured datasets, but we aim at bias mitigation from unstructured text data. In this paper, we make two important contributions. First, we systematically design methods to quantify the bias for any model and propose algorithms for identifying the set of words which the model stereotypes. Second, we propose novel methods leveraging knowledge-based generalizations for bias-free learning. Knowledge-based generalization provides an effective way to encode knowledge because the abstraction they provide not only generalizes content but also facilitates retraction of information from the hate speech detection classifier, thereby reducing the imbalance. We experiment with multiple knowledge generalization policies and analyze their effect on general performance and in mitigating bias. Our experiments with two real-world datasets, a Wikipedia Talk Pages dataset (WikiDetox) of size ~ 96k and a Twitter dataset of size ~ 24k, show that the use of knowledge-based generalizations results in better performance by forcing the classifier to learn from generalized content. Our methods utilize existing knowledge-bases and can easily be extended to other tasks.","ACMDL - Bias","Case Study",56
"93","ACMDL - Bias","Case Study","Babaeianjelodar, Marzieh and Lorenz, Stephen and Gordon, Josh and Matthews, Jeanna and Freitag, Evan","Quantifying Gender Bias in Different Corpora","https://doi.org/10.1145/3366424.3383559","WWW '20",2020,"BERT, natural language processing, gender bias, datasets","Word embedding models have been shown to be effective in performing a wide variety of Natural Language Processing (NLP) tasks such as identifying audiences for web advertisements, parsing resum\'es to select promising job candidates, and translating documents from one language to another. However, it has been demonstrated that NLP systems learn gender bias from the corpora of documents on which they are trained. It is increasingly common for pre-trained models to be used as a starting point for building applications in a wide range of areas including critical decision making applications. It is also very easy to use a pre-trained model as the basis for a new application without careful consideration of the original nature of the training set. In this paper, we quantify the degree to which gender bias differs with the corpora used for training. We look especially at the impact of starting with a pre-trained model and fine-tuning with additional data. Specifically, we calculate a measure of direct gender bias on several pre-trained models including BERTâ€™s Wikipedia and Book corpus models as well as on several fine-tuned General Language Understanding Evaluation (GLUE) benchmarks. In addition, we evaluate the bias from several more extreme corpora including the Jigsaw identity toxic dataset that includes toxic speech biased against race, gender, religion, and disability and the RtGender dataset that includes speech specifically labelled by gender. Our results reveal that the direct gender bias of the Jigsaw toxic identity dataset is surprisingly close to that of the base pre-trained Google model, but the RtGender dataset has significantly higher direct gender bias than the base model. When the bias learned by an NLP system can vary significantly with the corpora used for training, it becomes important to consider and report these details, especially for use in critical decision-making applications.","ACMDL - Bias","Case Study",12
"94","ACMDL - Bias","Case Study","Wan, Mengting and Ni, Jianmo and Misra, Rishabh and McAuley, Julian","Addressing Marketing Bias in Product Recommendations","https://doi.org/10.1145/3336191.3371855","WSDM '20",2020,"marketing bias, recommender systems, machine learning fairness","Modern collaborative filtering algorithms seek to provide personalized product recommendations by uncovering patterns in consumer-product interactions. However, these interactions can be biased by how the product is marketed, for example due to the selection of a particular human model in a product image. These correlations may result in the underrepresentation of particular niche markets in the interaction data; for example, a female user who would potentially like motorcycle products may be less likely to interact with them if they are promoted using stereotypically 'male' images.In this paper, we first investigate this correlation between users' interaction feedback and products' marketing images on two real-world e-commerce datasets. We further examine the response of several standard collaborative filtering algorithms to the distribution of consumer-product market segments in the input interaction data, revealing that marketing strategy can be a source of bias for modern recommender systems. In order to protect recommendation performance on underrepresented market segments, we develop a framework to address this potential marketing bias. Quantitative results demonstrate that the proposed approach significantly improves the recommendation fairness across different market segments, with a negligible loss (or better) recommendation accuracy.","ACMDL - Bias","Case Study",15
"95","ACMDL - Bias","Case Study","Zhu, Ziwei and Wang, Jianling and Caverlee, James","Measuring and Mitigating Item Under-Recommendation Bias in Personalized Ranking Systems","https://doi.org/10.1145/3397271.3401177","SIGIR '20",2020,"recommender systems, recommendation bias, statistical parity, equal opportunity","Recommendation algorithms typically build models based on user-item interactions (e.g., clicks, likes, or ratings) to provide a personalized ranked list of items. These interactions are often distributed unevenly over different groups of items due to varying user preferences. However, we show that recommendation algorithms can inherit or even amplify this imbalanced distribution, leading to item under-recommendation bias. Concretely, we formalize the concepts of ranking-based statistical parity and equal opportunity as two measures of item under-recommendation bias. Then, we empirically show that one of the most widely adopted algorithms -- Bayesian Personalized Ranking -- produces biased recommendations, which motivates our effort to propose the novel debiased personalized ranking model. The debiased model is able to improve the two proposed bias metrics while preserving recommendation performance. Experiments on three public datasets show strong bias reduction of the proposed model versus state-of-the-art alternatives.","ACMDL - Bias","Case Study",41
"96","ACMDL - Bias","Case Study","Aires, Victoria and da Silva, Altigran and Nakamura, Fabiola and Nakamura, Eduardo","An Evaluation of Structural Characteristics of Networks to Identify Media Bias in News Portals","https://doi.org/10.1145/3428658.3431077","WebMedia '20",2020,"network analysis, news analysis, online news, media bias detection","Nowadays, news websites are the main sources of information for most people. But these outlets may have a bias in their publications, favoring some ideology. This can influence people's opinion regarding several topics. Methods to automatically detect bias are usually based on aspects such as text and embedded hyperlinks. In this work, we present a strategy to detect media bias by analyzing networks that model citations between news websites, evaluating if the bias is related to structural characteristics of these networks. We show that ideological bias can be captured by citation patterns and present a method that uses these patterns to automatically identify political bias in news websites, reaching accuracy and F1 scores above 72% and 0.70, respectively.","ACMDL - Bias","Case Study",49
"97","ACMDL - Bias","Case Study","Zhu, Ziwei and He, Yun and Zhao, Xing and Zhang, Yin and Wang, Jianling and Caverlee, James","Popularity-Opportunity Bias in Collaborative Filtering","https://doi.org/10.1145/3437963.3441820","WSDM '21",2021,"recommender systems, statistical parity, recommendation bias, equal opportunity","This paper connects equal opportunity to popularity bias in implicit recommenders to introduce the problem of popularity-opportunity bias. That is, conditioned on user preferences that a user likes both items, the more popular item is more likely to be recommended (or ranked higher) to the user than the less popular one. This type of bias is harmful, exerting negative effects on the engagement of both users and item providers. Thus, we conduct a three-part study: (i) By a comprehensive empirical study, we identify the existence of the popularity-opportunity bias in fundamental matrix factorization models on four datasets; (ii) coupled with this empirical study, our theoretical study shows that matrix factorization models inherently produce the bias; and (iii) we demonstrate the potential of alleviating this bias by both in-processing and post-processing algorithms. Extensive experiments on four datasets show the effective debiasing performance of these proposed methods compared with baselines designed for conventional popularity bias.","ACMDL - Bias","Case Study",17
"98","ACMDL - Bias","Case Study","Acosta, Halim and Henderson, Nathan and Rowe, Jonathan and Min, Wookhee and Minogue, James and Lester, James","What's Fair is Fair: Detecting and Mitigating Encoded Bias in Multimodal Models of Museum Visitor Attention","https://doi.org/10.1145/3462244.3479943","ICMI '21",2021,"Multimodal learning analytics, museum-based learning, algorithmic fairness, visitor modeling","Recent years have seen growing interest in modeling visitor engagement in museums with multimodal learning analytics. In parallel, there has also been growing concern about issues of fairness and encoded bias in machine learning models. In this paper, we investigate bias detection and mitigation techniques to address issues of algorithmic fairness in multimodal models of museum visitor visual attention. We employ slicing analysis using the Absolute Between-ROC Area (ABROCA) statistic to detect encoded bias present in multimodal models of visitor visual attention trained with facial expression and posture data from visitor interactions with a game-based museum exhibit about environmental sustainability. We investigate instances of gender bias that arise between different combinations of modalities across several machine learning techniques. We also measure the effectiveness of two different debiasing strategiesâ€”learned fair representations and reweighingâ€”when applied to the trained multimodal visitor attention models. Results indicate that patterns of bias can arise across different modality combinations for the different visitor visual attention models, and there is often an inherent tradeoff between predictive accuracy and ABROCA. Analyses suggest that debiasing strategies tend to be more effective on multimodal models of visitor visual attention than their unimodal counterparts","ACMDL - Bias","Case Study",32
"99","ACMDL - Bias","Case Study","Zhu, Ziwei and He, Yun and Zhao, Xing and Caverlee, James","Popularity Bias in Dynamic Recommendation","https://doi.org/10.1145/3447548.3467376","KDD '21",2021,"dynamic recommendation, popularity bias","Popularity bias is a long-standing challenge in recommender systems: popular items are overly recommended at the expense of less popular items that users may be interested in being under-recommended. Such a bias exerts detrimental impact on both users and item providers, and many efforts have been dedicated to studying and solving such a bias. However, most existing works situate the popularity bias in a static setting, where the bias is analyzed only for a single round of recommendation with logged data. These works fail to take account of the dynamic nature of real-world recommendation process, leaving several important research questions unanswered: how does the popularity bias evolve in a dynamic scenario? what are the impacts of unique factors in a dynamic recommendation process on the bias? and how to debias in this long-term dynamic process? In this work, we investigate the popularity bias in dynamic recommendation and aim to tackle these research gaps. Concretely, we conduct an empirical study by simulation experiments to analyze popularity bias in the dynamic scenario and propose a dynamic debiasing strategy and a novel False Positive Correction method utilizing false positive signals to debias, which show effective performance in extensive experiments.","ACMDL - Bias","Case Study",37
"100","ACMDL - Bias","Case Study","Abdollahpouri, Himan and Mansoury, Masoud and Burke, Robin and Mobasher, Bamshad and Malthouse, Edward","User-Centered Evaluation of Popularity Bias in Recommender Systems","https://doi.org/10.1145/3450613.3456821","UMAP '21",2021,"long-tail recommendation, popularity bias, calibration, recommender systems, fairness","Recommendation and ranking systems are known to suffer from popularity bias; the tendency of the algorithm to favor a few popular items while under-representing the majority of other items. Prior research has examined various approaches for mitigating popularity bias and enhancing the recommendation of long-tail, less popular, items. The effectiveness of these approaches is often assessed using different metrics to evaluate the extent to which over-concentration on popular items is reduced. However, not much attention has been given to the user-centered evaluation of this bias; how different users with different levels of interest towards popular items are affected by such algorithms. In this paper, we show the limitations of the existing metrics to evaluate popularity bias mitigation when we want to assess these algorithms from the usersâ€™ perspective and we propose a new metric that can address these limitations. In addition, we present an effective approach that mitigates popularity bias from the user-centered point of view. Finally, we investigate several state-of-the-art approaches proposed in recent years to mitigate popularity bias and evaluate their performances using the existing metrics and also from the usersâ€™ perspective. Our experimental results using two publicly-available datasets show that existing popularity bias mitigation techniques ignore the usersâ€™ tolerance towards popular items. Our proposed user-centered method can tackle popularity bias effectively for different users while also improving the existing metrics.","ACMDL - Bias","Case Study",39
"101","ACMDL - Bias","Case Study","Zhu, Ziwei and Caverlee, James","Fighting Mainstream Bias in Recommender Systems via Local Fine Tuning","https://doi.org/10.1145/3488560.3498427","WSDM '22",2022,"local models, recommender systems, mainstream bias","In collaborative filtering, the quality of recommendations critically relies on how easily a model can find similar users for a target user. Hence, a niche user who prefers items out of the mainstream may receive poor recommendations, while a mainstream user sharing interests with many others will likely receive recommendations of higher quality. In this work, we study this mainstream bias centering around three key thrusts. First, to distinguish mainstream and niche users, we explore four approaches based on outlier detection techniques to identify a mainstream score indicating the mainstream level for each user. Second, we empirically show that severe mainstream bias is produced by conventional recommendation models. Last, we explore both global and local methods to mitigate the bias. Concretely, we propose two global models: Distribution Calibration (DC) and Weighted Loss (WL) methods; and one local method: Local Fine Tuning (LFT) method. Extensive experiments show the effectiveness of the proposed methods to improve utility for niche users and also show that the proposed LFT can improve the utility for mainstream users at the same time.","ACMDL - Bias","Case Study",51
"102","ACMDL - Bias","Case Study","Ahmed, Md. Arshad and Chatterjee, Madhura and Dadure, Pankaj and Pakray, Partha","The Role of Biased Data in Computerized Gender Discrimination","https://doi.org/10.1145/3524501.3527599","GE@ICSE '22",2022,"gender bias, debiasing, natural language processing, artificial intelligence","Gender bias is prevalent in all walks of life from schools to colleges, corporate as well as government offices. This has led to the under-representation of the female gender in many professions. Most of the Artificial Intelligence-Natural Language Processing (AI-NLP) models learning from these underrepresented real world datasets amplify the bias in many cases, resulting in traditional biases being reinforced. In this paper, we have discussed how gender bias became ingrained in our society and how it results in the underrepresentation of the female gender in several fields such as education, healthcare, STEM, film industry, food industry, and sports. We shed some light on how traditional gender bias is reflected in AI-NLP systems such as automated resume screening, machine translation, text generation, etc. Future prospects of these AI-NLP applications need to include possible solutions to these existing biased AI-NLP applications, such as debiasing the word embeddings and having guidelines for more ethical and transparent standards.","ACMDL - Bias","Case Study",53
"103","ACMDL - Case Study","Data Audit","Ciliberto, Mathias and Wang, Lin and Roggen, Daniel and Zillmer, Ruediger","A Case Study for Human Gesture Recognition from Poorly Annotated Data","https://doi.org/10.1145/3267305.3267508","UbiComp '18",2018,"dataset annotation, dataset curation, Gesture recognition, activity discovery","In this paper we present a case study on drinking gesture recognition from a dataset annotated by Experience Sampling (ES). The dataset contains 8825 ""sensor events"" and users reported 1808 ""drink events"" through experience sampling. We first show that the annotations obtained through ES do not reflect accurately true drinking events. We present then how we maximise the value of this dataset through two approaches aiming at improving the quality of the annotations post-hoc. First, we use template-matching (Warping Longest Common Subsequence) to spot a subset of events which are highly likely to be drinking gestures. We then propose an unsupervised approach which can perform drinking gesture recognition by combining K-Means clustering with WLCSS. Experimental results verify the effectiveness of the proposed method.","ACMDL - Case Study","Data Audit",74
"104","ACMDL - Case Study","Case Study","Sanabria, Mariutsi Alexandra Osorio and Fern\'andez, Ferney Orlando Amaya and Zabala, Mayda Patricia Gonz\'alez","Colombian Case Study for the Analysis of Open Data Government: A Data Quality Approach","https://doi.org/10.1145/3209415.3209474","ICEGOV '18",2018,"Open Government Data, Open Data, Data Profile, Data quality","Public entities have high volumes of stored government data that serve as a source of information for their management, improve the provision of services and interaction with the community, generating social and economic value. Some public administrations, despite being interested in opening the data, have faced obstacles to disseminate them, especially when data are of poor quality. The objective of this paper is to present the importance of evaluating the quality of the data sets that comply with the open format and that are published by public entities for the use of the community, being the case study the public administration of the municipality of Sabaneta The level of quality of open government data is evaluated in three dimensions: Precision, integrity and consistency, to propose strategies and data quality indicators for the improvement of Open Government Data (DGA) to be published.","ACMDL - Case Study","Case Study",89
"105","ACMDL - Case Study","Meta-Commentary","Barboza, Thais Mester and Santoro, Flavia Maria and Revoredo, Kate Cerqueira and Costa, Rosa M.M.","A Case Study of Process Mining in Auditing","https://doi.org/10.1145/3330204.3330241","SBSI'19",2019,"Process mining, Conformance Checking, Process Auditing","A business process is a sequence of activities logically organized with the goal to produce a service or product which add value for a customer. Process auditing in corporate environment aims to assess the degree of compliance of processes and their controls. Due to the volume of information that needs to be analyzed in an audit job, its cost can be very high. We argue that process mining has the potential to improve this activity, allowing the auditor to meet the short deadlines, as well as bringing greater value to the senior management and reliability in the service provided by the audit. Our goal is to discuss how process mining can improve and bring agility to the verification of conformity of the process model against the process actually carried out in an organization.","ACMDL - Case Study","Meta-Commentary",72
"106","ACMDL - Case Study","Case Study","Goens, Andr\'es and Brauckmann, Alexander and Ertel, Sebastian and Cummins, Chris and Leather, Hugh and Castrillon, Jeronimo","A Case Study on Machine Learning for Synthesizing Benchmarks","https://doi.org/10.1145/3315508.3329976","MAPL 2019",2019,"Synthetic program generation, Machine Learning, Benchmarking, Generative models, CLGen","Good benchmarks are hard to find because they require a substantial effort to keep them representative for the constantly changing challenges of a particular field. Synthetic benchmarks are a common approach to deal with this, and methods from machine learning are natural candidates for synthetic benchmark generation. In this paper we investigate the usefulness of machine learning in the prominent CLgen benchmark generator. We re-evaluate CLgen by comparing the benchmarks generated by the model with the raw data used to train it. This re-evaluation indicates that, for the use case considered, machine learning did not yield additional benefit over a simpler method using the raw data. We investigate the reasons for this and provide further insights into the challenges the problem could pose for potential future generators.","ACMDL - Case Study","Case Study",78
"107","ACMDL - Case Study","Case Study","Gordon, Mitchell and Althoff, Tim and Leskovec, Jure","Goal-Setting And Achievement In Activity Tracking Apps: A Case Study Of MyFitnessPal","https://doi.org/10.1145/3308558.3313432","WWW '19",2019,NA,"Activity tracking apps often make use of goals as one of their core motivational tools. There are two critical components to this tool: setting a goal, and subsequently achieving that goal. Despite its crucial role in how a number of prominent self-tracking apps function, there has been relatively little investigation of the goal-setting and achievement aspects of self-tracking apps. Here we explore this issue, investigating a particular goal setting and achievement process that is extensive, recorded, and crucial for both the app and its users' success: weight loss goals in MyFitnessPal. We present a large-scale study of 1.4 million users and weight loss goals, allowing for an unprecedented detailed view of how people set and achieve their goals. We find that, even for difficult long-term goals, behavior within the first 7 days predicts those who ultimately achieve their goals, that is, those who lose at least as much weight as they set out to, and those who do not. For instance, high amounts of early weight loss, which some researchers have classified as unsustainable, leads to higher goal achievement rates. We also show that early food intake, self-monitoring motivation, and attitude towards the goal are important factors. We then show that we can use our findings to predict goal achievement with an accuracy of 79% ROC AUC just 7 days after a goal is set. Finally, we discuss how our findings could inform steps to improve goal achievement in self-tracking apps.","ACMDL - Case Study","Case Study",79
"108","ACMDL - Case Study","Case Study","Islam, M. D. Samiul and Liu, Daizong and Wang, Kewei and Zhou, Pan and Yu, Li and Wu, Dapeng","A Case Study of HealthCare Platform Using Big Data Analytics and Machine Learning","https://doi.org/10.1145/3341069.3342980","HPCCT '19",2019,"Data Mining, Disease Prediction, Big Data, Healthcare, Machine Learning","The medical services in Bangladesh are shortage nowadays; people are suffering from getting the correct treatment from the hospital. With the low proportion of the doctors and the low per capita salary in Bangladesh, patients need to spend more money to get the appropriate treatments. Therefore, it is necessary to apply modern information technologies by which the scaffold between the patients and specialists can be reduced, and the patients can take proper treatment at a lower cost. Fortunately, we can solve this critical problem by utilizing interaction among electrical devices. With the big data collected from these devices, machine learning is a powerful tool for the data analytics because of its high accuracy, lower computational costs, and lower power consumption. This research is based on a case of study by the incorporation of the database, mobile application, web application and develops a novel platform through which the patients and the doctors can interact. In addition, the platform helps to store the patients' health data to make the final prediction using machine learning methods to get the proper healthcare treatment with the help of the machines and the doctors. The experiment result shows the high accuracy over 95% of the disease detection using machine learning methods, with the cost 90% lower than the local hospital in Bangladesh, which provides the strong support to implement of our platform in the remote area of the country.","ACMDL - Case Study","Case Study",83
"109","ACMDL - Case Study","Case Study","\'Alvarez Fern\'andez, Pablo \'Angel and Hajek, Jeremy R.","A Case Study in Comparative Speech-to-Text Libraries for Use in Transcript Generation for Online Education Recordings","https://doi.org/10.1145/3368308.3415380","SIGITE '20",2020,"algorithms, floss, speech-to-text, deepspeech, kubernetes, automatic speech recognition, subtitles, cloud, test suites","With a proliferation of Cloud based Speech-to-Text services it can be difficult to decide where to start and how to make use of these technologies. These include the major Cloud providers as well as several Open Source Speech-to-Text projects available. We desired to investigate a sample of the available libraries and their attributes relating to the recording artifacts that are the by-product of Online Education.The fact that so many resources are available means that the computing and technical barriers for applying speech recognition algorithms have decreased to the point of being a non-factor in the decision to use Speech-to-Text services. New barriers such as price, compute time, and access to the services? source code (software freedom) can be factored into the decision of which platform to use.This case study provides a beginning to developing a test-suite and guide to compare Speech-to-Text libraries and their out-of-the-box accuracy. Our initial test suite employed two models: 1) a Cloud model employing AWS S3 using AWS Transcribe, 2) an on-premises Open Source model that relies on Mozilla's DeepSpeech[1]. We present our findings and recommendations based on the criteria discovered.In order to deliver this test-suite, we also conducted research into the latest web development technologies with emphasis on security. This was done to produce a reliable and secure development process and to provide open access to this proof of concept for further testing and development.","ACMDL - Case Study","Case Study",66
"110","ACMDL - Case Study","Case Study","Ameko, Mawulolo K. and Beltzer, Miranda L. and Cai, Lihua and Boukhechba, Mehdi and Teachman, Bethany A. and Barnes, Laura E.","Offline Contextual Multi-Armed Bandits for Mobile Health Interventions: A Case Study on Emotion Regulation","https://doi.org/10.1145/3383313.3412244","RecSys '20",2020,"Emotion regulation, Mobile health, User modeling, Health recommender systems, Offline contextual bandits","Delivering treatment recommendations via pervasive electronic devices such as mobile phones has the potential to be a viable and scalable treatment medium for long-term health behavior management. But active experimentation of treatment options can be time-consuming, expensive and altogether unethical in some cases. There is a growing interest in methodological approaches that allow an experimenter to learn and evaluate the usefulness of a new treatment strategy before deployment. We present the first development of a treatment recommender system for emotion regulation using real-world historical mobile digital data from n = 114 high socially anxious participants to test the usefulness of new emotion regulation strategies. We explore a number of offline contextual bandits estimators for learning and propose a general framework for learning algorithms. Our experimentation shows that the proposed doubly robust offline learning algorithms performed significantly better than baseline approaches, suggesting that this type of recommender algorithm could improve emotion regulation. Given that emotion regulation is impaired across many mental illnesses and such a recommender algorithm could be scaled up easily, this approach holds potential to increase access to treatment for many people. We also share some insights that allow us to translate contextual bandit models to this complex real-world data, including which contextual features appear to be most important for predicting emotion regulation strategy effectiveness.","ACMDL - Case Study","Case Study",70
"111","ACMDL - Case Study","Case Study","Boukhayma, Khaoula and Idrissi, Mohammed Abdou Janati and Benhiba, Lamia","Evaluating Ongoing Decision Support System: A Case Study","https://doi.org/10.1145/3419604.3419802","SITA'20",2020,"Decision support systems, software evaluation, user learning","The aim of this paper is to put into practice a framework we proposed in an earlier work for evaluating on-going decision support systems. The framework serves as a base for a method to compute evaluation criteria scores using an AHP model. The paper also illustrates the automation of this method into a tool to enable GASCO company to choose between two already implemented DSSs in order to reduce IT operating cost. The evaluation covers three axes: Process, outcome and decision maker metrics. The results highlight the importance of using users' learning as a measure to determine DSS success.","ACMDL - Case Study","Case Study",73
"112","ACMDL - Case Study","Case Study","Graells-Garrido, Eduardo and Baeza-Yates, Ricardo and Lalmas, Mounia","Representativeness of Abortion Legislation Debate on Twitter: A Case Study in Argentina and Chile","https://doi.org/10.1145/3366424.3383561","WWW '20",2020,"Data Bias, Social Networks, Stance Prediction","The role of the Web in political exchange has been crucial for society. Its platforms have connected people and allowed manifestation, organization, and access to information; however, they have also produced negative outcomes, such as increased polarization and fast disinformation spreading. These types of phenomena are not completely understood in the context of continuous technological change. Here we propose to grow knowledge in these issues by focusing on representativeness, through the following question: How demographic groups are represented in the discussion on micro-blogging platforms? Our aim is to answer this question on the discussion about a specific topic, abortion, as observed on one of the most popular micro-blogging platforms. As a case study, we followed the abortion discussion on Twitter in two Spanish-speaking countries from 2015 to 2018. Our results indicate differences in representativeness with respect to country, stance, and time of publication, a process that affects to on-going legislation. These findings show that demographic groups differ in how they generate content, and that under- and over-represented groups are not the same between countries, implying that single-country outcomes are not generalizable.","ACMDL - Case Study","Case Study",80
"113","ACMDL - Case Study","Case Study","Agarwal, Pushkal and Hawkins, Oliver and Amaxopoulou, Margarita and Dempsey, Noel and Sastry, Nishanth and Wood, Edward","Hate Speech in Political Discourse: A Case Study of UK MPs on Twitter","https://doi.org/10.1145/3465336.3475113","HT '21",2021,"politics, topics, hate-speech, twitter","Online presence is becoming unavoidable for politicians worldwide. In countries such as the UK, Twitter has become the platform of choice, with over 85% (553 of 650) of the Members of Parliament (MPs) having an active online presence. Whereas this has allowed ordinary citizens unprecedented and immediate access to their elected representatives, it has also led to serious concerns about online hate towards MPs. This work attempts to shed light on the problem using a dataset of conversations between MPs and non-MPs over a two month period. Deviating from other approaches in the literature, our data captures entire threads of conversations between Twitter handles of MPs and citizens in order to provide a full context for content that may be flagged as 'hate'. By combining widely-used hate speech detection tools trained on several widely available datasets, we analyse 2.5 million tweets to identify hate speech against MPs and we characterise hate across multiple dimensions of time, topics and MPs' demographics. We find that MPs are subject to intense 'pile on' hate by citizens whereby they get more hate when they are already busy with a high volume of mentions regarding some event or situation. We also show that hate is more dense with regard to certain topics and that MPs who have an ethnic minority background and those holding positions in Government receive more hate than other MPs. We find evidence of citizens expressing negative sentiments while engaging in cross-party conversations, with supporters of one party (e.g. Labour) directing hate against MPs of another party (e.g. Conservative).","ACMDL - Case Study","Case Study",67
"114","ACMDL - Case Study","Case Study","Dallmann, Alexander and Zoller, Daniel and Hotho, Andreas","A Case Study on Sampling Strategies for Evaluating Neural Sequential Item Recommendation Models","https://doi.org/10.1145/3460231.3475943","RecSys '21",2021,"Sequential Item Recommendation, Evaluation, Metrics, Sampled Metrics","At the present time, sequential item recommendation models are compared by calculating metrics on a small item subset (target set) to speed up computation. The target set contains the relevant item and a set of negative items that are sampled from the full item set. Two well-known strategies to sample negative items are uniform random sampling and sampling by popularity to better approximate the item frequency distribution in the dataset. Most recently published papers on sequential item recommendation rely on sampling by popularity to compare the evaluated models. However, recent work has already shown that an evaluation with uniform random sampling may not be consistent with the full ranking, that is, the model ranking obtained by evaluating a metric using the full item set as target set, which raises the question whether the ranking obtained by sampling by popularity is equal to the full ranking. In this work, we re-evaluate current state-of-the-art sequential recommender models from the point of view, whether these sampling strategies have an impact on the final ranking of the models. We therefore train four recently proposed sequential recommendation models on five widely known datasets. For each dataset and model, we employ three evaluation strategies. First, we compute the full model ranking. Then we evaluate all models on a target set sampled by the two different sampling strategies, uniform random sampling and sampling by popularity with the commonly used target set size of 100, compute the model ranking for each strategy and compare them with each other. Additionally, we vary the size of the sampled target set. Overall, we find that both sampling strategies can produce inconsistent rankings compared with the full ranking of the models. Furthermore, both sampling by popularity and uniform random sampling do not consistently produce the same ranking when compared over different sample sizes. Our results suggest that like uniform random sampling, rankings obtained by sampling by popularity do not equal the full ranking of recommender models and therefore both should be avoided in favor of the full ranking when establishing state-of-the-art.","ACMDL - Case Study","Case Study",75
"115","ACMDL - Case Study","Case Study","Forkan, Abdur Rahim Mohammad and Jayaraman, Prem Prakash and Kaul, Rohit and Zhang, Yuxin and McCarthy, Chris and Delir Haghighi, Pari and Ranjan, Rajiv","MobDL: A Framework for Profiling Deep Learning Models: A Case Study Using Mobile Digital Health Applications","https://doi.org/10.1145/3448891.3448896","MobiQuitous '20",2021,"Framework, Profiling, Digital Health, Deep Learning, CNN","Smart mobile devices coupled with the Internet of Things (IoT) and Artificial Intelligence (AI) have emerged as a key enabler of modern digital health applications. While cloud computing is now a well established paradigm for analysing IoT captured data in mobile health applications, on-board analysis of data using AI approaches such as Deep Learning (DL) is gaining significant momentum. This is driven primarily by advances in on-board resources enabling modern mobile devices to execute complex DL models, while also offering improved response time and accuracy for rapid decision-making, and enhanced user privacy. While the number of mobile digital health applications that use IoT and DL is increasing, progress is currently impeded by a lack of framework for profiling and evaluating the performance of DL models on mobile devices. To this end, we propose MobDL, a framework for profiling and evaluating DL models running on smart mobile devices. We present the architecture of this framework and devise a novel evaluation methodology for conducting quantitative comparisons of various DL models running on mobile devices. Three diverse digital health applications using heterogeneous data (e.g. image, time series) are introduced. We conduct extensive experimental evaluations using several DL models that have been developed using the data sets obtained for the three digital health applications to validate the effectiveness of the proposed MobDLÂ framework.","ACMDL - Case Study","Case Study",77
"116","ACMDL - Case Study","Case Study","Agarwal, Pushkal and Raman, Aravindh and Ibosiola, Damiola and Sastry, Nishanth and Tyson, Gareth and Garimella, Kiran","Jettisoning Junk Messaging in the Era of End-to-End Encryption: A Case Study of WhatsApp","https://doi.org/10.1145/3485447.3512130","WWW '22",2022,"junk messaging, WhatsApp, spam, end-to-end encryption","WhatsApp is a popular messaging app used by over a billion users around the globe. Due to this popularity, understanding misbehavior on WhatsApp is an important issue. The sending of unwanted junk messages by unknown contacts via WhatsApp remains understudied by researchers, in part because of the end-to-end encryption offered by the platform. We address this gap by studying junk messaging on a multilingual dataset of 2.6M messages sent to 5K public WhatsApp groups in India. We characterise both junk content and senders. We find that nearly 1 in 10 messages is unwanted content sent by junk senders, and a number of unique strategies are employed to reflect challenges faced on WhatsApp, e.g., the need to change phone numbers regularly. We finally experiment with on-device classification to automate the detection of junk, whilst respecting end-to-end encryption.","ACMDL - Case Study","Case Study",68
"117","ACMDL - Case Study","Case Study","Akritidis, Georgios and Katsanos, Christos","Effect of Potential Issues Flagged by Automated Tools on Web Accessibility Evaluation Results: A Case Study on University Department Websites","https://doi.org/10.1145/3503823.3503845","PCI 2021",2022,"University department websites, Web accessibility, Human-computer interaction, Automated evaluation","Web accessibility expresses the ease with which one can interact with a website effectively, regardless of disabilities and devices used. Despite the availability of relevant guidelines and technologies, many websites remain inaccessible. This may be due to the amount of manual effort required to ensure that a website conforms to web accessibility guidelines, such as the Web Content Accessibility Guidelines (WCAG). Software tools have been proposed to facilitate the process. However, these tools report many potential accessibility issues that require human judgment, and thus substantial manual resources. This paper investigates the effect of such potential issues reported by automated tools on web accessibility evaluation results. To this end, 441 university department websites were evaluated against WCAG 2 using the Siteimprove Accessibility Checker. Results found a significant effect of the manual inspection of potential issues on the accessibility results obtained. Regression analysis was used to predict the number of the semi-automatically identified accessibility issues based on the automatically identified ones.","ACMDL - Case Study","Case Study",69
"118","ACMDL - Case Study","Meta-Commentary","Angeli, Vaia-Maria and Atamli, Ahmad and Karafili, Erisa","Forensic Analysis of Tor in Windows Environment: A Case Study","https://doi.org/10.1145/3538969.3543808","ARES '22",2022,"Case Study, Forensic analysis, Tor browser., Windows 10","The Tor browser is a popular tool that is used by many users around the world. The browser is common among cyber criminals who use the tool to hide their activities. Until now, little research has been conducted by forensics researchers on the Tor browser, its application, and the data that can be obtained from the artefacts generated from its execution. In this work, we present a forensics analysis of the footprint left by the Tor application in the Windows environment. Our analysis focuses on three critical areas that are examined: network, memory, and hard disk. We provide a methodology that allows a structured forensic investigation. In this work, we examine multiple toolsâ€™ abilities in obtaining artefacts. The artefacts were identified not only when the Tor browser was running, but also when it was closed and uninstalled. We provide a methodology to analyse Tor applications with a focused case study of the Tor browser, allowing investigators to analyse Tor browsers and reproduce our results.","ACMDL - Case Study","Method",71
"119","ACMDL - Case Study","Case Study","Duan, Xiaoni and Ho, Chien-Ju and Yin, Ming","The Influences of Task Design on Crowdsourced Judgement: A Case Study of Recidivism Risk Evaluation","https://doi.org/10.1145/3485447.3512239","WWW '22",2022,"quality, fairness, task design, bias, Crowdsourcing","Crowdsourcing is widely used to solicit judgement from people in diverse applications ranging from evaluating information quality to rating gig worker performance. To encourage the crowd to put in genuine effort in the judgement tasks, various ways to structure and organize these tasks have been explored, though the understandings of how these task design choices influence the crowdâ€™s judgement are still largely lacking. In this paper, using recidivism risk evaluation as an example, we conduct a randomized experiment to examine the effects of two common designs of crowdsourcing judgement tasksâ€”encouraging the crowd to deliberate and providing feedback to the crowdâ€”on the quality, strictness, and fairness of the crowdâ€™s recidivism risk judgements. Our results show that different designs of the judgement tasks significantly affect the strictness of the crowdâ€™s judgements. Moreover, task designs also have the potential to significantly influence how fairly the crowd judges defendants from different racial groups, on those cases where the crowd exhibits substantial in-group bias. Finally, we find that the impacts of task designs on the judgement also vary with the crowd workersâ€™ own characteristics, such as their cognitive reflection levels. Together, these results highlight the importance of obtaining a nuanced understanding on the relationship between task designs and properties of the crowdsourced judgements.","ACMDL - Case Study","Case Study",76
"120","ACMDL - Case Study","Case Study","Gunawan, Johanna and Santos, Cristiana and Kamara, Irene","Redress for Dark Patterns Privacy Harms? A Case Study on Consent Interactions","https://doi.org/10.1145/3511265.3550448","CSLAW '22",2022,"gdpr, consent, policy and law, deceptive design, redress, data protection infringement, harm, dark patterns, damages","Internet users are constantly subjected to incessant demands for attention in a noisy digital world. Countless inputs compete for the chance to be clicked, to be seen, and to be interacted with, and they can deploy tactics that take advantage of behavioral psychology to 'nudge' users into doing what they want. Some nudges are benign; others deceive, steer, or manipulate users, as the U.S. FTC Commissioner says, ""into behavior that is profitable for an online service, but often harmful to [us] or contrary to [our] intent"". These tactics are dark patterns, which are manipulative and deceptive interface designs used at-scale in more than ten percent of global shopping websites and more than ninety-five percent of the most popular apps in online services. Literature discusses several types of harms caused by dark patterns that includes harms of a material nature, such as financial harms, or anticompetitive issues, as well as harms of a non-material nature, such as privacy invasion, time loss, addiction, cognitive burdens, loss of autonomy, and emotional or psychological distress. Through a comprehensive literature review of this scholarship and case law analysis conducted by our interdisciplinary team of HCI and legal scholars, this paper investigates whether harms caused by such dark patterns could give rise to redress for individuals subject to dark pattern practices using consent interactions and the GDPR consent requirements as a case study.","ACMDL - Case Study","Case Study",81
"121","ACMDL - Case Study","Case Study","Hadi Mogavi, Reza and Guo, Bingcan and Zhang, Yuanhao and Haq, Ehsan-Ul and Hui, Pan and Ma, Xiaojuan","When Gamification Spoils Your Learning: A Qualitative Case Study of Gamification Misuse in a Language-Learning App","https://doi.org/10.1145/3491140.3528274","L@S '22",2022,"L@S, qualitative research, student-centered education, gamification, misuse, HCI, duolingo, learning app, gamified education","More and more learning apps like Duolingo are using some form of gamification (e.g., badges, points, and leaderboards) to enhance user learning. However, they are not always successful. Gamification misuse is a phenomenon that occurs when users become too fixated on gamification and get distracted from learning. This undesirable phenomenon wastes users' precious time and negatively impacts their learning performance. However, there has been little research in the literature to understand gamification misuse and inform future gamification designs. Therefore, this paper aims to fill this knowledge gap by conducting the first extensive qualitative research on gamification misuse in a popular learning app called Duolingo. Duolingo is currently the world's most downloaded learning app used to learn languages. This study consists of two phases: (I)a content analysis of data from Duolingo forums (from the past nine years) and (II)semi-structured interviews with 15 international Duolingo users. Our research contributes to the Human-Computer Interaction (HCI) and Learning at Scale (L@S) research communities in three ways: (1) elaborating the ramifications of gamification misuse on user learning, well-being, and ethics, (2) identifying the most common reasons for gamification misuse (e.g., competitiveness, overindulgence in playfulness, and herding), and (3) providing designers with practical suggestions to prevent (or mitigate) the occurrence of gamification misuse in their future designs of gamified learning apps.","ACMDL - Case Study","Case Study",82
"122","ACMDL - Case Study","Case Study","Livraga, Giovanni and Motta, Alessandro and Viviani, Marco","Assessing User Privacy on Social Media: The Twitter Case Study","https://doi.org/10.1145/3524010.3539502","OASIS '22",2022,"Vector Space Model, Social Media, Confidentiality, Privacy","At the time of writing, nearly four billion people worldwide employ social media platforms such as Facebook, Instagram, WeChat, TikTok, etc. to share content of various kinds, which may also include personal data. In addition to this, users interact with members of the virtual community, leaving behind important behavioral traces. In most cases, people do not have a full understanding of who will be able to access and use such a body of information, and for what purposes. Although social platforms provide users with some tools to protect their privacy, the very nature of these technologies and the psychological characteristics of users often lead them to ignore such solutions. To address this issue, in this paper we aim to propose a model for assessing the privacy of users on social media by identifying the critical aspects associated with their content and interactions generated on such platforms. This model, in particular, considers distinct features, of different kinds, that capture the level of usersâ€™ exposure with respect to privacy. These features, dropped into a vector space, are used to derive a score that expresses, in a measurable way, the privacy risk of users compared to the information available on social media about them. The proposed model is instantiated and tested on data collected from the microblogging platform Twitter, on which the results of the experimental evaluation are analyzed. Specifically, the model is tested by considering both a binary scenario, i.e., where usersâ€™ privacy is evaluated as at risk or not, a multi-class scenario, i.e., where their privacy is evaluated against different risk ranges, and a ranking scenario, i.e., where the users are ranked according to their privacy assessment.","ACMDL - Case Study","Case Study",84
"123","ACMDL - Case Study","Ecosystem","Ma, Shuhao and Ferreira, Marta and Nicolau, Hugo and Prandi, Catia and Esteves, Augusto and Nunes, Nuno Jardim and Nisi, Valentina","Catering for Studentsâ€™ Well-Being during COVID-19 Social Distancing: A Case Study from a University Campus","https://doi.org/10.1145/3524458.3547261","GoodIT '22",2022,"crowdsensing, social distancing, design evaluation, student well-being","COVID-19 gave rise to discussions around designing for life during the pandemic, in particular related to health, leisure and education. In 2020, an online survey aimed at university students (N=225) pointed the authors to various challenges related to well-being in terms of studying, socializing, community, and safety during the COVID-19 pandemic. These results shaped the crowdsensing-enabled service design of a mobile application, Tecnico GO!, aimed at supporting students’ well-being. Considering the constant changing context caused by the pandemic, we present a study conducted during the academic year 2021-2022 and if/how the App’s features continue to respond to student’s needs. The evaluation of the App focused on 12 semi-structured interviews and think-aloud protocols. Findings cluster around three themes: a) Supporting the study experience; b) Building a sense of community; c) Improving gamification for better participation. Discussion elaborates on the student’s perceptions around well-being during pandemics. Students’ insights of the App are overall positive and highlight that crowdsensing-enabled design does contribute to learning, community and safety, but the gamification as currently deployed does not.","ACMDL - Case Study","Ecosystem",85
"124","ACMDL - Case Study","Case Study","Mantovani, Alessandro and Compagna, Luca and Shoshitaishvili, Yan and Balzarotti, Davide","The Convergence of Source Code and Binary Vulnerability Discovery -- A Case Study","https://doi.org/10.1145/3488932.3497764","ASIA CCS '22",2022,"reversing, decompiler, vulnerability, sast","Decompilers are tools designed to recover a high-level language representation (typically in C code) from program binaries. Over the past five years, decompilers have improved enormously, not only in terms of the readability of the produced pseudocode, but also in terms of similarity of the recovered representation to the original source code. Albeit decompilers are routinely used by reverse engineers in different disciplines (e.g., to support vulnerability discovery or malware analysis), they are not yet adopted to produce input for source-code static analysis tools. In particular, source code vulnerability discovery and binary vulnerability discovery remain today two very different areas of research, despite the fact that decompilers could potentially bridge this gap and enable source-code analysis on binary files.In this paper, we conducted a number of experiments on real world vulnerabilities to evaluate the feasibility of this approach. In particular, our measurements are intended to show how the differences between original and decompiled code impact the accuracy of static analysis tools.Remarkably, our results show that in 71% of the cases, the same vulnerabilities can be detected by running the static analyzers on the decompiled code, even though for several cases we observe a steep increment in the number of false positives. To understand the reasons behind these differences, we manually investigated all cases and we identified a number of root causes that affected the ability of static tools to 'understand' the generated code.","ACMDL - Case Study","Case Study",86
"125","ACMDL - Case Study","Data Audit","Omar, Marwan and Mohaisen, David","Making Adversarially-Trained Language Models Forget with Model Retraining: A Case Study on Hate Speech Detection","https://doi.org/10.1145/3487553.3524667","WWW '22",2022,"concept drift, robustness, Hate speech detection, adversarial training","Adversarial training has become almost the de facto standard for robustifying Natural Language Processing models against adversarial attacks. Although adversarial training has proven to achieve accuracy gains and boost the performance of algorithms, research has not shown how adversarial training will stand â€œthe test of timesâ€ when models are deployed and updated with new non-adversarial data samples. In this study, we aim to quantify the temporal impact of adversarial training on naturally-evolving language models using the hate speech task. We conduct extensive experiments on the Tweet Eval benchmark dataset using multiple hate speech classification models. In particular, our findings indicate that adversarial training is highly task-dependent as well as dataset dependent as models trained on the same dataset achieve high prediction accuracy but fare poorly when tested with new dataset even after retraining models with adversarial examples. We attribute this temporal and limited effect of adversarial training to distribution shift of the training data which implies that modelsâ€™ quality will degrade over-time as models are deployed in the real world and start serving new data.","ACMDL - Case Study","Data Audit",87
"126","ACMDL - Case Study","Case Study","Sachdeva, Pratik S. and Barreto, Renata and von Vacano, Claudia and Kennedy, Chris J.","Assessing Annotator Identity Sensitivity via Item Response Theory: A Case Study in a Hate Speech Corpus","https://doi.org/10.1145/3531146.3533216","FAccT '22",2022,"item response theory, annotator sensitivity, hate speech, annotation, differential rater functioning","Content Warning: This paper contains content considered profane, hateful, and offensive. Annotators, by labeling data samples, play an essential role in the production of machine learning datasets. Their role is increasingly prevalent for more complex tasks such as hate speech or disinformation classification, where labels may be particularly subjective, as evidenced by low inter-annotator agreement statistics. Annotators may exhibit observable differences in their labeling patterns when grouped by their self-reported demographic identities, such as race, gender, etc. We frame these patterns as annotator identity sensitivities, referring to an annotatorâ€™s increased likelihood of assigning a particular label on a data sample, conditional on a self-reported identity group. We purposefully refrain from using the term annotator bias, which we argue is problematic terminology in such subjective scenarios. Since annotator identity sensitivities can play a role in the patterns learned by machine learning algorithms, quantifying and characterizing them is of paramount importance for fairness and accountability in machine learning. In this work, we utilize item response theory (IRT), a methodological approach developed for measurement theory, to quantify annotator identity sensitivity. IRT models can be constructed to incorporate diverse factors that influence a label on a specific data sample, such as the data sample itself, the annotator, and the labeling instrumentâ€™s wording and response options. An IRT model captures the contributions of these facets to the label via a latent-variable probabilistic model, thereby allowing the direct quantification of annotator sensitivity. As a case study, we examine a hate speech corpus containing over 50,000 social media comments from Reddit, YouTube, and Twitter, rated by 10,000 annotators on 10 components of hate speech (e.g., sentiment, respect, violence, dehumanization, etc.). We leverage three different IRT techniques which are complementary in that they quantify sensitivity from different perspectives: separated measurements, annotator-level interactions, and group-level interactions. We use these techniques to assess whether an annotatorâ€™s racial identity is associated with their ratings on comments that target different racial identities. We find that, after controlling for the estimated hatefulness of social media comments, annotators tended to be more sensitive when rating comments targeting a group they identify with. Specifically, annotators were more likely to rate comments targeting their own racial identity as possessing elements of hate speech. Our results identify a correspondence between annotator identity and the target identity of hate speech comments, and provide a set of tools that can assess annotator identity sensitivity in machine learning datasets at large.","ACMDL - Case Study","Case Study",88
"127","ACMDL - Case Study","Ecosystem","Wang, Clarice and Wang, Kathryn and Bian, Andrew and Islam, Rashidul and Keya, Kamrun Naher and Foulds, James and Pan, Shimei","Do Humans Prefer Debiased AI Algorithms? A Case Study in Career Recommendation","https://doi.org/10.1145/3490099.3511108","IUI '22",2022,"Human-centered computing → Empirical studies in HCI;  Computing methodologies → Machine learning.","Currently, there is a surge of interest in fair Artificial Intelligence (AI) and Machine Learning (ML) research which aims to mitigate discriminatory bias in AI algorithms, e.g. along lines of gender, age, and race. While most research in this domain focuses on developing fair AI algorithms, in this work, we examine the challenges which arise when human- fair-AI interact. Our results show that due to an apparent conflict between human preferences and fairness, a fair AI algorithm on its own may be insufficient to achieve its intended results in the real world. Using college major recommendation as a case study, we build a fair AI recommender by employing gender debiasing machine learning techniques. Our offline evaluation showed that the debiased recommender makes fairer and more accurate college major recommendations. Nevertheless, an online user study of more than 200 college students revealed that participants on average prefer the original biased system over the debiased system. Specifically, we found that the perceived gender disparity associated with a college major is a determining factor for the acceptance of a recommendation. In other words, our results demonstrate we cannot fully address the gender bias issue in AI recommendations without addressing the gender bias in humans. They also highlight the urgent need to extend the current scope of fair AI research from narrowly focusing on debiasing AI algorithms to including new persuasion and bias explanation technologies in order to achieve intended societal impacts.","ACMDL - Case Study","Ecosystem",90
"128","ACMDL - Case Study","Case Study","Zilka, Miri and Sargeant, Holli and Weller, Adrian","Transparency, Governance and Regulation of Algorithmic Tools Deployed in the Criminal Justice System: A UK Case Study","https://doi.org/10.1145/3514094.3534200","AIES '22",2022,"Criminal Justice, Trustworthy AI, Policy, Governance and regulation, Algorithms in deployment","We present a survey of tools used in the criminal justice system in the UK in three categories: data infrastructure, data analysis, and risk prediction. Many tools are currently in deployment, offering potential benefits, including improved efficiency and consistency. However, there are also important concerns. Transparent information about these tools, their purpose, how they are used, and by whom is difficult to obtain. Even when information is available, it is often insufficient to enable a satisfactory evaluation. More work is needed to establish governance mechanisms to ensure that tools are deployed in a transparent, safe and ethical way. We call for more engagement with stakeholders and greater documentation of the intended goal of a tool, how it will achieve this goal compared to other options, and how it will be monitored in deployment. We highlight additional points to consider when evaluating the trustworthiness of deployed tools and make concrete proposals for policy.","ACMDL - Case Study","Case Study",91
"129","FAccT","Case Study","Speicher et al.","Potential for Discrimination in Online Targeted Advertising","https://proceedings.mlr.press/v81/speicher18a/speicher18a.pdf","FAccT",2018,"Discrimination, advertising, Facebook","Recently, online targeted advertising plat-
forms like Facebook have been criticized for
allowing advertisers to discriminate against
users belonging to sensitive groups, i.e., to
exclude users belonging to a certain race
or gender from receiving their ads. Such
criticisms have led, for instance, Facebook
to disallow the use of attributes such as
ethnic affinity from being used by adver-
tisers when targeting ads related to hous-
ing or employment or financial services. In
this paper, we show that such measures are
far from sufficient and that the problem
of discrimination in targeted advertising is
much more pernicious. We argue that dis-
crimination measures should be based on
the targeted population and not on the at-
tributes used for targeting. We system-
atically investigate the different targeting
methods offered by Facebook for their abil-
ity to enable discriminatory advertising.
We show that a malicious advertiser can
create highly discriminatory ads without
using sensitive attributes. Our findings call
for exploring fundamentally new methods
for mitigating discrimination in online tar-
geted advertising.","ACMDL - FAccT","Case Study",497
"130","FAccT","Case Study","Buolamwini, Gebru","Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification","https://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf","FAccT",2018,"Computer Vision, Algorithmic Audit, Gender Classification","Recent studies demonstrate that machine
learning algorithms can discriminate based
on classes like race and gender. In this
work, we present an approach to evaluate
bias present in automated facial analysis al-
gorithms and datasets with respect to phe-
notypic subgroups. Using the dermatolo-
gist approved Fitzpatrick Skin Type clas-
sification system, we characterize the gen-
der and skin type distribution of two facial
analysis benchmarks, IJB-A and Adience.
We find that these datasets are overwhelm-
ingly composed of lighter-skinned subjects
(79.6% for IJB-A and 86.2% for Adience)
and introduce a new facial analysis dataset
which is balanced by gender and skin type.
We evaluate 3 commercial gender clas-
sification systems using our dataset and
show that darker-skinned females are the
most misclassified group (with error rates
of up to 34.7%). The maximum error rate
for lighter-skinned males is 0.8%. The
substantial disparities in the accuracy of
classifying darker females, lighter females,
darker males, and lighter males in gender
classification systems require urgent atten-
tion if commercial companies are to build
genuinely fair, transparent and accountable
facial analysis algorithms.","ACMDL - FAccT","Case Study",498
"131","FAccT","Case Study","Chouldechova et al.","A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions","https://proceedings.mlr.press/v81/chouldechova18a/chouldechova18a.pdf","FAccT",2018,NA,"Every year there are more than 3.6 mil-
lion referrals made to child protection agen-
cies across the US. The practice of screen-
ing calls is left to each jurisdiction to fol-
low local practices and policies, potentially
leading to large variation in the way in
which referrals are treated across the coun-
try. Whilst increasing access to linked ad-
ministrative data is available, it is difficult
for welfare workers to make systematic use
of historical information about all the chil-
dren and adults on a single referral call.
Risk prediction models that use routinely
collected administrative data can help call
workers to better identify cases that are
likely to result in adverse outcomes. How-
ever, the use of predictive analytics in the
area of child welfare is contentious. There
is a possibility that some communities—
such as those in poverty or from particu-
lar racial and ethnic groups—will be dis-
advantaged by the reliance on government
administrative data. On the other hand,
these analytics tools can augment or re-
place human judgments, which themselves
are biased and imperfect. In this paper we
describe our work on developing, validat-
ing, fairness auditing, and deploying a risk
prediction model in Allegheny County, PA,
USA. We discuss the results of our analy-
sis to-date, and also highlight key problems
and data bias issues that present challenges
for model evaluation and deployment","ACMDL - FAccT","Case Study",499
"132","FAccT","Case Study","Engelmann et al.","Clear Sanctions, Vague Rewards: How China's Social Credit System Currently Defines ""Good"" and ""Bad"" Behavior","https://dl.acm.org/doi/10.1145/3287560.3287585","FAccT",2019,"Social Credit System, Socio-Technical Systems, Transparency, Behavioral Engineering.","China's Social Credit System (SCS, 社会信用体系 or shehui xinyong tixi) is expected to become the first digitally-implemented nationwide scoring system with the purpose to rate the behavior of citizens, companies, and other entities. Thereby, in the SCS, ""good"" behavior can result in material rewards and reputational gain while ""bad"" behavior can lead to exclusion from material resources and reputational loss. Crucially, for the implementation of the SCS, society must be able to distinguish between behaviors that result in reward and those that lead to sanction. In this paper, we conduct the first transparency analysis of two central administrative information platforms of the SCS to understand how the SCS currently defines ""good"" and ""bad"" behavior. We analyze 194,829 behavioral records and 942 reports on citizens' behaviors published on the official Beijing SCS website and the national SCS platform ""Credit China"", respectively. By applying a mixed-method approach, we demonstrate that there is a considerable asymmetry between information provided by the so-called Redlist (information on ""good"" behavior) and the Blacklist (information on ""bad"" behavior). At the current stage of the SCS implementation, the majority of explanations on blacklisted behaviors includes a detailed description of the causal relation between inadequate behavior and its sanction. On the other hand, explanations on redlisted behavior, which comprise positive norms fostering value internalization and integration, are less transparent. Finally, this first SCS transparency analysis suggests that socio-technical systems applying a scoring mechanism might use different degrees of transparency to achieve particular behavioral engineering goals.","ACMDL - FAccT","Case Study",493
"133","FAccT","Case Study","Jiang, Martin, Wilson","Who's the Guinea Pig?: Investigating Online A/B/n Tests in-the-Wild","https://dl.acm.org/doi/10.1145/3287560.3287565","FAccT",2019,"online controlled experiments; A/B/n testing; personalization","A/B/n testing has been adopted by many technology companies as a data-driven approach to product design and optimization. These tests are often run on their websites without explicit consent from users. In this paper, we investigate such online A/B/n tests by using Optimizely as a lens. First, we provide measurement results of 575 websites that use Optimizely drawn from the Alexa Top-1M, and analyze the distributions of their audiences and experiments. Then, we use three case studies to discuss potential ethical pitfalls of such experiments, including involvement of political content, price discrimination, and advertising campaigns. We conclude with a suggestion for greater awareness of ethical concerns inherent in human experimentation and a call for increased transparency among A/B/n test operators.","ACMDL - FAccT","Case Study",494
"134","FAccT","Case Study","Ribeiro et al.","On Microtargeting Socially Divisive Ads: A Case Study of Russia-Linked Ad Campaigns on Facebook","https://dl.acm.org/doi/10.1145/3287560.3287580","FAccT",2019,"advertisements, targeting, social divisiveness, news media, social media, perception bias","Targeted advertising is meant to improve the efficiency of matching advertisers to their customers. However, targeted advertising can also be abused by malicious advertisers to efficiently reach people susceptible to false stories, stoke grievances, and incite social conflict. Since targeted ads are not seen by non-targeted and non-vulnerable people, malicious ads are likely to go unreported and their effects undetected. This work examines a specific case of malicious advertising, exploring the extent to which political ads1 from the Russian Intelligence Research Agency (IRA) run prior to 2016 U.S. elections exploited Facebook's targeted advertising infrastructure to efficiently target ads on divisive or polarizing topics (e.g., immigration, race-based policing) at vulnerable sub-populations. In particular, we do the following: (a) We conduct U.S. census-representative surveys to characterize how users with different political ideologies report, approve, and perceive truth in the content of the IRA ads. Our surveys show that many ads are ""divisive"": they elicit very different reactions from people belonging to different socially salient groups. (b) We characterize how these divisive ads are targeted to sub-populations that feel particularly aggrieved by the status quo. Our findings support existing calls for greater transparency of content and targeting of political ads. (c) We particularly focus on how the Facebook ad API facilitates such targeting. We show how the enormous amount of personal data Facebook aggregates about users and makes available to advertisers enables such malicious targeting.","ACMDL - FAccT","Case Study",495
"135","FAccT","Case Study","Obermeyer, Mullainathan","Dissecting Racial Bias in an Algorithm that Guides Health Decisions for 70 Million People","https://dl.acm.org/doi/10.1145/3287560.3287593","FAccT",2019,"bias, algorithms, racial disparities, health policy, medicine","A single algorithm drives an important health care decision for over 70 million people in the US. When health systems anticipate that a patient will have especially complex and intensive future health care needs, she is enrolled in a 'care management' program, which provides considerable additional resources: greater attention from trained providers and help with coordination of her care.","ACMDL - FAccT","Case Study",496
"136","FAccT","Meta-Commentary","Selbst et al.","Fairness and Abstraction in Sociotechnical Systems","https://dl.acm.org/doi/10.1145/3287560.3287598","FAccT",2019,"Fairness-aware Machine Learning, Sociotechnical Systems, Interdisciplinary","A key goal of the fair-ML community is to develop machine-learning based systems that, once introduced into a social context, can achieve social and legal outcomes such as fairness, justice, and due process. Bedrock concepts in computer science---such as abstraction and modular design---are used to define notions of fairness and discrimination, to produce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce ""fair"" outcomes. In this paper, however, we contend that these concepts render technical interventions ineffective, inaccurate, and sometimes dangerously misguided when they enter the societal context that surrounds decision-making systems. We outline this mismatch with five ""traps"" that fair-ML work can fall into even as it attempts to be more context-aware in comparison to traditional data science. We draw on studies of sociotechnical systems in Science and Technology Studies to explain why such traps occur and how to avoid them. Finally, we suggest ways in which technical designers can mitigate the traps through a refocusing of design in terms of process rather than solutions, and by drawing abstraction boundaries to include social actors rather than purely technical ones.","ACMDL - FAccT","Critique",513
"137","FAccT","Case Study","Ribeiro et al.","Auditing radicalization pathways on YouTube","https://dl.acm.org/doi/abs/10.1145/3351095.3372879","FAccT",2020,"Radicalization, hate speech, extremism, algorithmic auditing","Non-profits, as well as the media, have hypothesized the existence of a radicalization pipeline on YouTube, claiming that users systematically progress towards more extreme content on the platform. Yet, there is to date no substantial quantitative evidence of this alleged pipeline. To close this gap, we conduct a large-scale audit of user radicalization on YouTube. We analyze 330,925 videos posted on 349 channels, which we broadly classified into four types: Media, the Alt-lite, the Intellectual Dark Web (I.D.W.), and the Alt-right. According to the aforementioned radicalization hypothesis, channels in the I.D.W. and the Alt-lite serve as gateways to fringe far-right ideology, here represented by Alt-right channels. Processing 72M+ comments, we show that the three channel types indeed increasingly share the same user base; that users consistently migrate from milder to more extreme content; and that a large percentage of users who consume Alt-right content now consumed Alt-lite and I.D.W. content in the past. We also probe YouTube's recommendation algorithm, looking at more than 2M video and channel recommendations between May/July 2019. We find that Alt-lite content is easily reachable from I.D.W. channels, while Alt-right videos are reachable only through channel recommendations. Overall, we paint a comprehensive picture of user radicalization on YouTube.","ACMDL - FAccT","Case Study",487
"138","FAccT","Case Study","Wagner et al.","Regulating transparency?: Facebook, Twitter and the German Network Enforcement Act","https://dl.acm.org/doi/abs/10.1145/3351095.3372856","FAccT",2020,NA,"Regulatory regimes designed to ensure transparency often struggle to ensure that transparency is meaningful in practice. This challenge is particularly great when coupled with the widespread usage of dark patterns --- design techniques used to manipulate users. The following article analyses the implementation of the transparency provisions of the German Network Enforcement Act (NetzDG) by Facebook and Twitter, as well as the consequences of these implementations for the effective regulation of online platforms. This question of effective regulation is particularly salient, due to an enforcement action in 2019 by Germany's Federal Office of Justice (BfJ) against Facebook for what the BfJ claim were insufficient compliance with transparency requirements, under NetzDG.","ACMDL - FAccT","Case Study",488
"139","FAccT","Case Study","Marda, Narayan","Data in New Delhi's predictive policing system","https://dl.acm.org/doi/abs/10.1145/3351095.3372865","FAccT",2020,"Fairness-Aware Machine Learning, Predictive Policing, Interdisciplinary, Sociotechnical systems","In 2015, Delhi Police announced plans for predictive policing. The Crime Mapping, Analytics and Predictive System (CMAPS) would be implemented in India's capital, for live spatial hotspot mapping of crime, criminal behavior patterns and suspect analysis. Four years later, there is little known about the effect of CMAPS due to the lack of public accountability mechanisms and large exceptions for law enforcement under India's Right to Information Act. Through an ethnographic study of Delhi Police's data collection practices, and analysing the institutional and legal reality within which CMAPS will function, this paper presents one of the first accounts of smart policing in India. Through our findings and discussion we show what kinds of biases are present within Delhi Police's data collection practices currently and how they translate and transfer into initiatives like CMAPS. We further discuss what the biases in CMAPS can teach us about future public sector deployment of socio-technical systems in India and other global South geographies. We also offer methodological considerations for studying AI deployments in non-western contexts. We conclude with a set of recommendations for civil society and social justice actors to consider when engaging with opaque systems implemented in the public sector.","ACMDL - FAccT","Case Study",489
"140","FAccT","Case Study","Raghavan et al.","Mitigating bias in algorithmic hiring: evaluating claims and practices","https://dl.acm.org/doi/abs/10.1145/3351095.3372828","FAccT",2020,"algorithmic hiring, discrimination law, algorithmic bias","There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.","ACMDL - FAccT","Case Study",490
"141","FAccT","Case Study","Lum, Boudin, Price","The impact of overbooking on a pre-trial risk assessment tool","https://dl.acm.org/doi/abs/10.1145/3351095.3372846","FAccT",2020,"risk assessment, police accountability, overbooking, fairness","Pre-trial risk assessment tools are used to make recommendations to judges about appropriate conditions of pre-trial supervision for people who have been arrested. Increasingly, there is concern about whether these models are operating fairly, including concerns about whether the models' input factors are fair measures of one's criminal activity. In this paper, we assess the impact of booking charges that do not result in a conviction on a popular risk assessment tool, the Arnold Public Safety Assessment. Using data from a pilot run of the tool in San Francisco, CA, we find that booking charges that do not result in a conviction (i.e. charges that are dropped or end in an acquittal) increased the recommended level of pre-trial supervision in around 27% of cases evaluated by the tool.","ACMDL - FAccT","Case Study",491
"142","FAccT","Case Study","Borradaile, Burkhardt, LeClerc","Whose tweets are surveilled for the police: an audit of a social-media monitoring tool via log files","https://dl.acm.org/doi/abs/10.1145/3351095.3372841","FAccT",2020,"social media monitoring, surveillance, police, demographics, keywords, audit","Social media monitoring by law enforcement is becoming commonplace, but little is known about what software packages for it do. Through public records requests, we obtained log files from the Corvallis (Oregon) Police Department's use of social media monitoring software called DigitalStakeout. These log files include the results of proprietary searches by DigitalStakeout that were running over a period of 13 months and include 7240 social media posts. In this paper, we focus on the Tweets logged in this data and consider the racial and ethnic identity (through manual coding) of the users that are therein flagged by DigitalStakeout. We observe differences in the demographics of the users whose Tweets are flagged by DigitalStakeout compared to the demographics of the Twitter users in the region, however, our sample size is too small to determine significance. Further, the demographics of the Twitter users in the region do not seem to reflect that of the residents of the region, with an apparent higher representation of Black and Hispanic people. We also reconstruct the keywords related to a Narcotics report set up by DigitalStakeout for the Corvallis Police Department and find that these keywords flag Tweets unrelated to narcotics or flag Tweets related to marijuana, a drug that is legal for recreational use in Oregon. Almost all of the keywords have a common meaning unrelated to narcotics (e.g. broken, snow, hop, high) that call into question the utility that such a keyword based search could have to law enforcement.","ACMDL - FAccT","Case Study",492
"143","FAccT","Meta-Commentary","Barabas et al.","Studying up: reorienting the study of algorithmic fairness around issues of power","https://dl.acm.org/doi/abs/10.1145/3351095.3372859","FAccT",2020,NA,"Research within the social sciences and humanities has long characterized the work of data science as a sociotechnical process, comprised of a set of logics and techniques that are inseparable from specific social norms, expectations and contexts of development and use. Yet all too often the assumptions and premises underlying data analysis remain unexamined, even in contemporary debates about the fairness of algorithmic systems. This blindspot exists in part because the methodological toolkit used to evaluate the fairness of algorithmic systems remains limited to a narrow set of computational and legal modes of analysis. In this paper, we expand on Elish and Boyd's [17] call for data scientists to develop more robust frameworks for understanding their work as situated practice by examining a specific methodological debate within the field of anthropology, frequently referred to as the practice of ""studying up"". We reflect on the contributions that the call to ""study up"" has made in the field of anthropology before making the case that the field of algorithmic fairness would similarly benefit from a reorientation ""upward"". A case study from our own work illustrates what it looks like to reorient one's research questions ""up"" in a high-profile debate regarding the fairness of an algorithmic system - namely, pretrial risk assessment in American criminal law. We discuss the limitations of contemporary fairness discourse with regard to pretrial risk assessment before highlighting the insights gained when we reframe our research questions to focus on those who inhabit positions of power and authority within the U.S. court system. Finally, we reflect on the challenges we have encountered in implementing data science projects that ""study up"". In the process, we surface new insights and questions about what it means to ethically engage in data science work that directly confronts issues of power and authority.","ACMDL - FAccT","Critique",511
"144","FAccT","Meta-Commentary","Hanna et al.","Towards a critical race methodology in algorithmic fairness","https://dl.acm.org/doi/abs/10.1145/3351095.3372826","FAccT",2020,"algorithmic fairness, critical race theory, race and ethnicity","We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.","ACMDL - FAccT","Critique",512
"145","FAccT","Data Audit","Yang et al.","Towards fairer datasets: filtering and balancing the distribution of the people subtree in the ImageNet hierarchy","https://dl.acm.org/doi/abs/10.1145/3351095.3375709","FAccT",2020,"computer vision, fairness, dataset construction, representative datasets","Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior. In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the person subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively.","ACMDL - FAccT","Data Audit",514
"146","FAccT","Meta-Commentary","Wieringa","What to account for when accounting for algorithms: a systematic literature review on algorithmic accountability","https://dl.acm.org/doi/abs/10.1145/3351095.3372833","FAccT",2020,"Algorithmic accountability, algorithmic systems, data-driven governance, accountability theory","As research on algorithms and their impact proliferates, so do calls for scrutiny/accountability of algorithms. A systematic review of the work that has been done in the field of 'algorithmic accountability' has so far been lacking. This contribution puts forth such a systematic review, following the PRISMA statement. 242 English articles from the period 2008 up to and including 2018 were collected and extracted from Web of Science and SCOPUS, using a recursive query design coupled with computational methods. The 242 articles were prioritized and ordered using affinity mapping, resulting in 93 'core articles' which are presented in this contribution. The recursive search strategy made it possible to look beyond the term 'algorithmic accountability'. That is, the query also included terms closely connected to the theme (e.g. ethics and AI, regulation of algorithms). This approach allows for a perspective not just from critical algorithm studies, but an interdisciplinary overview drawing on material from data studies to law, and from computer science to governance studies. To structure the material, Bovens's widely accepted definition of accountability serves as a focal point. The material is analyzed on the five points Bovens identified as integral to accountability: its arguments on (1) the actor, (2) the forum, (3) the relationship between the two, (3) the content and criteria of the account, and finally (5) the consequences which may result from the account. The review makes three contributions. First, an integration of accountability theory in the algorithmic accountability discussion. Second, a cross-sectoral overview of the that same discussion viewed in light of accountability theory which pays extra attention to accountability risks in algorithmic systems. Lastly, it provides a definition of algorithmic accountability based on accountability theory and algorithmic accountability literature.","ACMDL - FAccT","Meta-Commentary",527
"147","FAccT","Meta-Commentary","Kaminski, Malgieri","Multi-layered explanations from algorithmic impact assessments in the GDPR","https://dl.acm.org/doi/abs/10.1145/3351095.3372875","FAccT",2020,"Law, General Data Protection Regulation, Impact Assessments,","Impact assessments have received particular attention on both sides of the Atlantic as a tool for implementing algorithmic accountability. The aim of this paper is to address how Data Protection Impact Assessments (DPIAs) (Art. 35) in the European Union (EU)'s General Data Protection Regulation (GDPR) link the GDPR's two approaches to algorithmic accountability---individual rights and systemic governance--- and potentially lead to more accountable and explainable algorithms. We argue that algorithmic explanation should not be understood as a static statement, but as a circular and multi-layered transparency process based on several layers (general information about an algorithm, group-based explanations, and legal justification of individual decisions taken). We argue that the impact assessment process plays a crucial role in connecting internal company heuristics and risk mitigation to outward-facing rights, and in forming the substance of several kinds of explanations.","ACMDL - FAccT","Meta-Commentary",528
"148","FAccT","Meta-Commentary","Mustafaraj, Lurie, Devine","The case for voter-centered audits of search engines during political elections","https://dl.acm.org/doi/abs/10.1145/3351095.3372835","FAccT",2020,"algorithm audits, search engines, Google, voters, elections, bias","Search engines, by ranking a few links ahead of million others based on opaque rules, open themselves up to criticism of bias. Previous research has focused on measuring political bias of search engine algorithms to detect possible search engine manipulation effects on voters or unbalanced ideological representation in search results. Insofar that these concerns are related to the principle of fairness, this notion of fairness can be seen as explicitly oriented toward election candidates or political processes and only implicitly oriented toward the public at large. Thus, we ask the following research question: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate? To answer this question, we qualitatively explore four datasets about elections and politics in the United States: 1) a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections, 2) a dataset of biased political phrases used in a large-scale Google audit ahead of the 2018 U.S. election, 3) Google's ""related searches"" phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women), and 4) autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019. We find that voters have much broader information needs than the search engine audit literature has accounted for in the past, and that relying on political science theories of voter modeling provides a good starting point for informing the design of voter-centered audits.","ACMDL - FAccT","Meta-Commentary",529
"149","FAccT","Meta-Commentary","Raji et al.","Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing","https://dl.acm.org/doi/abs/10.1145/3351095.3372873","FAccT",2020,"Algorithmic audits, machine learning, accountability, responsible innovation","Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source.","ACMDL - FAccT","Method",530
"150","FAccT","Meta-Commentary","Roldan, Vila, Marca","Dirichlet uncertainty wrappers for actionable algorithm accuracy accountability and auditability","https://dl.acm.org/doi/abs/10.1145/3351095.3372825","FAccT",2020,"auditability, accuracy, uncertainty, black-box models, machine learning","Nowadays, the use of machine learning models is becoming a utility in many applications. Companies deliver pre-trained models encapsulated as application programming interfaces (APIs) that developers combine with third-party components and their own models and data to create complex data products to solve specific problems. The complexity of such products and the lack of control and knowledge of the internals of each component used unavoidable cause effects, such as lack of transparency, difficulty in auditability, and the emergence of potential uncontrolled risks. They are effectively black-boxes. Accountability of such solutions is a challenge for the auditors and the machine learning community.","ACMDL - FAccT","Method",533
"151","FAccT","Case Study","Albert, Delano","This Whole Thing Smacks of Gender: Algorithmic Exclusion in Bioimpedance-based Body Composition Analysis","https://dl.acm.org/doi/10.1145/3442188.3445898","FAccT",2021,"data collection and curation, sex/gender, bioelectrical impedance analysis, body composition, critical data/algorithm studies, science and technology studies, critical HCI and the design of algorithmic systems","Smart weight scales offer bioimpedance-based body composition analysis as a supplement to pure body weight measurement. Companies such as Withings and Fitbit tout composition analysis as providing self-knowledge and the ability to make more informed decisions. However, these aspirational statements elide the reality that these numbers are a product of proprietary regression equations that require a binary sex/gender as their input. Our paper combines transgender studies-influenced personal narrative with an analysis of the scientific basis of bioimpedance technology used as part of the Withings smart scale. Attempting to include nonbinary people reveals that bioelectrical impedance analysis has always rested on physiologically shaky ground. White nonbinary people are merely the tip of the iceberg of those who may find that their smart scale is not so intelligent when it comes to their bodies. Using body composition analysis as an example, we explore how the problem of trans and nonbinary inclusion in personal health tech goes beyond the issues of adding a third ""gender"" box or slapping a rainbow flag on the packaging. We also provide recommendations as to how to approach creating more inclusive technologies even while still relying on exclusionary data.","ACMDL - FAccT","Case Study",484
"152","FAccT","Case Study","Dash et al.","When the Umpire is also a Player: Bias in Private Label Product Recommendations on E-commerce Marketplaces","https://dl.acm.org/doi/10.1145/3442188.3445944","FAccT",2021,"Recommendation, e-commerce marketplace, algorithmic auditing","Algorithmic recommendations mediate interactions between millions of customers and products (in turn, their producers and sellers) on large e-commerce marketplaces like Amazon. In recent years, the producers and sellers have raised concerns about the fairness of black-box recommendation algorithms deployed on these marketplaces. Many complaints are centered around marketplaces biasing the algorithms to preferentially favor their own 'private label' products over competitors. These concerns are exacerbated as marketplaces increasingly de-emphasize or replace 'organic' recommendations with ad-driven 'sponsored' recommendations, which include their own private labels. While these concerns have been covered in popular press and have spawned regulatory investigations, to our knowledge, there has not been any public audit of these marketplace algorithms. In this study, we bridge this gap by performing an end-to-end systematic audit of related item recommendations on Amazon. We propose a network-centric framework to quantify and compare the biases across organic and sponsored related item recommendations. Along a number of our proposed bias measures, we find that the sponsored recommendations are significantly more biased toward Amazon private label products compared to organic recommendations. While our findings are primarily interesting to producers and sellers on Amazon, our proposed bias measures are generally useful for measuring link formation bias in any social or content networks.","ACMDL - FAccT","Case Study",485
"153","FAccT","Case Study","Srinivasan, Uchino","Biases in Generative Art: A Causal Look from the Lens of Art History","https://dl.acm.org/doi/pdf/10.1145/3442188.3445869","FAccT",2021,"generative art, style transfer, biases, AI, socio-cultural impacts","With rapid progress in artificial intelligence (AI), popularity of generative art has grown substantially. From creating paintings to generating novel art styles, AI based generative art has showcased a variety of applications. However, there has been little focus concerning the ethical impacts of AI based generative art. In this work, we investigate biases in the generative art AI pipeline right from those that can originate due to improper problem formulation to those related to algorithm design. Viewing from the lens of art history, we discuss the socio-cultural impacts of these biases. Leveraging causal models, we highlight how current methods fall short in modeling the process of art creation and thus contribute to various types of biases. We illustrate the same through case studies, in particular those related to style transfer. To the best of our knowledge, this is the first extensive analysis that investigates biases in the generative art AI pipeline from the perspective of art history. We hope our work sparks interdisciplinary discussions related to accountability of generative art.","ACMDL - FAccT","Case Study",486
"154","FAccT","Case Study","Wilson et al.","Building and Auditing Fair Algorithms: A Case Study in Candidate Screening","https://dl.acm.org/doi/10.1145/3442188.3445928","FAccT",2021,"algorithm auditing, four-fifths rule, adverse impact testing, fairness","Academics, activists, and regulators are increasingly urging companies to develop and deploy sociotechnical systems that are fair and unbiased. Achieving this goal, however, is complex: the developer must (1) deeply engage with social and legal facets of ""fairness"" in a given context, (2) develop software that concretizes these values, and (3) undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms. To date, there are few examples of companies that have transparently undertaken all three steps.","ACMDL - FAccT","Case Study",500
"155","FAccT","Case Study","Steed, Caliskan","Image Representations Learned With Unsupervised Pre-Training Contain Human-like Biases","https://dl.acm.org/doi/10.1145/3442188.3445932","FAccT",2021,"implicit bias, unsupervised learning, computer vision","Recent advances in machine learning leverage massive datasets of unlabeled images from the web to learn general-purpose image representations for tasks from image classification to face recognition. But do unsupervised computer vision models automatically learn implicit patterns and embed social biases that could have harmful downstream effects? We develop a novel method for quantifying biased associations between representations of social concepts and attributes in images. We find that state-of-the-art unsupervised models trained on ImageNet, a popular benchmark image dataset curated from internet images, automatically learn racial, gender, and intersectional biases. We replicate 8 previously documented human biases from social psychology, from the innocuous, as with insects and flowers, to the potentially harmful, as with race and gender. Our results closely match three hypotheses about intersectional bias from social psychology. For the first time in unsupervised computer vision, we also quantify implicit human biases about weight, disabilities, and several ethnicities. When compared with statistical patterns in online image datasets, our findings suggest that machine learning models can automatically learn bias from the way people are stereotypically portrayed on the web.","ACMDL - FAccT","Case Study",504
"156","FAccT","Case Study","Cho et al.","Towards Cross-Lingual Generalization of Translation Gender Bias","https://dl.acm.org/doi/10.1145/3442188.3445907","FAccT",2021,"machine translation, gender bias, evaluation, cross-linguality","Cross-lingual generalization issues for less explored languages have been broadly tackled in recent NLP studies. In this study, we apply the philosophy on the problem of translation gender bias, which necessarily involves multilingualism and socio-cultural diversity. Beyond the conventional evaluation criteria for the social bias, we aim to put together various aspects of linguistic viewpoints into the measuring process, to create a template that makes evaluation less tilted to specific types of language pairs. With a manually constructed set of content words and template, we check both the accuracy of gender inference and the fluency of translation, for German, Korean, Portuguese, and Tagalog. Inference accuracy and disparate impact, namely the biasedness factors associated with each other, show that the failure of bias mitigation threatens the delicacy of translation. Furthermore, our analyses on each system and language indicate that the translation fluency and inference accuracy are not necessarily correlated. The results implicitly suggest that the amount of available language resources that boost up the performance might amplify the bias cross-linguistically.","ACMDL - FAccT","Case Study",505
"157","FAccT","Data Audit","Yang, Roberts","Censorship of Online Encyclopedias: Implications for NLP Models","https://dl.acm.org/doi/10.1145/3442188.3445916","FAccT",2021,"word embeddings, censorship, training data, machine learning","While artificial intelligence provides the backbone for many tools people use around the world, recent work has brought to attention that the algorithms powering AI are not free of politics, stereotypes, and bias. While most work in this area has focused on the ways in which AI can exacerbate existing inequalities and discrimination, very little work has studied how governments actively shape training data. We describe how censorship has affected the development of Wikipedia corpuses, text data which are regularly used for pre-trained inputs into NLP algorithms. We show that word embeddings trained on Baidu Baike, an online Chinese encyclopedia, have very different associations between adjectives and a range of concepts about democracy, freedom, collective action, equality, and people and historical events in China than its regularly blocked but uncensored counterpart - Chinese language Wikipedia. We examine the implications of these discrepancies by studying their use in downstream AI applications. Our paper shows how government repression, censorship, and self-censorship may impact training data and the applications that draw from them.","ACMDL - FAccT","Data Audit",516
"158","FAccT","Data Audit","Khan, Fu","One Label, One Billion Faces: Usage and Consistency of Racial Categories in Computer Vision","https://dl.acm.org/doi/10.1145/3442188.3445920","FAccT",2021,"datasets, bias, fairness, faces, computer vision, race","Computer vision is widely deployed, has highly visible, society-altering applications, and documented problems with bias and representation. Datasets are critical for benchmarking progress in fair computer vision, and often employ broad racial categories as population groups for measuring group fairness. Similarly, diversity is often measured in computer vision datasets by ascribing and counting categorical race labels. However, racial categories are ill-defined, unstable temporally and geographically, and have a problematic history of scientific use. Although the racial categories used across datasets are superficially similar, the complexity of human race perception suggests the racial system encoded by one dataset may be substantially inconsistent with another. Using the insight that a classifier can learn the racial system encoded by a dataset, we conduct an empirical study of computer vision datasets supplying categorical race labels for face images to determine the cross-dataset consistency and generalization of racial categories. We find that each dataset encodes a substantially unique racial system, despite nominally equivalent racial categories, and some racial categories are systemically less consistent than others across datasets. We find evidence that racial categories encode stereotypes, and exclude ethnic groups from categories on the basis of nonconformity to stereotypes. Representing a billion humans under one racial category may obscure disparities and create new ones by encoding stereotypes of racial systems. The difficulty of adequately converting the abstract concept of race into a tool for measuring fairness underscores the need for a method more flexible and culturally aware than racial categories.","ACMDL - FAccT","Data Audit",517
"159","FAccT","Meta-Commentary","Kacianka, Pretschner","Designing Accountable Systems","https://dl.acm.org/doi/10.1145/3442188.3445905","FAccT",2021,"Accountability, Structural Causal Models, Socio-Technical Systems","Accountability is an often called for property of technical systems. It is a requirement for algorithmic decision systems, autonomous cyber-physical systems, and for software systems in general. As a concept, accountability goes back to the early history of Liberalism and is suggested as a tool to limit the use of power. This long history has also given us many, often slightly differing, definitions of accountability. The problem that software developers now face is to understand what accountability means for their systems and how to reflect it in a system's design. To enable the rigorous study of accountability in a system, we need models that are suitable for capturing such a varied concept. In this paper, we present a method to express and compare different definitions of accountability using Structural Causal Models. We show how these models can be used to evaluate a system's design and present a small use case based on an autonomous car.","ACMDL - FAccT","Meta-Commentary",524
"160","FAccT","Meta-Commentary","Metcalf et al.","Algorithmic Impact Assessments and Accountability: The Co-construction of Impacts","https://dl.acm.org/doi/10.1145/3442188.3445935","FAccT",2021,"algorithmic impact assessment, impact, harm, accountability, governance","Algorithmic impact assessments (AIAs) are an emergent form of accountability for organizations that build and deploy automated decision-support systems. They are modeled after impact assessments in other domains. Our study of the history of impact assessments shows that ""impacts"" are an evaluative construct that enable actors to identify and ameliorate harms experienced because of a policy decision or system. Every domain has different expectations and norms around what constitutes impacts and harms, how potential harms are rendered as impacts of a particular undertaking, who is responsible for conducting such assessments, and who has the authority to act on them to demand changes to that undertaking. By examining proposals for AIAs in relation to other domains, we find that there is a distinct risk of constructing algorithmic impacts as organizationally understandable metrics that are nonetheless inappropriately distant from the harms experienced by people, and which fall short of building the relationships required for effective accountability. As impact assessments become a commonplace process for evaluating harms, the FAccT community, in its efforts to address this challenge, should A) understand impacts as objects that are co-constructed accountability relationships, B) attempt to construct impacts as close as possible to actual harms, and C) recognize that accountability governance requires the input of various types of expertise and affected communities. We conclude with lessons for assembling cross-expertise consensus for the co-construction of impacts and building robust accountability relationships.","ACMDL - FAccT","Meta-Commentary",525
"161","FAccT","Meta-Commentary","Cobbe, Lee, Singh","Reviewable Automated Decision-Making: A Framework for Accountable Algorithmic Systems","https://dl.acm.org/doi/10.1145/3442188.3445921","FAccT",2021,"Algorithmic systems, automated decision-making, accountability, audit, artificial intelligence, machine learning","This paper introduces reviewability as a framework for improving the accountability of automated and algorithmic decisionmaking (ADM) involving machine learning. We draw on an understanding of ADM as a socio-technical process involving both human and technical elements, beginning before a decision is made and extending beyond the decision itself. While explanations and other model-centric mechanisms may assist some accountability concerns, they often provide insufficient information of these broader ADM processes for regulatory oversight and assessments of legal compliance. Reviewability involves breaking down the ADM process into technical and organisational elements to provide a systematic framework for determining the contextually appropriate record-keeping mechanisms to facilitate meaningful review - both of individual decisions and of the process as a whole. We argue that a reviewability framework, drawing on administrative law's approach to reviewing human decision-making, offers a practical way forward towards more a more holistic and legally-relevant form of accountability for ADM.","ACMDL - FAccT","Meta-Commentary",526
"162","FAccT","Meta-Commentary","Kroll","Outlining Traceability: A Principle for Operationalizing Accountability in Computing Systems","https://dl.acm.org/doi/10.1145/3442188.3445937","FAccT",2021,"traceability, accountability, transparency, AI principles, AI ethics","Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes.","ACMDL - FAccT","Method",532
"163","FAccT","Meta-Commentary","Krafft et al.","An Action-Oriented AI Policy Toolkit for Technology Audits by Community Advocates and Activists","https://dl.acm.org/doi/10.1145/3442188.3445938","FAccT",2021,"Participatory design, participatory action research, accountability, algorithmic equity, algorithmic justice, surveillance, regulation","Motivated by the extensive documented disparate harms of artificial intelligence (AI), many recent practitioner-facing reflective tools have been created to promote responsible AI development. However, the use of such tools internally by technology development firms addresses responsible AI as an issue of closed-door compliance rather than a matter of public concern. Recent advocate and activist efforts intervene in AI as a public policy problem, inciting a growing number of cities to pass bans or other ordinances on AI and surveillance technologies. In support of this broader ecology of political actors, we present a set of reflective tools intended to increase public participation in technology advocacy for AI policy action. To this end, the Algorithmic Equity Toolkit (the AEKit) provides a practical policy-facing definition of AI, a flowchart for assessing technologies against that definition, a worksheet for decomposing AI systems into constituent parts, and a list of probing questions that can be posed to vendors, policy-makers, or government agencies. The AEKit carries an action-orientation towards political encounters between community groups in the public and their representatives, opening up the work of AI reflection and remediation to multiple points of intervention. Unlike current reflective tools available to practitioners, our toolkit carries with it a politics of community participation and activism.","ACMDL - FAccT","Tool",534
"164","FAccT","Case Study","Wolfe, Banaji, Caliskan","Evidence for Hypodescent in Visual Semantic AI","https://dl.acm.org/doi/10.1145/3531146.3533185","FAccT",2022,"multimodal, bias in AI, visual semantics, language-image models, racial bias, hypodescent","We examine the state-of-the-art multimodal ”visual semantic” model CLIP (”Contrastive Language Image Pretraining”) for the rule of hypodescent, or one-drop rule, whereby multiracial people are more likely to be assigned a racial or ethnic label corresponding to a minority or disadvantaged racial or ethnic group than to the equivalent majority or advantaged group. A face morphing experiment grounded in psychological research demonstrating hypodescent indicates that, at the midway point of 1,000 series of morphed images, CLIP associates 69.7% of Black-White female images with a Black text label over a White text label, and similarly prefers Latina (75.8%) and Asian (89.1%) text labels at the midway point for Latina-White female and Asian-White female morphs, reflecting hypodescent. Additionally, assessment of the underlying cosine similarities in the model reveals that association with White is correlated with association with ”person,” with Pearson’s ρ as high as 0.82, p < 10− 90 over a 21,000-image morph series, indicating that a White person corresponds to the default representation of a person in CLIP. Finally, we show that the stereotype-congruent pleasantness association of an image correlates with association with the Black text label in CLIP, with Pearson’s ρ = 0.48, p < 10− 90 for 21,000 Black-White multiracial male images, and ρ = 0.41, p < 10− 90 for Black-White multiracial female images. CLIP is trained on English-language text gathered using data collected from an American website (Wikipedia), and our findings demonstrate that CLIP embeds the values of American racial hierarchy, reflecting the implicit and explicit beliefs that are present in human minds. We contextualize these findings within the history of and psychology of hypodescent. Overall, the data suggests that AI supervised using natural language will, unless checked, learn biases that reflect racial hierarchies.","ACMDL - FAccT","Case Study",480
"165","FAccT","Case Study","Wang, Zhu","How are ML-Based Online Content Moderation Systems Actually Used? Studying Community Size, Local Activity, and Disparate Treatment","https://dl.acm.org/doi/10.1145/3531146.3533147","FAccT",2022,"Content moderation, Wikipedia, Causal inference, Fairness, Online communities","Machine learning-based predictive systems are increasingly used to assist online groups and communities in various content moderation tasks. However, there are limited quantitative understandings of whether and how different groups and communities use such predictive systems differently according to their community characteristics. In this research, we conducted a field evaluation of how content moderation systems are used in 17 Wikipedia language communities. We found that 1) larger communities tend to use predictive systems to identify the most damaging edits, while smaller communities tend to use them to identify any edit that could be damaging; 2) predictive systems are used less in content areas where there are more local editing activities; 3) predictive systems have mixed effects on reducing disparate treatment between anonymous and registered editors across communities of different characteristics. Finally, we discuss the theoretical and practical implications for future human-centered moderation algorithms.","ACMDL - FAccT","Case Study",481
"166","FAccT","Case Study","Wolfe, Caliskan","Markedness in Visual Semantic AI","https://dl.acm.org/doi/pdf/10.1145/3531146.3533183","FAccT",2022,"multimodal, bias in AI, visual semantics, language-and-vision AI, markedness, age bias","We evaluate a state-of-the-art multimodal ""visual semantic"" model, OpenAI's CLIP (""Contrastive Language Image Pretraining""), for biases related to the marking of age, gender, and race or ethnicity. Given the option to label an image as ""a photo of a person"" or to select any of seven labels which marks the race or ethnicity of a person, CLIP chooses the ""person"" label 47.9% of the time for individuals perceived to be White according to the FairFace computer vision dataset, compared with 5.0% or less for people perceived to be Black, East Asian, Southeast Asian, Indian, or Latino or Hispanic. The model is also more likely to rank the unmarked ""person"" label higher than any gender label for people perceived to be Male (26.7% of the time) vs. people perceived to be Female (15.2% of the time). Perceived age affects whether an individual is marked by the model: people perceived to be Female are more likely to be marked based on gender at younger ages (under 20), but less likely to be marked with an age label, while those over the age of 40 are much more likely to be marked based on age than people perceived to be Male. We trace our results back to the CLIP embedding space by examining the self-similarity for each social group, where higher self-similarity denotes greater attention directed by CLIP to the shared characteristics i.e., age, race, or gender) of the social group. As age increases, the self-similarity of representations of people perceived to be Female increases at ahigher rate than for people perceived to be Male, with the disparity most pronounced at the ""more than 70"" age range. Six of the ten least self-similar social groups are people perceived to be White and Male, while all ten of the most self-similar social groups are people perceived to be under the age of 10 or over the age of 70. Existing biases of self-similarity and markedness between Male and Female gender groups are further exacerbated when the groups compared are people perceived to be White and Male and people perceived to be Black and Female. CLIP is an English-language model trained on internet content gathered based on data from an American website (Wikipedia), and our results indicate that CLIP reflects the biases of the language and society which produced the data on which it was trained.","ACMDL - FAccT","Case Study",482
"167","FAccT","Case Study","Ganguli et al.","Predictability and Surprise in Large Generative Models","https://dl.acm.org/doi/10.1145/3531146.3533229","FAccT",2022,NA,"Large-scale pre-training has recently emerged as a technique for creating capable, general-purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have a paradoxical combination of predictable loss on a broad training distribution (as embodied in their ”scaling laws”), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend for this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, funders who want to support work addressing these challenges, and academics who want to analyze, critique, and potentially develop large generative models.","ACMDL - FAccT","Case Study",483
"168","FAccT","Case Study","Pahl et al.","Female, white, 27? Bias Evaluation on Data and Algorithms for Affect Recognition in Faces","https://dl.acm.org/doi/10.1145/3531146.3533159","FAccT",2022,"affective computing, action units, categorical emotions, metadata post-annotation, bias, fairness, data evaluation, algorithm evaluation","Nowadays, Artificial Intelligence (AI) algorithms show a strong performance for many use cases, making them desirable for real-world scenarios where the algorithms provide high-impact decisions. However, one major drawback of AI algorithms is their susceptibility to bias and resulting unfairness. This has a huge influence for their application, as they have a higher failure rate for certain subgroups. In this paper, we focus on the field of affective computing and particularly on the detection of bias for facial expressions. Depending on the deployment scenario, bias in facial expression models can have a disadvantageous impact and it is therefore essential to evaluate the bias and limitations of the model. In order to analyze the metadata distribution in affective computing datasets, we annotate several benchmark training datasets, containing both Action Units and categorical emotions, with age, gender, ethnicity, glasses, and beards. We show that there is a significantly skewed distribution, particularly for ethnicity and age. Based on this metadata annotation, we evaluate two trained state-of-the-art affective computing algorithms. Our evaluation shows that the strongest bias is in age, with the best performance for persons under 34 and a sharp decrease for older persons. Furthermore, we see an ethnicity bias with varying direction depending on the algorithm, a slight gender bias and worse performance for facial parts occluded by glasses.","ACMDL - FAccT","Case Study",501
"169","FAccT","Case Study","Hutiri, Ding","Bias in Automated Speaker Recognition","https://dl.acm.org/doi/10.1145/3531146.3533089","FAccT",2022,"speaker recognition, speaker verification, bias, fairness, audit, evaluation","Automated speaker recognition uses data processing to identify speakers by their voice. Today, automated speaker recognition is deployed on billions of smart devices and in services such as call centres. Despite their wide-scale deployment and known sources of bias in related domains like face recognition and natural language processing, bias in automated speaker recognition has not been studied systematically. We present an in-depth empirical and analytical study of bias in the machine learning development workflow of speaker verification, a voice biometric and core task in automated speaker recognition. Drawing on an established framework for understanding sources of harm in machine learning, we show that bias exists at every development stage in the well-known VoxCeleb Speaker Recognition Challenge, including data generation, model building, and implementation. Most affected are female speakers and non-US nationalities, who experience significant performance degradation. Leveraging the insights from our findings, we make practical recommendations for mitigating bias in automated speaker recognition, and outline future research directions.","ACMDL - FAccT","Case Study",502
"170","FAccT","Case Study","Hundt et al","Robots Enact Malignant Stereotypes","https://dl.acm.org/doi/10.1145/3531146.3533138","FAccT",2022,NA,"Stereotypes, bias, and discrimination have been extensively documented in Machine Learning (ML) methods such as Computer Vision (CV) [18, 80], Natural Language Processing (NLP) [6], or both, in the case of large image and caption models such as OpenAI CLIP [14]. In this paper, we evaluate how ML bias manifests in robots that physically and autonomously act within the world. We audit one of several recently published CLIP-powered robotic manipulation methods, presenting it with objects that have pictures of human faces on the surface which vary across race and gender, alongside task descriptions that contain terms associated with common stereotypes. Our experiments definitively show robots acting out toxic stereotypes with respect to gender, race, and scientifically-discredited physiognomy, at scale. Furthermore, the audited methods are less likely to recognize Women and People of Color. Our interdisciplinary sociotechnical analysis synthesizes across fields and applications such as Science Technology and Society (STS), Critical Studies, History, Safety, Robotics, and AI. We find that robots powered by large datasets and Dissolution Models (sometimes called “foundation models”, e.g. CLIP) that contain humans risk physically amplifying malignant stereotypes in general; and that merely correcting disparities will be insufficient for the complexity and scale of the problem. Instead, we recommend that robot learning methods that physically manifest stereotypes or other harmful outcomes be paused, reworked, or even wound down when appropriate, until outcomes can be proven safe, effective, and just. Finally, we discuss comprehensive policy changes and the potential of new interdisciplinary research on topics like Identity Safety Assessment Frameworks and Design Justice to better understand and address these harms.","ACMDL - FAccT","Case Study",503
"171","FAccT","Meta-Commentary","Lum, Zhang, Bower","De-biasing “bias” measurement","https://dl.acm.org/doi/10.1145/3531146.3533105","FAccT",2022,NA,"When a model’s performance differs across socially or culturally relevant groups–like race, gender, or the intersections of many such groups–it is often called ”biased.” While much of the work in algorithmic fairness over the last several years has focused on developing various definitions of model fairness (the absence of group-wise model performance disparities) and eliminating such “bias,” much less work has gone into rigorously measuring it. In practice, it important to have high quality, human digestible measures of model performance disparities and associated uncertainty quantification about them that can serve as inputs into multi-faceted decision-making processes. In this paper, we show both mathematically and through simulation that many of the metrics used to measure group-wise model performance disparities are themselves statistically biased estimators of the underlying quantities they purport to represent. We argue that this can cause misleading conclusions about the relative group-wise model performance disparities along different dimensions, especially in cases where some sensitive variables consist of categories with few members. We propose the “double-corrected” variance estimator, which provides unbiased estimates and uncertainty quantification of the variance of model performance across groups. It is conceptually simple and easily implementable without statistical software package or numerical optimization. We demonstrate the utility of this approach through simulation and show on a real dataset that while statistically biased estimators of model group-wise model performance disparities indicate statistically significant between-group model performance disparities, when accounting for statistical bias in the estimator, the estimated group-wise disparities in model performance are no longer statistically significant.","ACMDL - FAccT","Critique",506
"172","FAccT","Meta-Commentary","Hutchinson et al.","Evaluation Gaps in Machine Learning Practice","https://dl.acm.org/doi/10.1145/3531146.3533233","FAccT",2022,"machine learning, evaluation, applications","Forming a reliable judgement of a machine learning (ML) model’s appropriateness for an application ecosystem is critical for its responsible use, and requires considering a broad range of factors including harms, benefits, and responsibilities. In practice, however, evaluations of ML models frequently focus on only a narrow range of decontextualized predictive behaviours. We examine the evaluation gaps between the idealized breadth of evaluation concerns and the observed narrow focus of actual evaluations. Through an empirical study of papers from recent high-profile conferences in the Computer Vision and Natural Language Processing communities, we demonstrate a general focus on a handful of evaluation methods. By considering the metrics and test data distributions used in these methods, we draw attention to which properties of models are centered in the field, revealing the properties that are frequently neglected or sidelined during evaluation. By studying these properties, we demonstrate the machine learning discipline’s implicit assumption of a range of commitments which have normative impacts; these include commitments to consequentialism, abstractability from context, the quantifiability of impacts, the limited role of model inputs in evaluation, and the equivalence of different failure modes. Shedding light on these assumptions enables us to question their appropriateness for ML system contexts, pointing the way towards more contextualized evaluation methodologies for robustly examining the trustworthiness of ML models.","ACMDL - FAccT","Critique",507
"173","FAccT","Meta-Commentary","Young, Katell, Krafft","Confronting Power and Corporate Capture at the FAccT Conference","https://dl.acm.org/doi/10.1145/3531146.3533194","FAccT",2022,"pymetrics, conflict of interest, corporate capture, industry engagement, research funding, agonism","Fields such as medicine and public health attest to deep conflict of interest concerns present when private companies fund evaluation of their own products and services. We draw on these lessons to consider corporate capture of the ACM Fairness, Accountability, and Transparency (FAccT) conference. We situate our analysis within scholarship on the entanglement of industry and academia and focus on the silences it produces in the research record. Our analysis of the institutional design at FAccT indicates the conference’s neglect of those people most negatively impacted by algorithmic systems. We focus on a 2021 paper by Wilson et al., “Building and auditing fair algorithms: A case study in candidate screening” as a key example of conflicted research accepted via peer review at FAccT. We call on the conference to (1) lead on models for how to manage conflicts of interest in the field of computing beyond individual disclosure of funding sources, (2) hold space for advocates and activists able to speak directly to questions of algorithmic harm, and (3) reconstitute the conference with attention to fostering agonistic dissensus—un-making the present manufactured consensus and nurturing challenges to power. These changes will position our community to contend with the political dimensions of research on AI harms.","ACMDL - FAccT","Critique",508
"174","FAccT","Meta-Commentary","Raji et al.","The Fallacy of AI Functionality","https://doi.org/10.1145/3531146.3533158","FAccT",2022,NA,"Deployed AI systems often do not work. They can be constructed haphazardly, deployed indiscriminately, and promoted deceptively. However, scholars, the press, and policymakers pay too little attention to functionality. This leads to technical and policy solutions focused on “ethical” or value-aligned deployments, often skipping over the prior question of whether a given system functions, or provides any benefits at all. To describe the harms of various types of functionality failures, we create a taxonomy of known AI functionality issues. We then point to policy and organizational responses that are often overlooked and become more readily available once functionality is drawn into focus. We argue that functionality is a meaningful AI policy issue, operating as a necessary first step towards protecting affected communities from algorithmic harm.","ACMDL - FAccT","Critique",509
"175","FAccT","Meta-Commentary","Birhane et al.","The forgotten margins of AI ethics","https://doi.org/10.1145/3531146.3533157","FAccT",2022,"AI Ethics, Trends, Justice, FAccT, AIES","How has recent AI Ethics literature addressed topics such as fairness and justice in the context of continued structural power asymmetries? We trace both the historical roots and current landmark work that have been shaping the field and we categorize these works under three broad umbrellas: (i) those grounded in Western canonical philosophy, (ii) mathematical and statistical methods,and (iii) those emerging from critical data/algorithm/information studies. We also survey the field and explore emerging trends by examining the rapidly growing body of literature that falls under the broad umbrella ofAI Ethics. To that end, we read and annotated peer-reviewed papers published over the past four years in two premier conferences; FAccT and AIES. We classified the literature based on an annotation scheme we developed according to three main dimensions: whether the paper deals with concrete applications,use-cases, and/or people’s lived experience; to what extent it addresses harmed, threatened, or otherwise marginalized groups; and if so, whether it explicitly names such groups. We note that although the goals of various annotated papers were often commendable,there exists a problematic shallow consideration of the negative impacts of AI on traditionally marginalized groups. Taken together,our conceptual analysis and the data from annotated papers indicates that the field would benefit from an increased focus on ethical analysis grounded in concrete use-cases, people, and applications that incorporates structural and historical power asymmetries.","ACMDL - FAccT","Critique",510
"176","FAccT","Data Audit","Pastaltzidis et al.","Data augmentation for fairness-aware machine learning: Preventing algorithmic bias in law enforcement systems","https://dl.acm.org/doi/10.1145/3531146.3534644","FAccT",2022,"computer vision, fairness, algorithmic bias, AI ethics, violence detection, law enforcement technology","Researchers and practitioners in the fairness community have highlighted the ethical and legal challenges of using biased datasets in data-driven systems, with algorithmic bias being a major concern. Despite the rapidly growing body of literature on fairness in algorithmic decision-making, there remains a paucity of fairness scholarship on machine learning algorithms for the real-time detection of crime. This contribution presents an approach for fairness-aware machine learning to mitigate the algorithmic bias / discrimination issues posed by the reliance on biased data when building law enforcement technology. Our analysis is based on RWF-2000, which has served as the basis for violent activity recognition tasks in data-driven law enforcement projects. We reveal issues of overrepresentation of minority subjects in violence situations that limit the external validity of the dataset for real-time crime detection systems and propose data augmentation techniques to rebalance the dataset. The experiments on real world data show the potential to create more balanced datasets by synthetically generated samples, thus mitigating bias and discrimination concerns in law enforcement applications.","ACMDL - FAccT","Data Audit",515
"177","FAccT","Data Audit","Hirota, Nakashima, Garcia","Gender and Racial Bias in Visual Question Answering Datasets","https://dl.acm.org/doi/10.1145/3531146.3533184","FAccT",2022,"visual question answering, gender stereotype, racial stereotype, datasets","Vision-and-language tasks have increasingly drawn more attention as a means to evaluate human-like reasoning in machine learning models. A popular task in the field is visual question answering (VQA), which aims to answer questions about images. However, VQA models have been shown to exploit language bias by learning the statistical correlations between questions and answers without looking into the image content: e.g., questions about the color of a banana are answered with yellow, even if the banana in the image is green. If societal bias (e.g., sexism, racism, ableism, etc.) is present in the training data, this problem may be causing VQA models to learn harmful stereotypes. For this reason, we investigate gender and racial bias in five VQA datasets. In our analysis, we find that the distribution of answers is highly different between questions about women and men, as well as the existence of detrimental gender-stereotypical samples. Likewise, we identify that specific race-related attributes are underrepresented, whereas potentially discriminatory samples appear in the analyzed datasets. Our findings suggest that there are dangers associated to using VQA datasets without considering and dealing with the potentially harmful stereotypes. We conclude the paper by proposing solutions to alleviate the problem before, during, and after the dataset collection process.","ACMDL - FAccT","Data Audit",518
"178","FAccT","Ecosystem","Scott et al.","Algorithmic Tools in Public Employment Services: Towards a Jobseeker-Centric Perspective","https://dl.acm.org/doi/10.1145/3531146.3534631","FAccT",2022,"Public Employment Services, Participatory Design, Algorithmic Decision-Making","Data-driven and algorithmic systems have been introduced to support Public Employment Services (PES) throughout the world. Their deployment has sparked public controversy and, as a consequence, some of these systems have been removed from use or their role was reduced. Yet the implementation of similar systems continues. In this paper, we use a participatory approach to determine a course forward for research and development in this area. We draw attention to the needs and expectations of people directly affected by these systems, i.e., jobseekers. Our investigation comprises two workshops: the first a fact-finding workshop with academics, system developers, the public sector, and civil-society organizations, the second a co-design workshop with 13 unemployed migrants to Germany. Based on the discussion in the fact-finding workshop we identified challenges of existing PES (algorithmic) systems. From the co-design workshop we identified our participants’ needs and desires when contacting PES: the need for human contact, the expectation to receive genuine orientation, and the desire to be seen as a whole human being. We map these expectations to three design considerations for data-driven and algorithmic systems for PES: the importance of interpersonal interaction, jobseeker assessment as direction, and the challenge of mitigating misrepresentation. Finally, we argue that the limitations and risks of current systems cannot be addressed through minor adjustments but require a more fundamental change to the role of PES.","ACMDL - FAccT","Ecosystem",519
"179","FAccT","Ecosystem","Stapleton et al.","Imagining new futures beyond predictive systems in child welfare: A qualitative study with impacted stakeholders","https://dl.acm.org/doi/10.1145/3531146.3533177","FAccT",2022,"child welfare; machine learning; participatory design; human-centered AI; impacted stakeholder","Child welfare agencies across the United States are turning to data-driven predictive technologies (commonly called predictive analytics) which use government administrative data to assist workers’ decision-making. While some prior work has explored impacted stakeholders’ concerns with current uses of data-driven predictive risk models (PRMs), less work has asked stakeholders whether such tools ought to be used in the first place. In this work, we conducted a set of seven design workshops with 35 stakeholders who have been impacted by the child welfare system or who work in it to understand their beliefs and concerns around PRMs, and to engage them in imagining new uses of data and technologies in the child welfare system. We found that participants worried current PRMs perpetuate or exacerbate existing problems in child welfare. Participants suggested new ways to use data and data-driven tools to better support impacted communities and suggested paths to mitigate possible harms of these tools. Participants also suggested low-tech or no-tech alternatives to PRMs to address problems in child welfare. Our study sheds light on how researchers and designers can work in solidarity with impacted communities, possibly to circumvent or oppose child welfare agencies.","ACMDL - FAccT","Ecosystem",520
"180","FAccT","Ecosystem","Ehsan et al.","The Algorithmic Imprint","https://doi.org/10.1145/3531146.3533186","FAccT",2022,"Algorithmic Imprint, Algorithmic Impact Assessment, Situated Fairness, Infrastructure, Global South, Folk Theories of Algorithms, User Perceptions","When algorithmic harms emerge, a reasonable response is to stop using the algorithm to resolve concerns related to fairness, accountability, transparency, and ethics (FATE). However, as we illustrate in this paper, just because an algorithm is removed does not imply that FATE-related issues cease to exist. We introduce the notion of the “algorithmic imprint” to illustrate how merely removing an algorithm does not necessarily undo or mitigate its consequences. We illustrate this concept and its implications through the 2020 events surrounding the algorithmic grading of the General Certificate of Education (GCE) Advanced (A) Level exams, an internationally recognized UK-based high school diploma exam administered in over 160 countries. While the algorithmic standardization was ultimately removed due to global protests, we show how the removal failed to undo the algorithmic imprint on the sociotechnical infrastructures that shape students’, teachers’, and parents’ lives. These events provide a rare chance to analyze the state of the world both with and without algorithmic mediation. We situate our case study in Bangladesh to illustrate how algorithms made in the Global North disproportionately impact stakeholders in the Global South. Chronicling more than a year-long community engagement consisting of 47 interviews, we present the first coherent timeline of “what” happened in Bangladesh, contextualizing “why” and “how” they happened through the lenses of the algorithmic imprint and situated algorithmic fairness. Analyzing these events, we highlight how the contours of the algorithmic imprints can be inferred at the infrastructural, societal, and individual levels. We share conceptual and practical implications around how imprint-awareness can (a) broaden the boundaries of how we think about algorithmic impact, (b) inform how we design algorithms, and (c) guide us in AI governance. The imprint-aware design mindset can make the algorithmic development process more human-centered and sociotechnically-informed.","ACMDL - FAccT","Ecosystem",521
"181","FAccT","Meta-Commentary","Abebe, Hardt, Jin, Miller, Schmidt, Wexler","Adversarial Scrutiny of Evidentiary Statistical Software","https://dl.acm.org/doi/10.1145/3531146.3533228","FAccT",2022,"evidentiary software, statistical software, adversarial scrutiny, blackbox software, robust machine learning","The U.S. criminal legal system increasingly relies on software output to convict and incarcerate people. In a large number of cases each year, the government makes these consequential decisions based on evidence from statistical software—such as probabilistic genotyping, environmental audio detection and toolmark analysis tools—that the defense counsel cannot fully cross-examine or scrutinize. This undermines the commitments of the adversarial criminal legal system, which relies on the defense’s ability to probe and test the prosecution’s case to safeguard individual rights. Responding to this need to adversarially scrutinize output from such software, we propose robust adversarial testing as a framework to examine the validity of evidentiary statistical software. We define and operationalize this notion of robust adversarial testing for defense use by drawing on a large body of recent work in robust machine learning and algorithmic fairness. We demonstrate how this framework both standardizes the process for scrutinizing such tools and empowers defense lawyers to examine their validity for instances most relevant to the case at hand. We further discuss existing structural and institutional challenges within the U.S. criminal legal system which may create barriers for implementing this framework and close with a discussion on policy changes that could help address these concerns. ","ACMDL - FAccT","Meta-Commentary",522
"182","FAccT","Meta-Commentary","Costanza-Chock, Raji, Buolamwini","Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem","https://doi.org/10.1145/3531146.3533213","FAccT",2022,"AI audit, algorithm audit, audit, ethical AI, AI bias, AI harm, AI policy, algorithmic accountability","Algorithmic audits (or `AI audits') are an increasingly popular mechanism for algorithmic accountability; however, they remain poorly defined. Without a clear understanding of audit practices, let alone widely used standards or regulatory guidance, claims that an AI product or system has been audited, whether by first-, second-, or third-party auditors, are difficult to verify and may potentially exacerbate, rather than mitigate, bias and harm. To address this knowledge gap, we provide the first comprehensive field scan of the AI audit ecosystem. We share a catalog of individuals (N=438) and organizations (N=189) who engage in algorithmic audits or whose work is directly relevant to algorithmic audits; conduct an anonymous survey of the group (N=152); and interview industry leaders (N=10). We identify emerging best practices as well as methods and tools that are becoming commonplace, and enumerate common barriers to leveraging algorithmic audits as effective accountability mechanisms. We outline policy recommendations to improve the quality and impact of these audits, and highlight proposals with wide support from algorithmic auditors as well as areas of debate. Our recommendations have implications for lawmakers, regulators, internal company policymakers, and standards-setting bodies, as well as for auditors. They are: 1) require the owners and operators of AI systems to engage in independent algorithmic audits against clearly defined standards; 2) notify individuals when they are subject to algorithmic decision-making systems; 3) mandate disclosure of key components of audit findings for peer review; 4) consider real-world harm in the audit process, including through standardized harm incident reporting and response mechanisms; 5) directly involve the stakeholders most likely to be harmed by AI systems in the algorithmic audit process; and 6) formalize evaluation and, potentially, accreditation of algorithmic auditors.","ACMDL - FAccT","Meta-Commentary",523
"183","FAccT","Meta-Commentary","Norval, Cornelius, Cobbe, Singh","Disclosure by Design: Designing information disclosures to support meaningful transparency and accountability","https://dl.acm.org/doi/10.1145/3531146.3533133","FAccT",2022,"transparency, accountability, GDPR, document engineering, interfaces, data rights, usability","There is a strong push for organisations to become more transparent and accountable for their undertakings. Towards this, various transparency regimes oblige organisations to disclose certain information to relevant stakeholders (individuals, regulators, etc). This information intends to empower and support the monitoring, oversight, scrutiny and challenge of organisational practices. Importantly, however, these disclosures are of limited benefit if they are not meaningful for their recipients. Yet, in practice, the disclosures of tech/data-driven organisations are often highly technical, fragmented, and therefore of limited utility to all but experts. This undermines a disclosure’s effectiveness, works to disempower, and ultimately hinders broader transparency aims.","ACMDL - FAccT","Method",531
"184","ACMDL - Fairness","Ecosystem","Woodruff A,Fox SE,Rousso-Schindler S,Warshaw J","A Qualitative Exploration of Perceptions of Algorithmic Fairness","https://doi.org/10.1145/3173574.3174230;http://dx.doi.org/10.1145/3173574.3174230",NA,2018,"algorithmic fairness, algorithmic discrimination","Algorithmic systems increasingly shape information people are exposed to as well as influence decisions about employment, finances, and other opportunities. In some cases, algorithmic systems may be more or less favorable to certain groups or individuals, sparking substantial discussion of algorithmic fairness in public policy circles, academia, and the press. We broaden this discussion by exploring how members of potentially affected communities feel about algorithmic fairness. We conducted workshops and interviews with 44 participants from several populations traditionally marginalized by categories of race or class in the United States. While the concept of algorithmic fairness was largely unfamiliar, learning about algorithmic (un)fairness elicited negative feelings that connect to current national discussions about racial injustice and economic inequality. In addition to their concerns about potential harms to themselves and society, participants also indicated that algorithmic fairness (or lack thereof) could substantially affect their trust in a company or product.","ACMDL - Fairness","Ecosystem",110
"185","ACMDL - Fairness","Case Study","Kuhlman C,VanValkenburg M,Rundensteiner E","FARE: Diagnostics for Fair Ranking Using Pairwise Error Metrics","https://doi.org/10.1145/3308558.3313443;http://dx.doi.org/10.1145/3308558.3313443","WWW '19",2019,"Fair Ranking, Fairness Auditing, Pairwise Fairness, Fairness","Ranking, used extensively online and as a critical tool for decision making across many domains, may embed unfair bias. Tools to measure and correct for discriminatory bias are required to ensure that ranking models do not perpetuate unfair practices. Recently, a number of error-based criteria have been proposed to assess fairness with regard to the treatment of protected groups (as determined by sensitive data attributes, e.g., race, gender, or age). However this has largely been limited to classification tasks, and error metrics used in these approaches are not applicable for ranking. Therefore, in this work we propose to broaden the scope of fairness assessment to include error-based fairness criteria for rankings. Our approach supports three criteria: Rank Equality, Rank Calibration, and Rank Parity, which cover a broad spectrum of fairness considerations from proportional group representation to error rate similarity. The underlying error metrics are formulated to be rank-appropriate, using pairwise discordance to measure prediction error in a model-agnostic fashion. Based on this foundation, we then design a fair auditing mechanism which captures group treatment throughout the entire ranking, generating in-depth yet nuanced diagnostics. We demonstrate the efficacy of our error metrics using real-world scenarios, exposing trade-offs among fairness criteria and providing guidance in the selection of fair-ranking algorithms.","ACMDL - Fairness","Case Study",103
"186","ACMDL - Fairness","Case Study","Michalsky F","Fairness Criteria for Face Recognition Applications","https://doi.org/10.1145/3306618.3314308;http://dx.doi.org/10.1145/3306618.3314308","AIES '19",2019,"neural networks, fairness, datasets, machine learning, face recognition","Nowadays, machine learning algorithms play an important role in our daily lives and it is important to ensure their fairness and transparency. A number of methodologies for evaluating machine learning fairness have been introduced in the literature. In this research we propose a systematic confidence evaluation approach to measure fairness discrepancies of our deep learning architecture for image recognition using UTKFace database.","ACMDL - Fairness","Case Study",106
"187","ACMDL - Fairness","Case Study","Alam MA","AI-Fairness Towards Activity Recognition of Older Adults","https://doi.org/10.1145/3448891.3448943;http://dx.doi.org/10.1145/3448891.3448943",NA,2020,"Activity Recognition AI-Fairness Ubiquitous Computing","Wireless wearable sensor networks (WSN)-based activity recognition has implicit impacts on context-aware application and connected health research for older adults. Although, many existing researches focus on different WSN integration, signal processing and intelligent detection to recognize different activities, none of the existing works address the Artificial Intelligence (AI) fairness in activity recognition of older adults domain. We argue that AI-fairness towards detecting activities of older adults is different than to fairness for other protected attributes such as age, gender or race. The primary reason behind this difference is the diversity of same activity among different older adults based on their functional abilities (walking using crutches, walker). However, the diversity also exists among the same older adults based on time and space as well. The above constraints limit the AI capabilities and causes unfair detection of daily activities that has potential impacts on healthcare interventions for older adults. In this paper, we investigate, first of its kind, AI-fairness of activity recognition for older adults using a single wearable WSN sensor in presence of diverse disabilities. In this regard, we (i) employ signal processing and Bi-directional LSTM model to recognize diverse multi-label activities of older adults using single WSN; (ii) identify the existence of biases in activity recognition considering the age and functional ability as protected attributes; (iii) mitigate the biases by applying different bias mitigation techniques in different stages (pre-, in- and post-processing) of machine learning model development; and finally (iv) we experimentally evaluate the proposed AI-fairness framework using older adults data collected from a retirement center.","ACMDL - Fairness","Case Study",92
"188","ACMDL - Fairness","Case Study","Almuzaini AA,Singh VK","Balancing Fairness and Accuracy in Sentiment Detection Using Multiple Black Box Models","https://doi.org/10.1145/3422841.3423536;http://dx.doi.org/10.1145/3422841.3423536",NA,2020,"fusion, fairness, sentiment analysis, black-box models","Sentiment detection is an important building block for multiple information retrieval tasks such as product recommendation, cyberbullying, fake news and misinformation detection. Unsurprisingly, multiple commercial APIs, each with different levels of accuracy and fairness, are now publicly available for sentiment detection. Users can easily incorporate these APIs in their applications. While combining inputs from multiple modalities or black-box models for increasing accuracy is commonly studied in multimedia computing literature, there has been little work on combining different modalities for increasingfairness of the resulting decision. In this work, we audit multiple commercial sentiment detection APIs for the gender bias in two-actor news headlines settings and report on the level of bias observed. Next, we propose a ""Flexible Fair Regression"" approach, which ensures satisfactory accuracy and fairness by jointly learning from multiple black-box models. The results pave way for fair yet accurate sentiment detectors for multiple applications.","ACMDL - Fairness","Case Study",93
"189","ACMDL - Fairness","Case Study","Fenu G,Medda G,Marras M,Meloni G","Improving Fairness in Speaker Recognition","https://doi.org/10.1145/3393822.3432325;http://dx.doi.org/10.1145/3393822.3432325","ESSE 2020",2020,"Fairness, Speaker Recognition, Discrimination, Speaker Verification, Bias, Deep Learning, ResNet, X-Vector","The human voice conveys unique characteristics of an individual, making voice biometrics a key technology for verifying identities in various industries. Despite the impressive progress of speaker recognition systems in terms of accuracy, a number of ethical and legal concerns has been raised, specifically relating to the fairness of such systems. In this paper, we aim to explore the disparity in performance achieved by state-of-the-art deep speaker recognition systems, when different groups of individuals characterized by a common sensitive attribute (e.g., gender) are considered. In order to mitigate the unfairness we uncovered by means of an exploratory study, we investigate whether balancing the representation of the different groups of individuals in the training set can lead to a more equal treatment of these demographic groups. Experiments on two state-of-the-art neural architectures and a large-scale public dataset show that models trained with demographically-balanced training sets exhibit a fairer behavior on different groups, while still being accurate. Our study is expected to provide a solid basis for instilling beyond-accuracy objectives (e.g., fairness) in speaker recognition.","ACMDL - Fairness","Case Study",99
"190","ACMDL - Fairness","Case Study","Hu L,Chen Y","Fair Classification and Social Welfare","https://doi.org/10.1145/3351095.3372857;http://dx.doi.org/10.1145/3351095.3372857",NA,2020,NA,"Now that machine learning algorithms lie at the center of many important resource allocation pipelines, computer scientists have been unwittingly cast as partial social planners. Given this state of affairs, important questions follow. How do leading notions of fairness as defined by computer scientists map onto longer-standing notions of social welfare? In this paper, we present a welfare-based analysis of fair classification regimes. Our main findings assess the welfare impact of fairness-constrained empirical risk minimization programs on the individuals and groups who are subject to their outputs. We fully characterize the ranges of Δ'e perturbations to a fairness parameter 'e in a fair Soft Margin SVM problem that yield better, worse, and neutral outcomes in utility for individuals and by extension, groups. Our method of analysis allows for fast and efficient computation of ""fairness-to-welfare"" solution paths, thereby allowing practitioners to easily assess whether and which fair learning procedures result in classification outcomes that make groups better-off. Our analyses show that applying stricter fairness criteria codified as parity constraints can worsen welfare outcomes for both groups. More generally, always preferring ""more fair"" classifiers does not abide by the Pareto Principle---a fundamental axiom of social choice theory and welfare economics. Recent work in machine learning has rallied around these notions of fairness as critical to ensuring that algorithmic systems do not have disparate negative impact on disadvantaged social groups. By showing that these constraints often fail to translate into improved outcomes for these groups, we cast doubt on their effectiveness as a means to ensure fairness and justice.","ACMDL - Fairness","Case Study",100
"191","ACMDL - Fairness","Case Study","Jiang J,Vosoughi S","Not Judging a User by Their Cover: Understanding Harm in Multi-Modal Processing within Social Media Research","https://doi.org/10.1145/3422841.3423534;http://dx.doi.org/10.1145/3422841.3423534",NA,2020,"user annotation, gender, audit, bias, demographics, toxicity, amazon mechanical turk, perspective api, age, race, fairness","Social media has shaken the foundations of our society, unlikely as it may seem. Many of the popular tools used to moderate harmful digital content, however, have received widespread criticism from both the academic community and the public sphere for middling performance and lack of accountability. Though social media research is thought to center primarily on natural language processing, we demonstrate the need for the community to understand multimedia processing and its unique ethical considerations. Specifically, we identify statistical differences in the performance of Amazon Turk (MTurk) annotators when different modalities of information are provided and discuss the patterns of harm that arise from crowd-sourced human demographic prediction. Finally, we discuss the consequences of those biases through auditing the performance of a toxicity detector called Perspective API on the language of Twitter users across a variety of demographic categories.","ACMDL - Fairness","Case Study",101
"192","ACMDL - Fairness","Case Study","Singh VK,Hofenbitzer C","Fairness across Network Positions in Cyberbullying Detection Algorithms","https://doi.org/10.1145/3341161.3342949;http://dx.doi.org/10.1145/3341161.3342949","ASONAM '19",2020,"network position, cyberbullying, algorithmic fairness","Cyberbullying, which often has a deeply negative impact on the victim, has grown as a serious issue in online social networks. Recently, researchers have created automated machine learning algorithms to detect Cyberbullying using social and textual features. However, the very algorithms that are intended to fight off one threat (cyberbullying) may inadvertently be falling prey to another important threat (bias of the automatic detection algorithms). This is exacerbated by the fact that while the current literature on algorithmic fairness has multiple empirical results, metrics, and algorithms for countering bias across immediately observable demographic characteristics (e.g. age, race, gender), there have been no efforts at empirically quantifying the variation in algorithmic performance based on the network role or position of individuals. We audit an existing cyberbullying algorithm using Twitter data for disparity in detection performance based on the network centrality of the potential victim and then demonstrate how this disparity can be countered using an Equalized Odds postprocessing technique. The results pave the way for more accurate and fair cyberbullying detection algorithms.","ACMDL - Fairness","Case Study",108
"193","ACMDL - Fairness","Data Audit","Yan S,Kao HT,Ferrara E","Fair Class Balancing: Enhancing Model Fairness without Observing Sensitive Attributes","https://doi.org/10.1145/3340531.3411980;http://dx.doi.org/10.1145/3340531.3411980","CIKM '20",2020,"class balancing, fairness, bias","Machine learning models are at the foundation of modern society. Accounts of unfair models penalizing subgroups of a population have been reported in domains including law enforcement, job screening, etc. Unfairness can spur from biases in the training data, as well as from class imbalance, i.e., when a sensitive group's data is not sufficiently represented. Under such settings, balancing techniques are commonly used to achieve better prediction performance, but their effects on model fairness are largely unknown. In this paper, we first illustrate the extent to which common balancing techniques exacerbate unfairness in real-world data. Then, we propose a new method, called fair class balancing, that allows to enhance model fairness without using any information about sensitive attributes. We show that our method can achieve accurate prediction performance while concurrently improving fairness.","ACMDL - Fairness","Data Audit",111
"194","ACMDL - Fairness","Case Study","Kim MP,Korolova A,Rothblum GN,Yona G","Preference-Informed Fairness","https://doi.org/10.1145/3351095.3373155;http://dx.doi.org/10.1145/3351095.3373155",NA,2020,"algorithmic fairness","In this work, we study notions of fairness in decision-making systems when individuals have diverse preferences over the possible outcomes of the decisions. Our starting point is the seminal work of Dwork et al. [ITCS 2012] which introduced a notion of individual fairness (IF): given a task-specific similarity metric, every pair of individuals who are similarly qualified according to the metric should receive similar outcomes. We show that when individuals have diverse preferences over outcomes, requiring IF may unintentionally lead to less-preferred outcomes for the very individuals that IF aims to protect (e.g. a protected minority group). A natural alternative to IF is the classic notion of fair division, envy-freeness (EF): no individual should prefer another individual's outcome over their own. Although EF allows for solutions where all individuals receive a highly-preferred outcome, EF may also be overly-restrictive for the decision-maker. For instance, if many individuals agree on the best outcome, then if any individual receives this outcome, they all must receive it, regardless of each individual's underlying qualifications for the outcome.We introduce and study a new notion of preference-informed individual fairness (PIIF) that is a relaxation of both individual fairness and envy-freeness. At a high-level, PIIF requires that outcomes satisfy IF-style constraints, but allows for deviations provided they are in line with individuals' preferences. We show that PIIF can permit outcomes that are more favorable to individuals than any IF solution, while providing considerably more flexibility to the decision-maker than EF. In addition, we show how to efficiently optimize any convex objective over the outcomes subject to PIIF for a rich class of individual preferences. Finally, we demonstrate the broad applicability of the PIIF framework by extending our definitions and algorithms to the multiple-task targeted advertising setting introduced by Dwork and Ilvento [ITCS 2019].","ACMDL - Fairness","Case Study",417
"195","ACMDL - Fairness","Meta-Commentary","Dai J,Fazelpour S,Lipton Z","Fair Machine Learning Under Partial Compliance","https://doi.org/10.1145/3461702.3462521;http://dx.doi.org/10.1145/3461702.3462521",NA,2021,"simulations, fairness, regulation, segregation, fair machine learning, hiring, distributive justice","Typically, fair machine learning research focuses on a single decision maker and assumes that the underlying population is stationary. However, many of the critical domains motivating this work are characterized by competitive marketplaces with many decision makers. Realistically, we might expect only a subset of them to adopt any non-compulsory fairness-conscious policy, a situation that political philosophers call partial compliance. This possibility raises important questions: how does partial compliance and the consequent strategic behavior of decision subjects affect the allocation outcomes? If k% of employers were to voluntarily adopt a fairness-promoting intervention, should we expect k% progress (in aggregate) towards the benefits of universal adoption, or will the dynamics of partial compliance wash out the hoped-for benefits? How might adopting a global (versus local) perspective impact the conclusions of an auditor? In this paper, we propose a simple model of an employment market, leveraging simulation as a tool to explore the impact of both interaction effects and incentive effects on outcomes and auditing metrics. Our key findings are that at equilibrium: (1) partial compliance by k% of employers can result in far less than proportional (k%) progress towards the full compliance outcomes; (2) the gap is more severe when fair employers match global (vs local) statistics; (3) choices of local vs global statistics can paint dramatically different pictures of the performance vis-a-vis fairness desiderata of compliant versus non-compliant employers; (4) partial compliance based on local parity measures can induce extreme segregation. Finally, we discuss implications for auditors and insights concerning the design of regulatory frameworks.","ACMDL - Fairness","Method",96
"196","ACMDL - Fairness","Case Study","Kilby AE","Algorithmic Fairness in Predicting Opioid Use Disorder Using Machine Learning","https://doi.org/10.1145/3442188.3445891;http://dx.doi.org/10.1145/3442188.3445891","FAccT '21",2021,"ethics, algorithm development, social and organizational processes, auditing, evaluations, causality, algorithmic impacts on social phenomena, disability studies, fairness, critical data/algorithm studies","There has been recent interest by payers, health care systems, and researchers in the development of machine learning and artificial intelligence models that predict an individual's probability of developing opioid use disorder. The scores generated by these algorithms can be used by physicians to tailor the prescribing of opioids for the treatment of pain, reducing or foregoing prescribing to individuals deemed to be at high risk, or increasing prescribing for patients deemed to be at low risk. This paper constructs a machine learning algorithm to predict opioid use disorder risk using commercially available claims data similar to those utilized in the development of proprietary opioid use disorder prediction algorithms. We study risk scores generated by the machine learning model in a setting with quasi-experimental variation in the likelihood that doctors prescribe opioids, generated by changes in the legal structure for monitoring physician prescribing. We find that machine-predicted risk scores do not appear to correlate at all with the individual-specific heterogeneous treatment effect of receiving opioids. The paper identifies a new source of algorithmic unfairness in machine learning applications for health care and precision medicine, arising from the researcher's choice of objective function. While precision medicine should guide physician treatment decisions based on the heterogeneous causal impact of a course of treatment for an individual, allocating treatments to individuals receiving the most benefit and recommending caution for those most likely to experience harmful side effects, ML models in health care are often trained on proxies like individual baseline risk, and are not necessarily informative in deciding who would most benefit, or be harmed, by a course of treatment.","ACMDL - Fairness","Case Study",102
"197","ACMDL - Fairness","Data Audit","Y","Fairness Through Robustness: Investigating Robustness Disparity in Deep Learning","https://doi.org/10.1145/3442188.3445910;http://dx.doi.org/10.1145/3442188.3445910","FAccT '21",2021,NA,"Deep neural networks (DNNs) are increasingly used in real-world applications (e.g. facial recognition). This has resulted in concerns about the fairness of decisions made by these models. Various notions and measures of fairness have been proposed to ensure that a decision-making system does not disproportionately harm (or benefit) particular subgroups of the population. In this paper, we argue that traditional notions of fairness that are only based on models' outputs are not sufficient when the model is vulnerable to adversarial attacks. We argue that in some cases, it may be easier for an attacker to target a particular subgroup, resulting in a form of robustness bias. We show that measuring robustness bias is a challenging task for DNNs and propose two methods to measure this form of bias. We then conduct an empirical study on state-of-the-art neural networks on commonly used real-world datasets such as CIFAR-10, CIFAR-100, Adience, and UTKFace and show that in almost all cases there are subgroups (in some cases based on sensitive attributes like race, gender, etc) which are less robust and are thus at a disadvantage. We argue that this kind of bias arises due to both the data distribution and the highly complex nature of the learned decision boundary in the case of DNNs, thus making mitigation of such biases a non-trivial task. Our results show that robustness bias is an important criterion to consider while auditing real-world systems that rely on DNNs for decision making. Code to reproduce all our results can be found here: https://github.com/nvedant07/Fairness-Through-Robustness","ACMDL - Fairness","Data Audit",107
"198","ACMDL - Fairness","Case Study","Balagopalan A,Zhang H,Hamidieh K,Hartvigsen T,Rudzicz F,Ghassemi M","The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations","https://doi.org/10.1145/3531146.3533179;http://dx.doi.org/10.1145/3531146.3533179",NA,2022,"explainability, fairness, machine learning","Machine learning models in safety-critical settings like healthcare are often “blackboxes”: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.","ACMDL - Fairness","Case Study",94
"199","ACMDL - Fairness","Meta-Commentary","Cachel K,Rundensteiner E","FINS Auditing Framework: Group Fairness for Subset Selections","https://doi.org/10.1145/3514094.3534160;http://dx.doi.org/10.1145/3514094.3534160",NA,2022,"subset selection, machine learning fairness, algorithmic fairness","Subset selection is an integral component of AI systems that is increasingly affecting people's livelihoods in applications ranging from hiring, healthcare, education, to financial decisions. Subset selections powered by AI-based methods include top-k analytics, data summarization, clustering, and multi-winner voting. While group fairness auditing tools have been proposed for classification systems, these state-of-the-art tools are not directly applicable to measuring and conceptualizing fairness in selected subsets. In this work, we introduce the first comprehensive auditing framework, FINS, to support stakeholders in interpretably quantifying group fairness across a diverse range of subset-specific fairness concerns. FINS offers a family of novel measures that provide a flexible means to audit group fairness for fairness goals ranging from item-based, score-based, and a combination thereof. FINS provides one unified easy-to-understand interpretation across these different fairness problems. Further, we develop guidelines through the FINS Fair Subset Chart, that supports auditors in determining which measures are relevant to their problem context and fairness objectives. We provide a comprehensive mapping between each fairness measure and the belief system (i.e., worldview) that is encoded within its measurement of fairness. Lastly, we demonstrate the interpretability and efficacy of FINS in supporting the identification of real bias with case studies using AirBnB listings and voter records.","ACMDL - Fairness","Method",95
"200","ACMDL - Fairness","Meta-Commentary","Dandl S,Pfisterer F,Bischl B","Multi-Objective Counterfactual Fairness","https://doi.org/10.1145/3520304.3528779;http://dx.doi.org/10.1145/3520304.3528779","GECCO '22",2022,"fairness, machine learning, multi-objective, counterfactuals","When machine learning is used to automate judgments, e.g. in areas like lending or crime prediction, incorrect decisions can lead to adverse effects for affected individuals. This occurs, e.g., if the data used to train these models is based on prior decisions that are unfairly skewed against specific subpopulations. If models should automate decision-making, they must account for these biases to prevent perpetuating or creating discriminatory practices. Counter-factual fairness audits models with respect to a notion of fairness that asks for equal outcomes between a decision made in the real world and a counterfactual world where the individual subject to a decision comes from a different protected demographic group. In this work, we propose a method to conduct such audits without access to the underlying causal structure of the data generating process by framing it as a multi-objective optimization task that can be efficiently solved using a genetic algorithm.","ACMDL - Fairness","Method",97
"201","ACMDL - Fairness","Case Study","Dash A,Chakraborty A,Ghosh S,Mukherjee A,Gummadi KP","Alexa, in You, I Trust! Fairness and Interpretability Issues in E-Commerce Search through Smart Speakers","https://doi.org/10.1145/3485447.3512265;http://dx.doi.org/10.1145/3485447.3512265",NA,2022,"explanation, fairness, interpretabity, search, e-commerce","In traditional (desktop) e-commerce search, a customer issues a specific query and the system returns a ranked list of products in order of relevance to the query. An increasingly popular alternative in e-commerce search is to issue a voice-query to a smart speaker (e.g., Amazon Echo) powered by a voice assistant (VA, e.g., Alexa). In this situation, the VA usually spells out the details of only one product, an explanation citing the reason for its selection, and a default action of adding the product to the customer’s cart. This reduced autonomy of the customer in the choice of a product during voice-search makes it necessary for a VA to be far more responsible and trustworthy in its explanation and default action. In this paper, we ask whether the explanation presented for a product selection by the Alexa VA installed on an Amazon Echo device is consistent with human understanding as well as with the observations on other traditional mediums (e.g., desktop e-commerce search). Through a user survey, we find that in 81% cases the interpretation of ‘a top result’ by the users is different from that of Alexa. While investigating for the fairness of the default action, we observe that over a set of as many as 1000 queries, in ≈ 68% cases, there exist one or more products which are more relevant (as per Amazon’s own desktop search results) than the product chosen by Alexa. Finally, we conducted a survey over 30 queries for which the Alexa-selected product was different from the top desktop search result, and observed that in ≈ 73% cases, the participants preferred the top desktop search result as opposed to the product chosen by Alexa. Our results raise several concerns and necessitates more discussions around the related fairness and interpretability issues of VAs for e-commerce search.","ACMDL - Fairness","Case Study",98
"202","ACMDL - Fairness","Data Audit","Li N,Goel N,Ash E","Data-Centric Factors in Algorithmic Fairness","https://doi.org/10.1145/3514094.3534147;http://dx.doi.org/10.1145/3514094.3534147",NA,2022,"recidivism prediction, machine learning, datasets, algorithmic fairness","Notwithstanding the widely held view that data generation and data curation processes are prominent sources of bias in machine learning algorithms, there is little empirical research seeking to document and understand the specific data dimensions affecting algorithmic unfairness. Contra the previous work, which has focused on modeling using simple, small-scale benchmark datasets, we hold the model constant and methodically intervene on relevant dimensions of a much larger, more diverse dataset. For this purpose, we introduce a new dataset on recidivism in 1.5 million criminal cases from courts in the U.S. state of Wisconsin, 2000-2018. From this main dataset, we generate multiple auxiliary datasets to simulate different kinds of biases in the data. Focusing on algorithmic bias toward different race/ethnicity groups, we assess the relevance of training data size, base rate difference between groups, representation of groups in the training data, temporal aspects of data curation, including race/ethnicity or neighborhood characteristics as features, and training separate classifiers by race/ethnicity or crime type. We find that these factors often do influence fairness metrics holding the classifier specification constant, without having a corresponding effect on accuracy metrics. The methodology and the results in the paper provide a useful reference point for a data-centric approach to studying algorithmic fairness in recidivism prediction and beyond.","ACMDL - Fairness","Data Audit",104
"203","ACMDL - Fairness","Case Study","Lin J,Chen C,Chmielewski M,Zaman S,Fain B","Auditing for Gerrymandering by Identifying Disenfranchised Individuals","https://doi.org/10.1145/3531146.3533174;http://dx.doi.org/10.1145/3531146.3533174",NA,2022,"simulated annealing, gerrymandering, fairness, audit","Gerrymandering is the practice of drawing congressional districts to advantage or disadvantage particular electoral outcomes or population groups. We study the problem of computationally auditing a districting for evidence of gerrymandering. Our approach is novel in its emphasis on identifying individual voters disenfranchised by packing and cracking in local fine-grained geographic regions. We define a local score based on comparison with a representative sample of alternative districtings and use simulated annealing to algorithmically generate a witness districting to show that the score can be substantially reduced by simple local alterations. Unlike commonly studied metrics for gerrymandering such as proportionality and compactness, our framework is inspired by the legal context for voting rights in the United States. We demonstrate the use of our framework to analyze the congressional districting of the state of North Carolina in 2016. We identify a substantial number of geographically localized disenfranchised individuals, mostly Democrats in the central and north-eastern parts of the state. Our simulated annealing algorithm is able to generate a witness districting with a roughly 50% reduction in the number of disenfranchised individuals, suggesting that the 2016 districting was not predetermined by North Carolina’s spatial structure.","ACMDL - Fairness","Case Study",105
"204","ACMDL - Fairness","Case Study","So W,Lohia P,Pimplikar R,Hosoi AE,D'Ignazio C","Beyond Fairness: Reparative Algorithms to Address Historical Injustices of Housing Discrimination in the US","https://doi.org/10.1145/3531146.3533160;http://dx.doi.org/10.1145/3531146.3533160",NA,2022,"reparations, mortgage lending, racial wealth gap, housing, fairness","Fairness in Machine Learning (ML) has mostly focused on interrogating the fairness of a particular decision point with assumptions made that the people represented in the data have been fairly treated throughout history. However, fairness cannot be ultimately achieved if such assumptions are not valid. This is the case for mortgage lending discrimination in the US, which should be critically understood as the result of historically accumulated injustices that were enacted through public policies and private practices including redlining, racial covenants, exclusionary zoning, and predatory inclusion, among others. With the erroneous assumptions of historical fairness in ML, Black borrowers with low income and low wealth are considered as a given condition in a lending algorithm, thus rejecting loans to them would be considered a “fair” decision even though Black borrowers were historically excluded from homeownership and wealth creation. To emphasize such issues, we introduce case studies using contemporary mortgage lending data as well as historical census data in the US. First, we show that historical housing discrimination has differentiated each racial group’s baseline wealth which is a critical input for algorithmically determining mortgage loans. The second case study estimates the cost of housing reparations in the algorithmic lending context to redress historical harms because of such discriminatory housing policies. Through these case studies, we envision what reparative algorithms would look like in the context of housing discrimination in the US. This work connects to emerging scholarship on how algorithmic systems can contribute to redressing past harms through engaging with reparations policies and programs.","ACMDL - Fairness","Case Study",109
"205","Other non-ACM","Case Study","Reuben Binns, Max Van Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao, Nigel Shadbolt","It's Reducing a Human Being to a Percentage' Perceptions of Justice in Algorithmic Decisions","https://dl.acm.org/doi/10.1145/3173574.3173951","CHI",2018,"H.5.m. Information Interfaces and Presentation (e.g. HCI): Miscellaneous; K.4.1 Computers and Society: Public Policy Issues","Data-driven decision-making consequential to individuals raises important questions of accountability and justice. Indeed, European law provides individuals limited rights to ‘meaningful information about the logic’ behind significant, autonomous decisions such as loan approvals, insurance quotes, and CV filtering. We undertake three experimental studies examining people’s perceptions of justice in algorithmic decision-making under different scenarios and explanation styles. Dimensions of justice previously observed in response to human decision-making appear similarly engaged in response to algorithmic decisions. Qualitative analysis identified several concerns and heuristics involved in justice perceptions including arbitrariness, generalisation, and (in)dignity. Quantitative analysis indicates that explanation styles primarily matter to justice perceptions only when subjects are exposed to multiple different styles—under repeated exposure of one style, scenario effects obscure any explanation effects. Our results suggests there may be no ‘best’ approach to explaining algorithmic decisions, and that reflection on their automated nature both implicates and mitigates justice dimensions.","OAT - Algorithm/Model Audit Case Studies","Case Study",200
"206","Other non-ACM","Case Study","Le Chen et al/","Investigating the Impact of Gender on Rank in Resume Search Engines","https://dl.acm.org/doi/10.1145/3173574.3174225","CHI",2018,"H.3.5 Online Information Services: Web-based services; J.4 Social and Behavioral Sciences: Sociology; K.4.2 Social Issues: Employment, information retrieval; algorithm auditing; discrimination","In this work we investigate gender-based inequalities in the context of resume search engines, which are tools that allow recruiters to proactively search for candidates based on keywords and filters. If these ranking algorithms take demographic features into account (directly or indirectly), they may produce rankings that disadvantage some candidates. We collect search results from Indeed, Monster, and CareerBuilder based on 35 job titles in 20 U. S. cities, resulting in data on 855K job candidates. Using statistical tests, we examine whether these search engines produce rankings that exhibit two types of indirect discrimination: individual and group unfairness. Furthermore, we use controlled experiments to show that these websites do not use inferred gender of candidates as explicit features in their ranking algorithms.","OAT - Algorithm/Model Audit Case Studies","Case Study",203
"207","Other non-ACM","Case Study","Kroll, Josh","The Fallacy of Inscrutability ","https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.2018.0084",NA,2018,"machine learning, artificial intelligence, governance, accountability","Contrary to the criticism that mysterious, unaccountable black-box software systems threaten to make the logic of critical decisions inscrutable, we argue that algorithms are fundamentally understandable pieces of technology. Software systems are designed to interact with the world in a controlled way and built or operated for a specific purpose, subject to choices and assumptions. Traditional power structures can and do turn systems into opaque black boxes, but technologies can always be understood at a higher level, intensionally in terms of their designs and operational goals and extensionally in terms of their inputs, outputs and outcomes. The mechanisms of a system’s operation can always be examined and explained, but a focus on machinery obscures the key issue of power dynamics. While structural inscrutability frustrates users and oversight entities, system creators and operators always determine that the technologiest hey deploy are fit for certain uses, making no system wholly inscrutable. We investigate the contours of inscrutability and opacity, the way theyarise from power dynamics surrounding software systems, and the value of proposed remedies from disparate disciplines, especially computer ethics and privacy by design. We conclude that policy should not accede to the idea that some systems are of necessity inscrutable. Effective governance of algorithms comes from demanding rigorous science and engineering in system design, operation and evaluation to make systems verifiably trustworthy. Rather than seeking explanations for each behaviour of a computer system, policies should formalize and make known the assumptions, choices, and adequacy determinations associated with a system. This article is part of the theme issue ‘Governing artificial intelligence: ethical, legal, and technical opportunities and challenges’.","OAT - Algorithm/Model Audit Case Studies","Case Study",205
"208","Other non-ACM","Case Study","Randy Goebel, Ajay Chander, Katharina Holzinger, Freddy Lecue, Zeynep Akata, Simone Stumpf, Peter Kieseberg, Andreas Holzinger","Explainable AI: The New 42?","https://link.springer.com/chapter/10.1007/978-3-319-99740-7_21",NA,2018,"Artificial intelligence · Machine learning · Explainability Explainable AI","Explainable AI is not a new field. Since at least the early exploitation of C.S. Pierce’s abductive reasoning in expert systems of the 1980s, there were reasoning architectures to support an explanation function for complex AI systems, including applications in medical diagnosis, complex multi-component design, and reasoning about the real world. So explainability is at least as old as early AI, and a natural consequence of the design of AI systems. While early expert systems consisted of handcrafted knowledge bases that enabled reasoning over narrowly well-defined domains (e.g., INTERNIST, MYCIN), such systems had no learning capabilities and had only primitive uncertainty handling. But the evolution of formal reasoning architectures to incorporate principled probabilistic reasoning helped address the capture and use of uncertain knowledge. There has been recent and relatively rapid success of AI/machine learning solutions arises from neural network architectures. A new generation of neural methods now scale to exploit the practical applicability of statistical and algebraic learning approaches in arbitrarily high dimensional spaces. But despite their huge successes, largely in problems which can be cast as classification problems, their effectiveness is still limited by their un-debuggability, and their inability to “explain” their decisions in a human understandable and reconstructable way. So while AlphaGo or DeepStack can crush the best humans at Go or Poker, neither program has any internal model of its task; its representations defy interpretation by humans, there is no mechanism to explain their actions and behaviour, and furthermore, there is no obvious instructional value ... the high performance systems can not help humans improve. Even when we understand the underlying mathematical scaffolding of current machine learning architectures, it is often impossible to get insight into the internal working of the models; we need explicit modeling and reasoning tools to explain how and why a result was achieved. We also know that a significant challenge for future AI is contextual adaptation, i.e., systems that incrementally help to construct explanatory models for solving real-world problems. Here it would be beneficial not to exclude human expertise, but to augment human intelligence with artificial intelligence.","OAT - Algorithm/Model Audit Case Studies","Case Study",207
"209","Other non-ACM","Case Study","Samuel Yeom, Irene Giacomelli, Matt Fredrikson, Somesh Jha","Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting","https://arxiv.org/abs/1709.01604",NA,2018,NA,"Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role.
This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.","OAT - Algorithm/Model Audit Case Studies","Case Study",213
"210","Other non-ACM","Case Study","Leike et al.","Scalable agent alignment via reward modeling: a research direction","https://arxiv.org/abs/1811.07871",NA,2018,NA,"     One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents. ","OAT - Algorithm/Model Audit Case Studies","Case Study",215
"211","Other non-ACM","Case Study","Bickmore et al.","Patient and Consumer Safety Risks When Using Conversational Assistants for Medical Information: An Observational Study of Siri, Alexa, and Google Assistant","https://pubmed.ncbi.nlm.nih.gov/30181110/",NA,2018,"conversational assistant; conversational interface; dialogue system; medical error; patient safety","Background: Conversational assistants, such as Siri, Alexa, and Google Assistant, are ubiquitous and are beginning to be used as portals for medical services. However, the potential safety issues of using conversational assistants for medical information by patients and consumers are not understood.

Objective: To determine the prevalence and nature of the harm that could result from patients or consumers using conversational assistants for medical information.

Methods: Participants were given medical problems to pose to Siri, Alexa, or Google Assistant, and asked to determine an action to take based on information from the system. Assignment of tasks and systems were randomized across participants, and participants queried the conversational assistants in their own words, making as many attempts as needed until they either reported an action to take or gave up. Participant-reported actions for each medical task were rated for patient harm using an Agency for Healthcare Research and Quality harm scale.

Results: Fifty-four subjects completed the study with a mean age of 42 years (SD 18). Twenty-nine (54%) were female, 31 (57%) Caucasian, and 26 (50%) were college educated. Only 8 (15%) reported using a conversational assistant regularly, while 22 (41%) had never used one, and 24 (44%) had tried one ""a few times."" Forty-four (82%) used computers regularly. Subjects were only able to complete 168 (43%) of their 394 tasks. Of these, 49 (29%) reported actions that could have resulted in some degree of patient harm, including 27 (16%) that could have resulted in death.

Conclusions: Reliance on conversational assistants for actionable medical information represents a safety risk for patients and consumers. Patients should be cautioned to not use these technologies for answers to medical questions they intend to act on without further consultation from a health care provider.

Keywords: conversational assistant; conversational interface; dialogue system; medical error; patient safety. ","OAT - Algorithm/Model Audit Case Studies","Case Study",218
"212","Other non-ACM","Case Study","Papernot et al.","Towards the Science of Security and Privacy in Machine Learning","https://arxiv.org/abs/1611.03814",NA,2018,NA,"Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive---new systems and models are being deployed in every domain imaginable, leading to rapid and widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize recent findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date. We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by formally exploring the opposing relationship between model accuracy and resilience to adversarial manipulation. Through these explorations, we show that there are (possibly unavoidable) tensions between model complexity, accuracy, and resilience that must be calibrated for the environments in which they will be used.","OAT - Algorithm/Model Audit Case Studies","Case Study",224
"213","Other non-ACM","Case Study","Filippas and Horton","The tragedy of your upstairs neighbors: When is the home-sharing externality internalized?","https://dx.doi.org/10.2139/ssrn.2443343",NA,2018,NA,"A common critique of home-sharing platforms is that they enable hosts to impose costs on their neighbors. We consider four potential public policy responses that differ in whether the decision right to host is allocated to: (1) individual tenants, (2) building owners, (3) cities, and (4) a social planner. We find that (2) and (4) are equivalent, with (3) leading to too little hosting, and (1) to too much hosting. The efficiency of (2) depends on building owners being indifferent between allowing and banning home-sharing in their buildings. To assess this “no policy arbitrage” prediction, we constructed a dataset of NYC rental apartments listings. Although we do not observe building home-sharing policies, there are several “policy” attributes captured in the data, such as whether buildings allow subletting. Consistent with our prediction, we find that policy choices have no detectable effect on rental prices. Despite the attractiveness of the equilibrium of policy (2), tenants must “sort” across buildings, potentially at substantial cost. We explore this sorting with an agent-based model, and show how individual preferences and moving costs affect the equilibrium.","OAT - Algorithm/Model Audit Case Studies","Case Study",225
"214","Other non-ACM","Case Study","Molnar and Mangrum","The marginal congestion of a taxi in New York City","https://www.danielmangrum.com/docs/Boro_current.pdf",NA,2018,"Urban transportation. Congestion. Externalities. Regulation. Taxi. Rideshare. Ridehail. Transportation Network Companies","We exploit the partial deregulation of New York City taxi medallions to provide a causal
estimate of the impact of vehicle density on congestion. We employ taxi trip records to
measure historical street-level speed. We find the taxi deregulation policy caused a local 8-9%
decrease in speed. We estimate an empirical congestion elasticity curve from heterogeneity in
vehicle density, counted from aerial orthoimagery. Additionally, we document a substantial
traffic slowdown since 2013 using novel urban sensor data. Most of the slowdown in midtown
Manhattan is accounted for by new supply from ridehail applications. We also assess current
congestion pricing policies.","OAT - Algorithm/Model Audit Case Studies","Case Study",227
"215","Other non-ACM","Case Study","Brundage and Avin et al.","The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation","https://arxiv.org/pdf/1802.07228.pdf",NA,2018,NA,"Artificial intelligence and machine learning capabilities are growing
at an unprecedented rate. These technologies have many widely
beneficial applications, ranging from machine translation to medical
image analysis. Countless more such applications are being
developed and can be expected over the long term. Less attention
has historically been paid to the ways in which artificial intelligence
can be used maliciously. This report surveys the landscape of
potential security threats from malicious uses of artificial intelligence
technologies, and proposes ways to better forecast, prevent, and
mitigate these threats. We analyze, but do not conclusively resolve,
the question of what the long-term equilibrium between attackers and
defenders will be. We focus instead on what sorts of attacks we are
likely to see soon if adequate defenses are not developed.","OAT - Algorithm/Model Audit Case Studies","Case Study",232
"216","Other non-ACM","Case Study","Ryan Burns, Blake Hawkins, Anne Lauren Hoffmann, Andrew Iliadis, Jim Thatcher","Transdisciplinary approaches to critical data studies","https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/pra2.2018.14505501074","Proceedings of the Association for Information Science and Technology",2018,NA,"In this paper we describe the rationale behind a panel concerning the transdisciplinary approaches to critical data studies. We illuminate emerging modes of conducting critical data studies research and highlights the interdisciplinary opportunities within future research directions. We describe the format for the panel and the themes that will be covered by each of the author. By having this panel to occur it creates a space for interdisciplinary dialogues on a timely topic that touches various sub- fields of information science along with the broader social science disciplines in the academy.","OAT - Algorithm/Model Audit Case Studies","Case Study",234
"217","Other non-ACM","Case Study","Michael Veale, Reuben Binns, Lilian Edwards","Algorithms that Remember: Model Inversion Attacks and Data Protection Law","https://arxiv.org/abs/1807.04644",NA,2018,"model inversion, personal data, model trading, machine learning","Many individuals are concerned about the governance of machine learning systems and the prevention of algorithmic harms. The EU's recent General Data Protection Regulation (GDPR) has been seen as a core tool for achieving better governance of this area. While the GDPR does apply to the use of models in some limited situations, most of its provisions relate to the governance of personal data, while models have traditionally been seen as intellectual property. We present recent work from the information security literature around `model inversion' and `membership inference' attacks, which indicate that the process of turning training data into machine learned systems is not one-way, and demonstrate how this could lead some models to be legally classified as personal data. Taking this as a probing experiment, we explore the different rights and obligations this would trigger and their utility, and posit future directions for algorithmic governance and regulation.","OAT - Algorithm/Model Audit Case Studies","Case Study",252
"218","Other non-ACM","Case Study","Gebru et al","Datasheets for Datasets","https://arxiv.org/abs/1803.09010",NA,2018,NA,"The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.","OAT - Algorithm/Model Audit Case Studies","Case Study",257
"219","Other non-ACM","Case Study","Markham et al.","Ethics as Methods: Doing Ethics in the Era of Big Data Research","https://journals.sagepub.com/doi/pdf/10.1177/2056305118784502",NA,2018,"ethics, method, social media","This is an introduction to the special issue of “Ethics as Methods: Doing Ethics in the Era of Big Data Research.” Building on
a variety of theoretical paradigms (i.e., critical theory, [new] materialism, feminist ethics, theory of cultural techniques) and
frameworks (i.e., contextual integrity, deflationary perspective, ethics of care), the Special Issue contributes specific cases and
fine-grained conceptual distinctions to ongoing discussions about the ethics in data-driven research. In the second decade
of the 21st century, a grand narrative is emerging that posits knowledge derived from data analytics as true, because of the
objective qualities of data, their means of collection and analysis, and the sheer size of the data set. The by-product of this
grand narrative is that the qualitative aspects of behavior and experience that form the data are diminished, and the human is
removed from the process of analysis. This situates data science as a process of analysis performed by the tool, which obscures
human decisions in the process. The scholars involved in this Special Issue problematize the assumptions and trends in big
data research and point out the crisis in accountability that emerges from using such data to make societal interventions. Our
collaborators offer a range of answers to the question of how to configure ethics through a methodological framework in the
context of the prevalence of big data, neural networks, and automated, algorithmic governance of much of human socia(bi)lity","OAT - Algorithm/Model Audit Case Studies","Case Study",267
"220","Other non-ACM","Case Study","Virginia Eubanks","Automating inequality: How high-tech tools profile, police, and punish the poor","https://virginia-eubanks.com/automating-inequality/",NA,2018,NA,NA,"OAT - Algorithm/Model Audit Case Studies","Case Study",268
"221","Other non-ACM","Case Study","Noble, Safiya Umoja","Algorithms of Oppression: How Search Engines Reinforce Racism","https://nyupress.org/9781479837243/algorithms-of-oppression/",NA,2018,NA,NA,"OAT - Algorithm/Model Audit Case Studies","Case Study",269
"222","Other non-ACM","Case Study","Ana-Andreea Stoica et al.","Algorithmic Glass Ceiling in Social Networks: The effects of social recommendations on network diversity","https://dl.acm.org/doi/10.1145/3178876.3186140",NA,2018,"social recommender, fairness, random walks, homophily","As social recommendations such as friend suggestions and people to follow become increasingly popular and influential on the growth of social media, we find that prominent social recommendation algorithms can exacerbate the under-representation of certain demographic groups at the top of the social hierarchy. To study this imbalance in online equal opportunities, we leverage new Instagram data and offer for the first time an analysis that studies the effect of gender, homophily and growth dynamics under social recommendations. Our mathematical analysis demonstrates the existence of an algorithmic glass ceiling that exhibits all the properties of the metaphorical social barrier that hinders groups like women or people of color from attaining equal representation. What raises concern is that our proof shows that under fixed minority and homophily parameters the algorithmic effect is systematically larger than the glass ceiling generated by the spontaneous growth of social networks. We discuss ways to address this concern in future design.","OAT - Algorithm/Model Audit Case Studies","Case Study",310
"223","Other non-ACM","Case Study","Ronald E. Robertson, Shan Jiang et al","Auditing Partisan Audience Bias within Google Search","https://www.semanticscholar.org/paper/Auditing-Partisan-Audience-Bias-within-Google-Robertson-Jiang/848ed2877d2833e1816fc2f1eedb25b41cbc8097","CSCW",2018,"Search engine rankings; quantifying partisan bias; algorithm auditing; political personalization; filter bubble","There is a growing consensus that online platforms have a systematic influence on the democratic process. However, research beyond social media is limited. In this paper, we report the results of a mixed-methods algorithm audit of partisan audience bias and personalization within Google Search. Following Donald Trump's inauguration, we recruited 187 participants to complete a survey and install a browser extension that enabled us to collect Search Engine Results Pages (SERPs) from their computers. To quantify partisan audience bias, we developed a domain-level score by leveraging the sharing propensities of registered voters on a large Twitter panel. We found little evidence for the ""filter bubble'' hypothesis. Instead, we found that results positioned toward the bottom of Google SERPs were more left-leaning than results positioned toward the top, and that the direction and magnitude of overall lean varied by search query, component type (e.g. ""answer boxes""), and other factors. Utilizing rank-weighted metrics that we adapted from prior work, we also found that Google's rankings shifted the average lean of SERPs to the right of their unweighted average.","OAT - Algorithm/Model Audit Case Studies","Case Study",349
"224","Other non-ACM","Case Study","Ali, Sapiezynski et al","Discrimination through optimization: How Facebook’s ad delivery can lead to skewed outcomes","https://arxiv.org/abs/1904.02095",NA,2019,NA,"The enormous financial success of online advertising platforms is partially due to the precise targeting features they offer. Although researchers and journalists have found many ways that advertisers can target---or exclude---particular groups of users seeing their ads, comparatively little attention has been paid to the implications of the platform's ad delivery process, comprised of the platform's choices about which users see which ads.
It has been hypothesized that this process can ""skew"" ad delivery in ways that the advertisers do not intend, making some users less likely than others to see particular ads based on their demographic characteristics. In this paper, we demonstrate that such skewed delivery occurs on Facebook, due to market and financial optimization effects as well as the platform's own predictions about the ""relevance"" of ads to different groups of users. We find that both the advertiser's budget and the content of the ad each significantly contribute to the skew of Facebook's ad delivery. Critically, we observe significant skew in delivery along gender and racial lines for ""real"" ads for employment and housing opportunities despite neutral targeting parameters.
Our results demonstrate previously unknown mechanisms that can lead to potentially discriminatory ad delivery, even when advertisers set their targeting parameters to be highly inclusive. This underscores the need for policymakers and platforms to carefully consider the role of the ad delivery optimization run by ad platforms themselves---and not just the targeting choices of advertisers---in preventing discrimination in digital advertising.","OAT - Algorithm/Model Audit Case Studies","Case Study",188
"225","Other non-ACM","Case Study","Ali, Sapiezynski et al","Ad Delivery Algorithms: The Hidden Arbiters of Political Messaging","https://dl.acm.org/doi/10.1145/3437963.3441801",NA,2019,NA,"Political campaigns are increasingly turning to targeted advertising platforms to inform and mobilize potential voters. The appeal of these platforms stems from their promise to empower advertisers to select (or “target"") users who see their messages with great precision, including through inferences about those users’ interests and political affiliations. However, prior work has shown that the targeting may not work as intended, as platforms’ ad delivery algorithms play a crucial role in selecting which subgroups of the targeted users see the ads. In particular, the platforms can selectively deliver ads to subgroups within the target audiences selected by advertisers in ways that can lead to demographic skews along race and gender lines, and do so without the advertiser’s knowledge. In this work we demonstrate that ad delivery algorithms used by Facebook, the most advanced targeted advertising platform, shape the political ad delivery in ways that may not be beneficial to the political campaigns and to societal discourse. In particular, the ad delivery algorithms lead to political messages on Facebook being shown predominantly to people who Facebook thinks already agree with the ad campaign’s message even if the political advertiser targets an ideologically diverse audience. Furthermore, an advertiser determined to reach ideologically non-aligned users is non-transparently charged a high premium compared to their more aligned competitor, a difference from traditional broadcast media. Our results demonstrate that Facebook exercises control over who sees which political messages beyond the control of those who pay for them or those who are exposed to them. Taken together, our findings suggest that the political discourse’s increased reliance on profit-optimized, non-transparent algorithmic systems comes at a cost of diversity of political views that voters are exposed to. Thus, the work raises important questions of fairness and accountability desiderata for ad delivery algorithms applied to political ads.","OAT - Algorithm/Model Audit Case Studies","Case Study",192
"226","Other non-ACM","Case Study","Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim ","Evaluating Feature Importance Estimates ","https://arxiv.org/abs/1806.10758","NeurIPS",2019,NA,"We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classification datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches—VarGrad and SmoothGrad-Squared—outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.","OAT - Algorithm/Model Audit Case Studies","Case Study",208
"227","Other non-ACM","Case Study","Carlini et al.","The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks","https://arxiv.org/abs/1802.08232",NA,2019,NA,"This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models—a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users’ private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization In experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google’s Smart Compose, a commercial text-completion neural network trained on millions of users’ email messages.","OAT - Algorithm/Model Audit Case Studies","Case Study",209
"228","Other non-ACM","Case Study","Koren and Kochenderfer","Efficient Autonomy Validation in Simulation with Adaptive Stress Testing","https://arxiv.org/abs/1907.06795",NA,2019,NA,"     During the development of autonomous systems such as driverless cars, it is important to characterize the scenarios that are most likely to result in failure. Adaptive Stress Testing (AST) provides a way to search for the most-likely failure scenario as a Markov decision process (MDP). Our previous work used a deep reinforcement learning (DRL) solver to identify likely failure scenarios. However, the solver's use of a feed-forward neural network with a discretized space of possible initial conditions poses two major problems. First, the system is not treated as a black box, in that it requires analyzing the internal state of the system, which leads to considerable implementation complexities. Second, in order to simulate realistic settings, a new instance of the solver needs to be run for each initial condition. Running a new solver for each initial condition not only significantly increases the computational complexity, but also disregards the underlying relationship between similar initial conditions. We provide a solution to both problems by employing a recurrent neural network that takes a set of initial conditions from a continuous space as input. This approach enables robust and efficient detection of failures because the solution generalizes across the entire space of initial conditions. By simulating an instance where an autonomous car drives while a pedestrian is crossing a road, we demonstrate the solver is now capable of finding solutions for problems that would have previously been intractable. ","OAT - Algorithm/Model Audit Case Studies","Case Study",216
"229","Other non-ACM","Case Study","Li et al.","Optimizing Collision Avoidance in Dense Airspace using Deep Reinforcement Learning","https://arxiv.org/abs/1912.10146",NA,2019,NA,"New methodologies will be needed to ensure the airspace remains safe and efficient as traffic densities rise to accommodate new unmanned operations. This paper explores how unmanned free-flight traffic may operate in dense airspace. We develop and analyze autonomous collision avoidance systems for aircraft operating in dense airspace where traditional collision avoidance systems fail. We propose a metric for quantifying the decision burden on a collision avoidance system as well as a metric for measuring the impact of the collision avoidance system on airspace. We use deep reinforcement learning to compute corrections for an existing collision avoidance approach to account for dense airspace. The results show that a corrected collision avoidance system can operate more efficiently than traditional methods in dense airspace while maintaining high levels of safety.","OAT - Algorithm/Model Audit Case Studies","Case Study",217
"230","Other non-ACM","Case Study","Bent","Is Algorithmic Affirmative Action Legal?","https://www.law.georgetown.edu/georgetown-law-journal/wp-content/uploads/sites/26/2020/04/Is-Algorithmic-Affirmative-Action-Legal.pdf","Georgetown Law",2019,NA,"This Article is the first to comprehensively explore whether algorithmic affirmative action is lawful. It concludes that both statutory and constitutional antidiscrimination law leave room for race-aware affirmative action in the design of fair algorithms. Along the way, the Article recommends some clarifications of current doctrine and proposes the pursuit of formally race-neutral methods to achieve the admittedly race-conscious goals of algorithmic affirmative action.

The Article proceeds as follows. Part I introduces algorithmic affirmative action. It begins with a brief review of the bias problem in machine learning and then identifies multiple design options for algorithmic fairness. These designs are presented at a theoretical level, rather than in formal mathematical detail. It also highlights some difficult truths that stakeholders, jurists, and legal scholars must understand about accuracy and fairness trade-offs inherent in fairness solutions. Part II turns to the legality of algorithmic affirmative action, beginning with the statutory challenge under Title VII of the Civil Rights Act. Part II argues that voluntary algorithmic affirmative action ought to survive a disparate treatment challenge under Ricci and under the antirace-norming provision of Title VII. Finally, Part III considers the constitutional challenge to algorithmic affirmative action by state actors. It concludes that at least some forms of algorithmic affirmative action, to the extent they are racial classifications at all, ought to survive strict scrutiny as narrowly tailored solutions designed to mitigate the effects of past discrimination.","OAT - Algorithm/Model Audit Case Studies","Case Study",246
"231","Other non-ACM","Case Study","Raji and Yang","ABOUT ML: Annotation and Benchmarking on Understanding and Transparency of Machine Learning Lifecycles","https://arxiv.org/abs/1912.06166",NA,2019,NA,"We present the ""Annotation and Benchmarking on Understanding and Transparency of Machine Learning Lifecycles"" (ABOUT ML) project as an initiative to operationalize ML transparency and work towards a standard ML documentation practice. We make the case for the project's relevance and effectiveness in consolidating disparate efforts across a variety of stakeholders, as well as bringing in the perspectives of currently missing voices that will be valuable in shaping future conversations. We describe the details of the initiative and the gaps we hope this project will help address.","OAT - Algorithm/Model Audit Case Studies","Case Study",256
"232","Other non-ACM","Case Study","Mitchell et al","Model Cards for Model Reporting","https://arxiv.org/abs/1810.03993",NA,2019,"datasheets, model cards, documentation, disaggregated evaluation, fairness evaluation, ML model evaluation, ethical considerations","Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related AI technology, increasing transparency into how well AI technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.","OAT - Algorithm/Model Audit Case Studies","Case Study",258
"233","Other non-ACM","Case Study","Draude, Claude, et al.","Situated algorithms: a sociotechnical systemic approach to bias","https://www.emerald.com/insight/content/doi/10.1108/OIR-10-2018-0332/full/html","OIR",2019,"Gender studies, Bias, Situated Knowledge, Algorithmic Culture, Sociotechnical Systems Design, Strong Objectivity","Purpose

The purpose of this paper is to propose that in order to tackle the question of bias in algorithms, a systemic, sociotechnical and holistic perspective is needed. With reference to the term “algorithmic culture,” the interconnectedness and mutual shaping of society and technology are postulated. A sociotechnical approach requires translational work between and across disciplines. This conceptual paper undertakes such translational work. It exemplifies how gender and diversity studies, by bringing in expertise on addressing bias and structural inequalities, provide a crucial source for analyzing and mitigating bias in algorithmic systems.
Design/methodology/approach

After introducing the sociotechnical context, an overview is provided regarding the contemporary discourse around bias in algorithms, debates around algorithmic culture, knowledge production and bias identification as well as common solutions. The key concepts of gender studies (situated knowledges and strong objectivity) and concrete examples of gender bias then serve as a backdrop for revisiting contemporary debates.
Findings

The key concepts reframe the discourse on bias and concepts such as algorithmic fairness and transparency by contextualizing and situating them. The paper includes specific suggestions for researchers and practitioners on how to account for social inequalities in the design of algorithmic systems.
Originality/value

A systemic, gender-informed approach for addressing the issue is provided, and a concrete, applicable methodology toward a situated understanding of algorithmic bias is laid out, providing an important contribution for an urgent multidisciplinary dialogue.","OAT - Algorithm/Model Audit Case Studies","Case Study",272
"234","Other non-ACM","Case Study","Lee et al","Procedural Justice in Algorithmic Fairness: Leveraging Transparency and Outcome Control for Fair Algorithmic Mediation","https://doi.org/10.1145/3359284",NA,2019,"algorithmic decision, algorithmic mediation, transparency, control, division","As algorithms increasingly take managerial and governance roles, it is ever more important to build them to be perceived as fair and adopted by people. With this goal, we propose a procedural justice framework in algorithmic decision-making drawing from procedural justice theory, which lays out elements that promote a sense of fairness among users. As a case study, we built an interface that leveraged two key elements of the framework---transparency and outcome control---and evaluated it in the context of goods division. Our interface explained the algorithm's allocative fairness properties (standards clarity) and outcomes through an input-output matrix (outcome explanation), then allowed people to interactively adjust the algorithmic allocations as a group (outcome control). The findings from our within-subjects laboratory study suggest that standards clarity alone did not increase perceived fairness; outcome explanation had mixed effects, increasing or decreasing perceived fairness and reducing algorithmic accountability; and outcome control universally improved perceived fairness by allowing people to realize the inherent limitations of decisions and redistribute the goods to better fit their contexts, and by bringing human elements into final decision-making.","OAT - Algorithm/Model Audit Case Studies","Case Study",309
"235","Other non-ACM","Case Study","Shan Jiang, Ronald Robertson, Christo Wilson","Bias Misperceived: The Role of Partisanship and Misinformation in YouTube Comment Moderation","https://ojs.aaai.org/index.php/ICWSM/article/view/3229",NA,2019,NA,"Social media platforms have been the subject of controversy and scrutiny due to the spread of hateful content. To address this problem, the platforms implement content moderation using a mix of human and algorithmic processes. However, content moderation itself has lead to further accusations against the platforms of political bias. In this study, we investigate how channel partisanship and video misinformation affect the likelihood of comment moderation on YouTube. Using a dataset of 84,068 comments on 258 videos, we find that although comments on right-leaning videos are more heavily moderated from a correlational perspective, we find no evidence to support claims of political bias when using a causal model that controls for common confounders (e.g., hate speech). Additionally, we find that comments are more likely to be moderated if the video channel is ideologically extreme, if the video content is false, and if the comments were posted after a fact-check.","OAT - Algorithm/Model Audit Case Studies","Case Study",321
"236","Other non-ACM","Case Study","Danaë Metaxa et al","Search Media and Elections: A Longitudinal Investigation of Political Search Results","https://www.researchgate.net/publication/337126154_Search_Media_and_Elections_A_Longitudinal_Investigation_of_Political_Search_Results","CSCW",2019,"Search media; search engine results; political partisanship","Concern about algorithmically-curated content and its impact on democracy is reaching a fever pitch worldwide. But relative to the role of social media in electoral processes, the role of search results has received less public attention. We develop a theoretical conceptualization of search results as a form of media-search media-and analyze search media in the context of political partisanship in the six months leading up to the 2018 U.S. midterm elections. Our empirical analyses use a total of over 4 million URLs, scraped daily from Google search queries for all candidates running for federal office in the United States in 2018. In our first set of analyses we characterize the nature of search media from the data collected in terms of the types of URLs present and the stability of search results over time. In our second, we annotate URLs' top-level domains with existing measures of political partisanship, examining trends by incumbency, election outcome, and other election characteristics. Among other findings, we note that partisanship trends in search media are largely similar for content about candidates from the two major political parties, whereas there are substantial differences in search media for incumbent versus challenger candidates. This work suggests that longitudinal, systematic audits of search media can reflect real-world political trends. We conclude with implications for web search designers and consumers of political content online.","OAT - Algorithm/Model Audit Case Studies","Case Study",348
"237","Other non-ACM","Data Audit",NA,"Ali, Muhammad, et al. ""Discrimination through optimization: How Facebook's Ad delivery can lead to biased outcomes."" Proceedings of the ACM on human-computer interaction 3.CSCW (2019): 1-30.","https://dl.acm.org/doi/pdf/10.1145/3359301",NA,2019,"online advertising; ad delivery; bias; fairness; policy","The enormous financial success of online advertising platforms is partially due to the precise targeting features they offer. Although researchers and journalists have found many ways that advertisers can target---or exclude---particular groups of users seeing their ads, comparatively little attention has been paid to the implications of the platform's ad delivery process, comprised of the platform's choices about which users see which ads. It has been hypothesized that this process can ""skew"" ad delivery in ways that the advertisers do not intend, making some users less likely than others to see particular ads based on their demographic characteristics. In this paper, we demonstrate that such skewed delivery occurs on Facebook, due to market and financial optimization effects as well as the platform's own predictions about the ""relevance"" of ads to different groups of users. We find that both the advertiser's budget and the content of the ad each significantly contribute to the skew of Facebook's ad delivery. Critically, we observe significant skew in delivery along gender and racial lines for ""real"" ads for employment and housing opportunities despite neutral targeting parameters. Our results demonstrate previously unknown mechanisms that can lead to potentially discriminatory ad delivery, even when advertisers set their targeting parameters to be highly inclusive. This underscores the need for policymakers and platforms to carefully consider the role of the ad delivery optimization run by ad platforms themselves---and not just the targeting choices of advertisers---in preventing discrimination in digital advertising.","OAT - Dataset audit papers","Data Audit",406
"238","Other non-ACM","Meta-Commentary","Daan Kolkman","The (in)credibility of algorithmic models to non-experts","https://www.tandfonline.com/doi/full/10.1080/1369118X.2020.1761860",NA,2019,"Algorithmsethnographycredibilityquantificationdecision making","This paper explores the practices through which experts and non-experts determine the credibility of algorithmic models.","OAT - Meta-Commentary","Meta-Commentary",413
"239","Other non-ACM","Meta-Commentary","Barocas et al.","FAIRNESS AND MACHINE LEARNING: Limitations and Opportunities: Testing discrimination in practice","https://fairmlbook.org/testing.html",NA,2019,NA,NA,"OAT - Meta-Commentary","Meta-Commentary",437
"240","Other non-ACM","Case Study","Muller, Zane","Algorithmic Harms to Workers in the Platform Economy: The Case of Uber.","https://jlsp.law.columbia.edu/wp-content/blogs.dir/213/files/2020/01/Vol53-Muller.pdf",NA,2020,NA,"Technological change has given rise to the much-discussed “gig” or “platform economy,” but labor law has yet to catch up. Platform firms, most prominently Uber, use machine learning algorithms processing torrents of data to power smartphone apps that promise efficiency, flexibility, and autonomy to users who both deliver and consume services. These tools give firms unprecedented information and power over their services, yet they are little-examined in legal scholarship, and case law has yet to meaningfully address them. The potential for exploitation of workers is immense, however the remedies available to workers who are harmed by algorithm design choices are as yet undeveloped. This Note analyzes a set of economic harms to workers uniquely enabled by algorithmic work platforms and explores common law torts as a remedy, using Uber and its driver-partners as a case study. Part II places the emerging “platform economy” in the context of existing labor law. Part III analyzes the design and function of machine learning algorithms, highlighting the Uber application. This Part of the Note also examines divergent incentives between Uber and its users alongside available algorithm design choices, identifying potential economic harms to workers that would be extremely difficult for workers to detect. Part IV surveys existing proposals to protect platform workers and offers common law causes of action sounding in tort and contract as recourse for workers harmed by exploitative algorithm design. ","OAT - Algorithm/Model Audit Case Studies","Case Study",185
"241","Other non-ACM","Case Study","Pandey and Caliskan","Iterative Effect-Size Bias in Ridehailing: Measuring Social Bias in Dynamic Pricing of 100 Million Rides","https://www.researchgate.net/publication/342026661_Iterative_Effect-Size_Bias_in_Ridehailing_Measuring_Social_Bias_in_Dynamic_Pricing_of_100_Million_Rides",NA,2020,NA,"Algorithmic bias is the systematic preferential or discriminatory treatment of a group of people by an artificial intelligence system. In this work we develop a random-effects based metric for the analysis of social bias in supervised machine learning prediction models where model outputs depend on U.S. locations. We define a methodology for using U.S. Census data to measure social bias on user attributes legally protected against discrimination, such as ethnicity, sex, and religion, also known as protected attributes. We evaluate our method on the Strategic Subject List (SSL) gun-violence prediction dataset, where we have access to both U.S. Census data as well as ground truth protected attributes for 224,235 individuals in Chicago being assessed for participation in future gun-violence incidents. Our results indicate that quantifying social bias using U.S. Census  data provides a valid approach to auditing a supervised algorithmic decision-making system. Using our methodology, we then quantify the potential social biases of 100 million ridehailing samples in the city of Chicago. This work is the first large-scale fairness analysis of the dynamic pricing  algorithms used by ridehailing applications. An analysis of Chicago ridehailing samples in conjunction with American Community Survey data indicates possible disparate impact due to social bias based on age, house pricing, education, and ethnicity in the dynamic fare pricing models used by  ridehailing applications, with effect-sizes of 0.74, 0.70, 0.34, and 0.31 (using Cohen’s d) for each demographic respectively. Further, our methodology provides a principled approach to quantifying algorithmic bias on datasets where protected attributes are unavailable, given that U.S. geolocations and algorithmic decisions are provided.","OAT - Algorithm/Model Audit Case Studies","Case Study",189
"242","Other non-ACM","Case Study","Kulynych, Overdorf et al","POTs: Protective Optimization Technologies","https://dl.acm.org/doi/abs/10.1145/3351095.3372853","FAccT",2020,"Fairness and Accountability; Protective Optimization Technologies","Algorithmic fairness aims to address the economic, moral, social, and political impact that digital systems have on populations through solutions that can be applied by service providers. Fairness frameworks do so, in part, by mapping these problems to a narrow definition and assuming the service providers can be trusted to deploy countermeasures. Not surprisingly, these decisions limit fairness frameworks’ ability to capture a variety of harms caused by systems. We characterize fairness limitations using concepts from requirements engineering and from social sciences. We show that the focus on algorithms’ inputs and outputs misses harms that arise from systems interacting with the world; that the focus on bias and discrimination omits broader harms on populations and their environments; and that relying on service providers excludes scenarios where they are not cooperative or intentionally adversarial. We propose Protective Optimization Technologies (POTs). POTs, provide means for affected parties to address the negative impacts of systems in the environment, expanding avenues for political contestation. POTs intervene from outside the system, do not require service providers to cooperate, and can serve to correct, shift, or expose harms that systems impose on populations and their environments. We illustrate the potential and limitations of POTs in two case studies: countering road congestion caused by traffic-beating applications, and recalibrating credit scoring for loan applicants.","OAT - Algorithm/Model Audit Case Studies","Case Study",196
"243","Other non-ACM","Case Study","Silva et al","Facebook Ads Monitor: An Independent Auditing System for Political Ads on Facebook","https://dl.acm.org/doi/fullHtml/10.1145/3366423.3380109","WWW",2020,"Misinformation, political ads, Facebook, transparency mechanisms","The 2016 United States presidential election was marked by the abuse of targeted advertising on Facebook. Concerned with the risk of the same kind of abuse to happen in the 2018 Brazilian elections, we designed and deployed an independent auditing system to monitor political ads on Facebook in Brazil. To do that we first adapted a browser plugin to gather ads from the timeline of volunteers using Facebook. We managed to convince more than 2000 volunteers to help our project and install our tool. Then, we use a Convolution Neural Network (CNN) to detect political Facebook ads using word embeddings. To evaluate our approach, we manually label a data collection of 10k ads as political or non-political and then we provide an in-depth evaluation of proposed approach for identifying political ads by comparing it with classic supervised machine learning methods. Finally, we deployed a real system that shows the ads identified as related to politics. We noticed that not all political ads we detected were present in the Facebook Ad Library for political ads. Our results emphasize the importance of enforcement mechanisms for declaring political ads and the need for independent auditing platforms.","OAT - Algorithm/Model Audit Case Studies","Case Study",202
"244","Other non-ACM","Case Study","Goldblum et al.","Data Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses","https://arxiv.org/abs/2012.10544",NA,2020,NA,"     As machine learning systems grow in scale, so do their training data requirements, forcing practitioners to automate and outsource the curation of training data in order to achieve state-of-the-art performance. The absence of trustworthy human supervision over the data collection process exposes organizations to security vulnerabilities; training data can be manipulated to control and degrade the downstream behaviors of learned models. The goal of this work is to systematically categorize and discuss a wide range of dataset vulnerabilities and exploits, approaches for defending against these threats, and an array of open problems in this space. In addition to describing various poisoning and backdoor threat models and the relationships among them, we develop their unified taxonomy. ","OAT - Algorithm/Model Audit Case Studies","Case Study",222
"245","Other non-ACM","Case Study","Kreps et al.","All the News that's Fit to Fabricate: AI-Generated Text as a Tool of Media Misinformation","https://www.cambridge.org/core/journals/journal-of-experimental-political-science/article/abs/all-the-news-thats-fit-to-fabricate-aigenerated-text-as-a-tool-of-media-misinformation/40F27F0661B839FA47375F538C19FA59",NA,2020,"misinformation, disinformation, foreign policy, public opinion, media","Online misinformation has become a constant; only the way actors create and distribute that information is changing. Advances in artificial intelligence (AI) such as GPT-2 mean that actors can now synthetically generate text in ways that mimic the style and substance of human-created news stories. We carried out three original experiments to study whether these AI-generated texts are credible and can influence opinions on foreign policy. The first evaluated human perceptions of AI-generated text relative to an original story. The second investigated the interaction between partisanship and AI-generated news. The third examined the distributions of perceived credibility across different AI model sizes. We find that individuals are largely incapable of distinguishing between AI- and human-generated text; partisanship affects the perceived credibility of the story; and exposure to the text does little to change individuals’ policy views. The findings have important implications in understanding AI in online misinformation campaigns.","OAT - Algorithm/Model Audit Case Studies","Case Study",233
"246","Other non-ACM","Case Study","Trask and Bluemke et al.","Beyond Privacy Trade-offs with Structured Transparency","https://arxiv.org/abs/2012.08347",NA,2020,NA,"Many socially valuable activities depend on sensitive information, such as medical research, public health policies, political coordination, and personalized digital services. This is often posed as an inherent privacy trade-off: we can benefit from data analysis or retain data privacy, but not both. Across several disciplines, a vast amount of effort has been directed toward overcoming this trade-off to enable productive uses of information without also enabling undesired misuse, a goal we term `structured transparency'. In this paper, we provide an overview of the frontier of research seeking to develop structured transparency. We offer a general theoretical framework and vocabulary, including characterizing the fundamental components -- input privacy, output privacy, input verification, output verification, and flow governance -- and fundamental problems of copying, bundling, and recursive oversight. We argue that these barriers are less fundamental than they often appear. Recent progress in developing `privacy-enhancing technologies' (PETs), such as secure computation and federated learning, may substantially reduce lingering use-misuse trade-offs in a number of domains. We conclude with several illustrations of structured transparency -- in open research, energy management, and credit scoring systems -- and a discussion of the risks of misuse of these tools.","OAT - Algorithm/Model Audit Case Studies","Case Study",235
"247","Other non-ACM","Case Study","Diamantis, Mihailis.","The Extended Corporate Mind: When Corporations Use AI to Break the Law.","https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3422429","NC Law Review",2020,NA,"Algorithms may soon replace employees as the leading cause of corporate harm. For centuries, the law has defined corporate misconduct — anything from civil discrimination to criminal insider trading — in terms of employee misconduct. Today, however, breakthroughs in artificial intelligence and big data allow automated systems to make many corporate decisions, e.g., who gets a loan or what stocks to buy. These technologies introduce valuable efficiencies, but they do not remove (or even always reduce) the incidence of corporate harm. Unless the law adapts, corporations will become increasingly immune to civil and criminal liability as they transfer responsibility from employees to algorithms.

This Article is the first to tackle the full extent of the growing doctrinal gap left by algorithmic corporate misconduct. To hold corporations accountable, the law must sometimes treat them as if they “know” information stored on their servers and “intend” decisions reached by their automated systems. Cognitive science and the philosophy of mind offer a path forward. The “extended mind thesis” complicates traditional views about the physical boundaries of the mind. The thesis states that the mind encompasses any system that sufficiently assists thought, e.g. by facilitating recall or enhancing decision-making. For natural people, the thesis implies that minds can extend beyond the brain to include external cognitive aids, like rolodexes and calculators. This Article adapts the thesis to corporate law. It motivates and proposes a doctrinal framework for extending the corporate mind to the algorithms that are increasingly integral to corporate thought. The law needs such an innovation if it is to hold future corporations to account for their most serious harms. ","OAT - Algorithm/Model Audit Case Studies","Case Study",242
"248","Other non-ACM","Case Study","Xiang and Ho","From Affirmative Action to Affirmative Algorithms: The Legal Challenges Threatening Progress on Algorithmic Fairness","https://law.stanford.edu/publications/affirmative-algorithms-the-legal-grounds-for-fairness-as-awareness/","Stanford Law",2020,NA,"While there has been a flurry of research in algorithmic fairness, what is less recognized is that modern antidiscrimination law may prohibit the adoption of such techniques. We make three contributions. First, we discuss how such approaches will likely be deemed “algorithmic affirmative action,” posing serious legal risks of violating equal protection, particularly under the higher education jurisprudence. Such cases have increasingly turned toward anticlassification, demanding “individualized consideration” and barring formal, quantitative weights for race regardless of purpose. This case law is hence fundamentally incompatible with fairness in machine learning. Second, we argue that the government-contracting cases offer an alternative grounding for algorithmic fairness, as these cases permit explicit and quantitative race-based remedies based on historical discrimination by the actor. Third, while limited, this doctrinal approach also guides the future of algorithmic fairness, mandating that adjustments be calibrated to the entity’s responsibility for historical discrimination causing present-day disparities. The contractor cases provide a legally viable path for algorithmic fairness under current constitutional doctrine but call for more research at the intersection of algorithmic fairness and causal inference to ensure that bias mitigation is tailored to specific causes and mechanisms of bias.","OAT - Algorithm/Model Audit Case Studies","Case Study",247
"249","Other non-ACM","Case Study","Smith, David.","The Citizen and the Automated State: Exploring the Implications of Algorithmic Decision-making in the New Zealand Public Sector.","https://researcharchive.vuw.ac.nz/xmlui/handle/10063/8874",NA,2020,NA,"Algorithms increasingly influence how the state treats its citizens. This thesis examines how the New Zealand public sector’s use of algorithms in decision-making brings benefits, but also invites risks of discrimination, bias, intrusion into privacy and unfair decision-making.



This thesis’s central conclusion is that these risks require a new response. New Zealand currently has a patchwork of existing protections which provide some deterrent against poor algorithmic decision-making. The Privacy Act 1993, Official Information Act 1982, New Zealand Bill of Rights Act 1990, Human Rights Act 1993 and applicable administrative law principles can provide remedies and correct agencies’ poor behaviour in certain cases. But important gaps remain. This thesis examines these protections to show that they do not adequately stem cumulative and systemic harms, and suffer from important practical drawbacks. They do not provide the sound preventative framework that is needed; that is, one which ensures good public sector practice.



This thesis proposes a new regulatory model for public sector use of algorithms. It argues that a key element of any effective regulatory response is the use of “algorithmic impact assessments”. These assessments would mitigate potential risks, and legitimise proportionate public sector use, of algorithms. It is also proposed that an independent regulator complements these assessments by issuing guidance, undertaking algorithm audits, and ensuring political accountability through annual reporting to Parliament. Agencies would have new obligations to disclose how and when algorithms are used in decision-making. Meanwhile, citizens would gain an enhanced right to reasons for algorithmic decisions affecting them and a right to human review. Together these measures would establish a model which would safeguard responsible and effective use of algorithms in New Zealand’s public sector.","OAT - Algorithm/Model Audit Case Studies","Case Study",250
"250","Other non-ACM","Case Study","Mohammed, Png, Isaac","Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence","https://link.springer.com/article/10.1007/s13347-020-00405-8",NA,2020,"Decolonisation , Coloniality ,Sociotechnical foresight , Intercultural ethics , Critical technical practice , Artificial intelligence, Affective community","This paper explores the important role of critical science, and in particular of post-colonial and decolonial theories, in understanding and shaping the ongoing advances in artificial intelligence. Artificial intelligence (AI) is viewed as amongst the technological advances that will reshape modern societies and their relations. While the design and deployment of systems that continually adapt holds the promise of far-reaching positive change, they simultaneously pose significant risks, especially to already vulnerable peoples. Values and power are central to this discussion. Decolonial theories use historical hindsight to explain patterns of power that shape our intellectual, political, economic, and social world. By embedding a decolonial critical approach within its technical practice, AI communities can develop foresight and tactics that can better align research and technology development with established ethical principles, centring vulnerable peoples who continue to bear the brunt of negative impacts of innovation and scientific progress. We highlight problematic applications that are instances of coloniality, and using a decolonial lens, submit three tactics that can form a decolonial field of artificial intelligence: creating a critical technical practice of AI, seeking reverse tutelage and reverse pedagogies, and the renewal of affective and political communities. The years ahead will usher in a wave of new scientific breakthroughs and technologies driven by AI research, making it incumbent upon AI communities to strengthen the social contract through ethical foresight and the multiplicity of intellectual perspectives available to us, ultimately supporting future technologies that enable greater well-being, with the goal of beneficence and justice for all.","OAT - Algorithm/Model Audit Case Studies","Case Study",253
"251","Other non-ACM","Case Study","Assad et al","Algorithmic Pricing and Competition: Empirical Evidence from the German Retail Gasoline Market","https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3682021",NA,2020,"artificial intelligence, pricing-algorithms, collusion, retail gasoline","Economic theory provides ambiguous and conflicting predictions about the association between algorithmic pricing and competition. In this paper we provide the first empirical analysis of this relationship. We study Germany’s retail gasoline market where algorithmic-pricing software became widely available by mid-2017, and for which we have access to comprehensive, highfrequency price data. Because adoption dates are unknown, we identify gas stations that adopt algorithmic-pricing software by testing for structural breaks in markers associated with algorithmic pricing. We find a large number of station-level structural breaks around the suspected time of large-scale adoption. Using this information we investigate the impact of adoption on outcomes linked to competition. Because station-level adoption is endogenous, we use brand headquarter-level adoption decisions as instruments. Our IV results show that adoption increases margins by 9%, but only in non-monopoly markets. Restricting attention to duopoly markets, we find that market-level margins do not change when only one of the two stations adopts, but increase by 28% in markets where both do. These results suggest that AI adoption has a significant effect on competition.","OAT - Algorithm/Model Audit Case Studies","Case Study",255
"252","Other non-ACM","Case Study","Koenecke et al","Racial disparities in automated speech recognition","https://www.pnas.org/content/117/14/7684.short",NA,2020,"fair machine learning, natural language processing, speech-to-text","Automated speech recognition (ASR) systems, which use sophisticated machine-learning algorithms to convert spoken language to text, have become increasingly widespread, powering popular virtual assistants, facilitating automated closed captioning, and enabling digital dictation platforms for health care. Over the last several years, the quality of these systems has dramatically improved, due both to advances in deep learning and to the collection of large-scale datasets used to train the systems. There is concern, however, that these tools do not work equally well for all subgroups of the population. Here, we examine the ability of five state-of-the-art ASR systems—developed by Amazon, Apple, Google, IBM, and Microsoft—to transcribe structured interviews conducted with 42 white speakers and 73 black speakers. In total, this corpus spans five US cities and consists of 19.8 h of audio matched on the age and gender of the speaker. We found that all five ASR systems exhibited substantial racial disparities, with an average word error rate (WER) of 0.35 for black speakers compared with 0.19 for white speakers. We trace these disparities to the underlying acoustic models used by the ASR systems as the race gap was equally large on a subset of identical phrases spoken by black and white individuals in our corpus. We conclude by proposing strategies—such as using more diverse training datasets that include African American Vernacular English—to reduce these performance differences and ensure speech recognition technology is inclusive.","OAT - Algorithm/Model Audit Case Studies","Case Study",259
"253","Other non-ACM","Case Study","Christin","The ethnographer and the algorithm: beyond the black box","https://link.springer.com/article/10.1007/s11186-020-09411-3",NA,2020,"Algorithms. Enrollment . Ethnography . Opacity","A common theme in social science studies of algorithms is that they are profoundly opaque and function as “black boxes.” Scholars have developed several methodological approaches in order to address algorithmic opacity. Here I argue that we can explicitly enroll algorithms in ethnographic research, which can shed light on unexpected aspects of algorithmic systems—including their opacity. I delineate three meso-level strategies for algorithmic ethnography. The first, algorithmic refraction, examines the reconfigurations that take place when computational software, people, and institutions interact. The second strategy, algorithmic comparison, relies on a similarity-and-difference approach to identify the instruments’ unique features. The third strategy, algorithmic triangulation, enrolls algorithms to help gather rich qualitative data. I conclude by discussing the implications of this toolkit for the study of algorithms and future of ethnographic fieldwork.","OAT - Algorithm/Model Audit Case Studies","Case Study",264
"254","Other non-ACM","Case Study","Oala et al ","ML4H Auditing: From Paper to Practice","https://proceedings.mlr.press/v136/oala20a.html",NA,2020,"Machine Learning, Health, Testing","Healthcare systems are currently adapting to digital technologies, producing large quantities of novel data. Based on these data, machine-learning algorithms have been developed to support practitioners in labor-intensive workflows such as diagnosis, prognosis, triage or treatment of disease. However, their translation into medical practice is often hampered by a lack of careful evaluation in different settings. Efforts have started worldwide to establish guidelines for evaluating machine learning for health (ML4H) tools, highlighting the necessity to evaluate models for bias, interpretability, robustness, and possible failure modes. However, testing and adopting these guidelines in practice remains an open challenge. In this work, we target the paper-to-practice gap by applying an ML4H audit framework proposed by the ITU/WHO Focus Group on Artificial Intelligence for Health (FG-AI4H) to three use cases: diagnostic prediction of diabetic retinopathy, diagnostic prediction of Alzheimer’s disease, and cytomorphologic classification for leukemia diagnostics. The assessment comprises dimensions such as bias, interpretability, and robustness. Our results highlight the importance of fine-grained and caseadapted quality assessment, provide support for incorporating proposed quality assessment considerations of ML4H during the entire development life cycle, and suggest improvements for future ML4H reference evaluation frameworks.","OAT - Algorithm/Model Audit Case Studies","Case Study",274
"255","Other non-ACM","Case Study","Sara Kingsley, Clara Wang, Alex Mikhalenko, Proteeti Sinha, Chinmay Kulkarni","Auditing Digital Platforms for Discrimination in Economic Opportunity Advertising","https://arxiv.org/abs/2008.09656",NA,2020,"discrimination, audit, digital platforms, civil rights","Digital platforms, including social networks, are major sources of economic information. Evidence suggests that digital platforms display different socioeconomic opportunities to demographic groups. Our work addresses this issue by presenting a methodology and software to audit digital platforms for bias and discrimination. To demonstrate, an audit of the Facebook platform and advertising network was conducted. Between October 2019 and May 2020, we collected 141,063 ads from the Facebook Ad Library API. Using machine learning classifiers, each ad was automatically labeled by the primary marketing category (housing, employment, credit, political, other). For each of the categories, we analyzed the distribution of the ad content by age group and gender. From the audit findings, we considered and present the limitations, needs, infrastructure and policies that would enable researchers to conduct more systematic audits in the future and advocate for why this work must be done. We also discuss how biased distributions impact what socioeconomic opportunities people have, especially when on digital platforms some demographic groups are disproportionately excluded from the population(s) that receive(s) content regulated by law.","OAT - Algorithm/Model Audit Case Studies","Case Study",311
"256","Other non-ACM","Case Study","Lodewijk Gelauff, Ashish Goel, Kamesh Munagala, Sravya Yandamuri","Advertising for Demographically Fair Outcomes","https://arxiv.org/pdf/2006.03983.pdf",NA,2020,NA,"Online advertising on platforms such as Google or Facebook has become an indispensable outreach tool, including for applications where it is desirable to engage different demographics in an equitable fashion, such as hiring, housing, civic processes, and public health outreach efforts. Somewhat surprisingly, the existing online advertising ecosystem provides very little support for advertising to (and recruiting) a demographically representative cohort.
We study the problem of advertising for demographic representativeness from both an empirical and algorithmic perspective. In essence, we seek fairness in the outcome or conversions generated by the advertising campaigns. We first present detailed empirical findings from real-world experiments for recruiting for civic processes, using which we show that methods using Facebook-inferred features are too inaccurate for achieving equity in outcomes, while targeting via custom audiences based on a list of registered voters segmented on known attributes has much superior accuracy.
This motivates us to consider the algorithmic question of optimally segmenting the list of individuals with known attributes into a few custom campaigns and allocating budgets to them so that we cost-effectively achieve outcome parity with the population on the maximum possible number of demographics. Under the assumption that a platform can reasonably enforce proportionality in spend across demographics, we present efficient exact and approximation algorithms for this problem. We present simulation results on our datasets to show the efficacy of these algorithms in achieving demographic parity. ","OAT - Algorithm/Model Audit Case Studies","Case Study",312
"257","Other non-ACM","Case Study","PINAR BARLAS et al","To “See” is to Stereotype Image Tagging Algorithms, Gender Recognition, and the Accuracy – Fairness Trade-of","https://dl.acm.org/doi/abs/10.1145/3432931","CSCW",2020,"bias detection; gender; image tagging algorithms; social stereotypes","Machine-learned computer vision algorithms for tagging images are increasingly used by developers and researchers, having become popularized as easy-to-use ""cognitive services."" Yet these tools struggle with gender recognition, particularly when processing images of women, people of color and non-binary individuals. Socio-technical researchers have cited data bias as a key problem; training datasets often over-represent images of people and contexts that convey social stereotypes. The social psychology literature explains that people learn social stereotypes, in part, by observing others in particular roles and contexts, and can inadvertently learn to associate gender with scenes, occupations and activities. Thus, we study the extent to which image tagging algorithms mimic this phenomenon. We design a controlled experiment, to examine the interdependence between algorithmic recognition of context and the depicted person's gender. In the spirit of auditing to understand machine behaviors, we create a highly controlled dataset of people images, imposed on gender-stereotyped backgrounds. Our methodology is reproducible and our code publicly available. Evaluating five proprietary algorithms, we find that in three, gender inference is hindered when a background is introduced. Of the two that ""see"" both backgrounds and gender, it is the one whose output is most consistent with human stereotyping processes that is superior in recognizing gender. We discuss the accuracy--fairness trade-off, as well as the importance of auditing black boxes in better understanding this double-edged sword.","OAT - Algorithm/Model Audit Case Studies","Case Study",347
"258","Other non-ACM","Case Study","Eni Mustafaraj","Google’s Top Stories and the Fairness Doctrine: Unbalanced Amplification of Far-Right News Sources","https://www.semanticscholar.org/paper/Google%E2%80%99s-Top-Stories-and-the-Fairness-Doctrine%3A-of-Mustafaraj/1f29794b3130226893fae05c4bbccdc06697964f","IC2S2",2020,NA,"Google’s Top stories is a component of Google Search that frequently surfaces current news when a user performs a search. Our one-year long audit of Google’s search results for the candidates of the 2020 US Presidential Elections indicated that the composition of the Top stories panel shows an unbalanced amplification of far-right news publishers.","OAT - Algorithm/Model Audit Case Studies","Case Study",351
"259","Other non-ACM","Meta-Commentary","Kazuto Fukuchi, Satoshi Hara, Takanori Maehara","Faking Fairness via Stealthily Biased Sampling","https://ojs.aaai.org/index.php/AAAI/article/view/5377",NA,2020,NA,"""We specifically put our focus on a situation where the decision-maker publishes a benchmark dataset as the evidence of his/her fairness and attempts to deceive a person who uses an auditing tool that computes a fairness metric. ""","OAT - Critique/Background","Critique",366
"260","Other non-ACM","Data Audit","Morgan Klaus Scheuerman, Kandrea Wade, Caitlin Lustig, Jed R. Brubaker     ","How We've Taught Algorithms to See Identity: Constructing Race and Gender in Image Databases for Facial Analysis","https://dl.acm.org/doi/10.1145/3392866",NA,2020,"Classification; computer vision; facial analysis; facial classification; facial recognition; facial detection; training and evaluation data; identity; gender; race and ethnicity","Race and gender have long sociopolitical histories of classification in technical infrastructures-from the passport to social media. Facial analysis technologies are particularly pertinent to understanding how identity is operationalized in new technical systems. What facial analysis technologies can do is determined by the data available to train and evaluate them with. In this study, we specifically focus on this data by examining how race and gender are defined and annotated in image databases used for facial analysis. We found that the majority of image databases rarely contain underlying source material for how those identities are defined. Further, when they are annotated with race and gender information, database authors rarely describe the process of annotation. Instead, classifications of race and gender are portrayed as insignificant, indisputable, and apolitical. We discuss the limitations of these approaches given the sociohistorical nature of race and gender. We posit that the lack of critical engagement with this nature renders databases opaque and less trustworthy. We conclude by encouraging database authors to address both the histories of classification inherently embedded into race and gender, as well as their positionality in embedding such classifications.","OAT - Dataset audit papers","Data Audit",378
"261","Other non-ACM","Data Audit","Wang, A. Narayanan, and O. Russakovsky","Revise: A tool for measuring and mitigating bias in visual datasets","https://arxiv.org/abs/2004.07999",NA,2020,"computer vision datasets · bias mitigation · tool","Machine learning models are known to perpetuate and even amplify the biases present in the data. However, these data biases frequently do not become apparent until after the models are deployed. Our work tackles this issue and enables the preemptive analysis of large-scale datasets. REVISE (REvealing VIsual biaSEs) is a tool that assists in the investigation of a visual dataset, surfacing potential biases along three dimensions: (1) object-based, (2) person-based, and (3) geography-based. Object-based biases relate to the size, context, or diversity of the depicted objects. Person-based metrics focus on analyzing the portrayal of people within the dataset. Geography-based analyses consider the representation of different geographic locations. These three dimensions are deeply intertwined in how they interact to bias a dataset, and REVISE sheds light on this; the responsibility then lies with the user to consider the cultural and historical context, and to determine which of the revealed biases may be problematic. The tool further assists the user by suggesting actionable steps that may be taken to mitigate the revealed biases. Overall, the key aim of our work is to tackle the machine learning bias problem early in the pipeline. REVISE is available at this https URL","OAT - Dataset audit papers","Data Audit",380
"262","Other non-ACM","Data Audit","Cynthia C. S. Liem, Annibale Panichella","Oracle Issues in Machine Learning and Where to Find Them","https://dl.acm.org/doi/abs/10.1145/3387940.3391490",NA,2020,NA,"The rise in popularity of machine learning (ML), and deep learning in particular, has both led to optimism about achievements of artificial intelligence, as well as concerns about possible weaknesses and vulnerabilities of ML pipelines. Within the software engineering community, this has led to a considerable body of work on ML testing techniques, including white- and black-box testing for ML models. This means the oracle problem needs to be addressed. For supervised ML applications, oracle information is indeed available in the form of dataset 'ground truth', that encodes input data with corresponding desired output labels. However, while ground truth forms a gold standard, there still is no guarantee it is truly correct. Indeed, syntactic, semantic, and conceptual framing issues in the oracle may negatively affect the ML system's integrity. While syntactic issues may automatically be verified and corrected, the higher-level issues traditionally need human judgment and manual analysis. In this paper, we employ two heuristics based on information entropy and semantic analysis on well-known computer vision models and benchmark data from ImageNet. The heuristics are used to semi-automatically uncover potential higher-level issues in (i) the label taxonomy used to define the ground truth oracle (labels), and (ii) data encoding and representation. In doing this, beyond existing ML testing efforts, we illustrate the need for software engineering strategies that especially target and assess the oracle.","OAT - Dataset audit papers","Data Audit",388
"263","Other non-ACM","Data Audit","Emily Denton, Alex Hanna, Razvan Amironesei, Andrew Smart, Hilary Nicole, Morgan Klaus Scheuerman","Bringing the People Back In: Contesting Benchmark Machine Learning Datasets","https://arxiv.org/abs/2007.07399",NA,2020,NA,"In response to algorithmic unfairness embedded in sociotechnical systems, significant attention has been focused on the contents of machine learning datasets which have revealed biases towards white, cisgender, male, and Western data subjects. In contrast, comparatively less attention has been paid to the histories, values, and norms embedded in such datasets. In this work, we outline a research program - a genealogy of machine learning data - for investigating how and why these datasets have been created, what and whose values influence the choices of data to collect, the contextual and contingent conditions of their creation. We describe the ways in which benchmark datasets in machine learning operate as infrastructure and pose four research questions for these datasets. This interrogation forces us to ""bring the people back in"" by aiding us in understanding the labor embedded in dataset construction, and thereby presenting new avenues of contestation for other researchers encountering the data.","OAT - Dataset audit papers","Data Audit",396
"264","Other non-ACM","Data Audit","Denis Newman-Griffis, Guy Divita, Bart Desmet, Ayah Zirikly, Carolyn P Rosé,  Eric Fosler-Lussier","Ambiguity in medical concept normalization: An analysis of types and coverage in electronic health record datasets","https://academic.oup.com/jamia/article/28/3/516/6034899?login=false",NA,2020,"natural language processing, machine learning, Unified Medical Language System, semantics, vocabulary, controlled","Objectives

Normalizing mentions of medical concepts to standardized vocabularies is a fundamental component of clinical text analysis. Ambiguity—words or phrases that may refer to different concepts—has been extensively researched as part of information extraction from biomedical literature, but less is known about the types and frequency of ambiguity in clinical text. This study characterizes the distribution and distinct types of ambiguity exhibited by benchmark clinical concept normalization datasets, in order to identify directions for advancing medical concept normalization research.
Materials and Methods

We identified ambiguous strings in datasets derived from the 2 available clinical corpora for concept normalization and categorized the distinct types of ambiguity they exhibited. We then compared observed string ambiguity in the datasets with potential ambiguity in the Unified Medical Language System (UMLS) to assess how representative available datasets are of ambiguity in clinical language.
Results

We found that <15% of strings were ambiguous within the datasets, while over 50% were ambiguous in the UMLS, indicating only partial coverage of clinical ambiguity. The percentage of strings in common between any pair of datasets ranged from 2% to only 36%; of these, 40% were annotated with different sets of concepts, severely limiting generalization. Finally, we observed 12 distinct types of ambiguity, distributed unequally across the available datasets, reflecting diverse linguistic and medical phenomena.
Discussion

Existing datasets are not sufficient to cover the diversity of clinical concept ambiguity, limiting both training and evaluation of normalization methods for clinical text. Additionally, the UMLS offers important semantic information for building and evaluating normalization methods.
Conclusions

Our findings identify 3 opportunities for concept normalization research, including a need for ambiguity-specific clinical datasets and leveraging the rich semantics of the UMLS in new methods and evaluation measures for normalization.","OAT - Dataset audit papers","Data Audit",398
"265","Other non-ACM","Meta-Commentary","Peter Polack","Beyond algorithmic reformism: Forward engineering the designs of algorithmic systems","https://journals.sagepub.com/doi/full/10.1177/2053951720913064",NA,2020,"Critical algorithm studies, predictive policing, design studies, algorithmic bias, algorithmic opacity, algorithmic accountability","we conducted a comparative audit of Google and Facebook’s political advertising policies during the 2018 U.S. midterm election. ","OAT - Meta-Commentary","Meta-Commentary",420
"266","Other non-ACM","Meta-Commentary","Sritej Attaluri, Sara Scheffler","Proposing Safeguards for Governmentally-Regulated Risk Assessment Mechanisms","https://www.sarahscheffler.net/2018/Scheffler_Attaluri-Risk_Assessment_Safeguards.pdf",NA,2020,NA,"""We examine the costs of the systems on individuals, the system holders, and society, we analyze the inputs to the system, and we describe the transparency (or lack thereof) within the systems. Using three case studies—the Unified Passenger system (UPAX), the COMPAS Risk & Need Assessment System, and the FICO score—we develop a standardized set of potential technical requirements to mitigate abuse and ensure individuals are treated fairlywhile remaining within the constraints levied by the system’s purpose.""","OAT - Meta-Commentary","Meta-Commentary",430
"267","Other non-ACM","Case Study","Kudzanai Dambanemuya et al ","Auditing the Information Quality of News-Related Queries on the Alexa Voice Assistant","https://dl.acm.org/doi/abs/10.1145/3449157",NA,2021,"algorithmic accountability, voice assistants, information quality, audit framework","Smart speakers are becoming increasingly ubiquitous in society and are now used for satisfying a variety of information needs, from asking about the weather or traffic to accessing the latest breaking news information. Their growing use for news and information consumption presents new questions related to the quality, source diversity, and comprehensiveness of the news-related information they convey. These questions have significant implications for voice assistant technologies acting as algorithmic information intermediaries, but systematic information quality audits have not yet been undertaken. To address this gap, we develop a methodological approach for evaluating information quality in voice assistants for news-related queries. We demonstrate the approach on the Amazon Alexa voice assistant, first characterising Alexa's performance in terms of response relevance, accuracy, and timeliness, and then further elaborating analyses of information quality based on query phrasing, news category, and information provenance. We discuss the implications of our findings for future audits of information quality on voice assistants and for the consumption of news information via such algorithmic intermediaries more broadly.","OAT - Algorithm/Model Audit Case Studies","Case Study",275
"268","Other non-ACM","Case Study","Matias et al","Software-Supported Audits of Decision-Making Systems: Testing Google and Facebook's Political Advertising Policies","https://arxiv.org/abs/2103.00064",NA,2021,"audits, system design, datasets, social networks, accountability","How can society understand and hold accountable complex human and algorithmic decision-making systems whose systematic errors are opaque to the public? These systems routinely make decisions on individual rights and well-being, and on protecting society and the democratic process. Practical and statistical constraints on external audits--such as dimensional complexity--can lead researchers and regulators to miss important sources of error in these complex decision-making systems. In this paper, we design and implement a software-supported approach to audit studies that auto-generates audit materials and coordinates volunteer activity. We implemented this software in the case of political advertising policies enacted by Facebook and Google during the 2018 U.S. election. Guided by this software, a team of volunteers posted 477 auto-generated ads and analyzed the companies' actions, finding systematic errors in how companies enforced policies. We find that software can overcome some common constraints of audit studies, within limitations related to sample size and volunteer capacity.","OAT - Algorithm/Model Audit Case Studies","Case Study",278
"269","Other non-ACM","Case Study","Laura Blattner, Scott Nelson, Jan Spiess","Unpacking the Black Box: Regulating Algorithmic Decisions","https://eaamo2021.eaamo.org/accepted/acceptednonarchival/EAMO21_paper_107.pdf",NA,2021,NA,"We characterize optimal oversight of algorithms in a world where an agent designs a complex prediction
function but a principal is limited in the amount of information she can learn about the prediction function.
We show that limiting agents to prediction functions that are simple enough to be fully transparent is inefficient
as long as the bias induced by misalignment between principal’s and agent’s preferences is small relative to
the uncertainty about the true state of the world. Ex-post algorithmic audits can improve welfare, but the
gains depend on the design of the audit tools. Tools that focus on minimizing overall information loss, the
focus of many post-hoc explainer tools, will generally be inefficient since they focus on explaining the average
behavior of the prediction function rather than sources of mis-prediction, which matter for welfare-relevant
outcomes. Targeted tools that focus on the source of incentive misalignment, e.g., excess false positives or
racial disparities, can provide first-best solutions. We provide empirical support for our theoretical findings
using an application in consumer lending.","OAT - Algorithm/Model Audit Case Studies","Case Study",313
"270","Other non-ACM","Case Study","
Danaë Metaxa et al","An Image of Society: Gender and Racial Representation and Impact in Image Search Results for Occupations","https://dl.acm.org/doi/abs/10.1145/3449100","CSCW",2021,"Algorithm audit; search media; marginalized identities; algorithmic bias","Algorithmically-mediated content is both a product and producer of dominant social narratives, and it has the potential to impact users' beliefs and behaviors. We present two studies on the content and impact of gender and racial representation in image search results for common occupations. In Study 1, we compare 2020 workforce gender and racial composition to that reflected in image search. We find evidence of underrepresentation on both dimensions: women are underrepresented in search at a rate of 42% women for a field with 50% women; people of color are underrepresented with 16% in search compared to an occupation with 22% people of color (the latter being proportional to the U.S. workforce). We also compare our gender representation data with that collected in 2015 by Kay et al., finding little improvement in the last half-decade. In Study 2, we study people's impressions of occupations and sense of belonging in a given field when shown search results with different proportions of women and people of color. We find that both axes of representation as well as people's own racial and gender identities impact their experience of image search results. We conclude by emphasizing the need for designers and auditors of algorithms to consider the disparate impacts of algorithmic content on users of marginalized identities.","OAT - Algorithm/Model Audit Case Studies","Case Study",344
"271","Other non-ACM","Case Study","JACK BANDY and BRENT HECHT","Errors in Geotargeted Display Advertising:Good News for Local Journalism?","https://www.researchgate.net/publication/351119278_Errors_in_Geotargeted_Display_Advertising_Good_News_for_Local_Journalism","CSCW",2021,"Geopositioning, Advertising, Algorithm Auditing, Google","The rise of geotargeted online advertising has disrupted the business model of local journalism, but it remains ambiguous whether online advertising platforms can effectively reach local audiences. To address this ambiguity, we present a focused study auditing the positional accuracy of geotargeted display advertisements on Google. We measure the frequency and severity of geotargeting errors by targeting display ads to random ZIP codes across the United States, collecting self-reported location information from users who click on the advertisement. We find evidence that geotargeting errors are common, but minor in terms of advertising goals. While 41% of respondents lived outside the target ZIP code, only 11% lived outside the target county, and only 2% lived outside the target state. We also present details regarding a high volume of suspicious clicks in our data, which made the cost per sample extremely expensive. The paper concludes by discussing implications for advertisers, the business of local journalism, and future research.","OAT - Algorithm/Model Audit Case Studies","Case Study",345
"272","Other non-ACM","Case Study","JACK BANDY and NICHOLAS DIAKOPOULOS,","More Accounts, Fewer Links: How Algorithmic CurationImpacts Media Exposure in Twitter Timelines","https://www.researchgate.net/publication/351150819_More_Accounts_Fewer_Links_How_Algorithmic_Curation_Impacts_Media_Exposure_in_Twitter_Timelines","CSCW",2021,"algorithm auditing, twitter, content ranking, social media","Algorithmic timeline curation is now an integral part of Twitter's platform, affecting information exposure for more than 150 million daily active users. Despite its large-scale and high-stakes impact, especially during a public health emergency such as the COVID-19 pandemic, the exact effects of Twitter's curation algorithm generally remain unknown. In this work, we present a sock-puppet audit that aims to characterize the effects of algorithmic curation on source diversity and topic diversity in Twitter timelines. We created eight sock puppet accounts to emulate representative real-world users, selected through a large-scale network analysis. Then, for one month during early 2020, we collected the puppets' timelines twice per day. Broadly, our results show that algorithmic curation increases source diversity in terms of both Twitter accounts and external domains, even though it drastically decreases the number of external links in the timeline. In terms of topic diversity, algorithmic curation had a mixed effect, slightly amplifying a cluster of politically-focused tweets while squelching clusters of tweets focused on COVID-19 fatalities and health information. Finally, we present some evidence that the timeline algorithm may exacerbate partisan differences in exposure to different sources and topics. The paper concludes by discussing broader implications in the context of algorithmic gatekeeping.","OAT - Algorithm/Model Audit Case Studies","Case Study",346
"273","Other non-ACM","Case Study","Kokil Jaidka, Subhayan Mukerjee and Yphtach Lelkes","An audit of Twitter's shadow ban sanctions in the United States","https://easychair.org/publications/preprint_download/z4jt","IC2S2",2021,"Political Communication, Social Media, Policy Discussion",NA,"OAT - Algorithm/Model Audit Case Studies","Case Study",350
"274","Other non-ACM","Case Study","Aleksandra Urman et al","Auditing Source Diversity Bias in Video Search Results Using Virtual Agents","https://arxiv.org/pdf/2106.02715.pdf","WWW",2021,"source diversity bias, algorithmic auditing, web search","We audit the presence of domain-level source diversity bias in video search results. Using a virtual agent-based approach, we compare outputs of four Western and one non-Western search engines for English and Russian queries. Our findings highlight that source diversity varies substantially depending on the language with English queries returning more diverse outputs. We also find disproportionately high presence of a single platform, YouTube, in top search outputs for all Western search engines except Google. At the same time, we observe that Youtube's major competitors such as Vimeo or Dailymotion do not appear in the sampled Google's video search results. This finding suggests that Google might be downgrading the results from the main competitors of Google-owned Youtube and highlights the necessity for further studies focusing on the presence of own-content bias in Google's search results.","OAT - Algorithm/Model Audit Case Studies","Case Study",353
"275","Other non-ACM","Meta-Commentary",NA,"It's COMPASlicated: The Messy Relationship between RAI Datasets and Algorithmic Fairness Benchmarks","https://arxiv.org/abs/2106.05498","NeurIPS Datasets & Benchmarks",2021,NA,"Risk assessment instrument (RAI) datasets, particularly ProPublica's COMPAS dataset, are commonly used in algorithmic fairness papers due to benchmarking practices of comparing algorithms on datasets used in prior work. In many cases, this data is used as a benchmark to demonstrate good performance without accounting for the complexities of criminal justice (CJ) processes. However, we show that pretrial RAI datasets can contain numerous measurement biases and errors, and due to disparities in discretion and deployment, algorithmic fairness applied to RAI datasets is limited in making claims about real-world outcomes. These reasons make the datasets a poor fit for benchmarking under assumptions of ground truth and real-world impact. Furthermore, conventional practices of simply replicating previous data experiments may implicitly inherit or edify normative positions without explicitly interrogating value-laden assumptions. Without context of how interdisciplinary fields have engaged in CJ research and context of how RAIs operate upstream and downstream, algorithmic fairness practices are misaligned for meaningful contribution in the context of CJ, and would benefit from transparent engagement with normative considerations and values related to fairness, justice, and equality. These factors prompt questions about whether benchmarks for intrinsically socio-technical systems like the CJ system can exist in a beneficial and ethical way.","OAT - Critique/Background","Critique",367
"276","Other non-ACM","Data Audit","K. Peng, A. Mathur, and A. Narayanan,","Mitigating dataset harms requires stewardship: Lessons from 1000 papers","https://arxiv.org/abs/2108.02922",NA,2021,NA,"Machine learning datasets have elicited concerns about privacy, bias, and unethical applications, leading to the retraction of prominent datasets such as DukeMTMC, MS-Celeb-1M, and Tiny Images. In response, the machine learning community has called for higher ethical standards in dataset creation. To help inform these efforts, we studied three influential but ethically problematic face and person recognition datasets -- Labeled Faces in the Wild (LFW), MS-Celeb-1M, and DukeMTM -- by analyzing nearly 1000 papers that cite them. We found that the creation of derivative datasets and models, broader technological and social change, the lack of clarity of licenses, and dataset management practices can introduce a wide range of ethical concerns. We conclude by suggesting a distributed approach to harm mitigation that considers the entire life cycle of a dataset.","OAT - Dataset audit papers","Data Audit",369
"277","Other non-ACM","Data Audit","Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco,  Dirk Groeneveld, Margaret Mitchell, Matt Gardner, Hugging Face","Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus ","https://arxiv.org/pdf/2104.08758.pdf",NA,2021,NA,"Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.","OAT - Dataset audit papers","Data Audit",370
"278","Other non-ACM","Data Audit","Amandalynne Paullada, Inioluwa Deborah Raji, Emily M Bender, Emily Denton, Alex Hanna","Data and its (dis) contents: A survey of dataset development and use in machine learning research","https://www.sciencedirect.com/science/article/pii/S2666389921001847",NA,2021,NA,"In this work, we survey a breadth of literature that has revealed the limitations of predominant practices for dataset collection and use in the field of machine learning. We cover studies that critically review the design and development of datasets with a focus on negative societal impacts and poor outcomes for system performance. We also cover approaches to filtering and augmenting data and modeling techniques aimed at mitigating the impact of bias in datasets. Finally, we discuss works that have studied data practices, cultures, and disciplinary norms and discuss implications for the legal, ethical, and functional challenges the field continues to face. Based on these findings, we advocate for the use of both qualitative and quantitative approaches to more carefully document and analyze datasets during the creation and usage phases.","OAT - Dataset audit papers","Data Audit",371
"279","Other non-ACM","Data Audit","Helen Ngo, João G.M. Araújo, Jeffrey Hui, Nicholas Frosst","No News is Good News: A Critique of the One Billion Word Benchmark ","https://arxiv.org/abs/2110.12609",NA,2021,NA,"The One Billion Word Benchmark is a dataset derived from the WMT 2011 News Crawl, commonly used to measure language modeling ability in natural language processing. We train models solely on Common Crawl web scrapes partitioned by year, and demonstrate that they perform worse on this task over time due to distributional shift. Analysis of this corpus reveals that it contains several examples of harmful text, as well as outdated references to current events. We suggest that the temporal nature of news and its distribution shift over time makes it poorly suited for measuring language modeling ability, and discuss potential impact and considerations for researchers building language models and evaluation datasets.","OAT - Dataset audit papers","Data Audit",372
"280","Other non-ACM","Data Audit","Jack Bandy, Nicholas Vincent","Addressing ""Documentation Debt"" in Machine Learning Research: A Retrospective Datasheet for BookCorpus ","https://arxiv.org/abs/2105.05241",NA,2021,"dataset documentation, language models, natural language processing, computational linguistics","Recent literature has underscored the importance of dataset documentation work for machine learning, and part of this work involves addressing ""documentation debt"" for datasets that have been used widely but documented sparsely. This paper aims to help address documentation debt for BookCorpus, a popular text dataset for training large language models. Notably, researchers have used BookCorpus to train OpenAI's GPT-N models and Google's BERT models, even though little to no documentation exists about the dataset's motivation, composition, collection process, etc. We offer a preliminary datasheet that provides key context and information about BookCorpus, highlighting several notable deficiencies. In particular, we find evidence that (1) BookCorpus likely violates copyright restrictions for many books, (2) BookCorpus contains thousands of duplicated books, and (3) BookCorpus exhibits significant skews in genre representation. We also find hints of other potential deficiencies that call for future research, including problematic content, potential skews in religious representation, and lopsided author contributions. While more work remains, this initial effort to provide a datasheet for BookCorpus adds to growing literature that urges more careful and systematic documentation for machine learning datasets.","OAT - Dataset audit papers","Data Audit",373
"281","Other non-ACM","Data Audit","Abeba Birhane; Vinay Uday Prabhu ","Large image datasets: A pyrrhic win for computer vision?","https://ieeexplore.ieee.org/abstract/document/9423393",NA,2021,"Computer vision ,Conferences ,IEEE Constitution ,Faces","In this paper we investigate problematic practices and consequences of large scale vision datasets (LSVDs). We examine broad issues such as the question of consent and justice as well as specific concerns such as the inclusion of verifiably pornographic images in datasets. Taking the ImageNet-ILSVRC-2012 dataset as an example, we perform a cross-sectional model-based quantitative census covering factors such as age, gender, NSFW content scoring, class- wise accuracy, human-cardinality-analysis, and the semanticity of the image class information in order to statistically investigate the extent and subtleties of ethical transgressions. We then use the census to help hand-curate a look-up-table of images in the ImageNet-ILSVRC-2012 dataset that fall into the categories of verifiably pornographic: shot in a non-consensual setting (up-skirt), beach voyeuristic, and exposed private parts. We survey the landscape of harm and threats both the society at large and individuals face due to uncritical and ill-considered dataset curation practices. We then propose possible courses of correction and critique their pros and cons. We have duly open-sourced all of the code and the census meta-datasets generated in this endeavor for the computer vision community to build on. By unveiling the severity of the threats, our hope is to motivate the constitution of mandatory Institutional Review Boards (IRB) for large scale dataset curation.","OAT - Dataset audit papers","Data Audit",374
"282","Other non-ACM","Data Audit","Abeba Birhane, Vinay Uday Prabhu, Emmanuel Kahembwe","Multimodal datasets: misogyny, pornography, and malignant stereotypes ","https://arxiv.org/abs/2110.01963",NA,2021,NA,"We have now entered the era of trillion parameter machine learning models trained on billion-sized datasets scraped from the internet. The rise of these gargantuan datasets has given rise to formidable bodies of critical work that has called for caution while generating these large datasets. These address concerns surrounding the dubious curation practices used to generate these datasets, the sordid quality of alt-text data available on the world wide web, the problematic content of the CommonCrawl dataset often used as a source for training large language models, and the entrenched biases in large-scale visio-linguistic models (such as OpenAI's CLIP model) trained on opaque datasets (WebImageText). In the backdrop of these specific calls of caution, we examine the recently released LAION-400M dataset, which is a CLIP-filtered dataset of Image-Alt-text pairs parsed from the Common-Crawl dataset. We found that the dataset contains, troublesome and explicit images and text pairs of rape, pornography, malign stereotypes, racist and ethnic slurs, and other extremely problematic content. We outline numerous implications, concerns and downstream harms regarding the current state of large scale datasets while raising open questions for various stakeholders including the AI community, regulators, policy makers and data subjects.","OAT - Dataset audit papers","Data Audit",375
"283","Other non-ACM","Data Audit","M. K. Scheuerman, E. Denton, and A. Hanna,","Do datasets have politics? disciplinary values in computer vision dataset development","https://arxiv.org/abs/2108.04308",NA,2021,"Datasets, computer vision, work practice, machine learning, values in design","Data is a crucial component of machine learning. The field is reliant on data to train, validate, and test models. With increased technical capabilities, machine learning research has boomed in both academic and industry settings, and one major focus has been on computer vision. Computer vision is a popular domain of machine learning increasingly pertinent to real-world applications, from facial recognition in policing to object detection for autonomous vehicles. Given computer vision's propensity to shape machine learning research and impact human life, we seek to understand disciplinary practices around dataset documentation - how data is collected, curated, annotated, and packaged into datasets for computer vision researchers and practitioners to use for model tuning and development. Specifically, we examine what dataset documentation communicates about the underlying values of vision data and the larger practices and goals of computer vision as a field. To conduct this study, we collected a corpus of about 500 computer vision datasets, from which we sampled 114 dataset publications across different vision tasks. Through both a structured and thematic content analysis, we document a number of values around accepted data practices, what makes desirable data, and the treatment of humans in the dataset construction process. We discuss how computer vision datasets authors value efficiency at the expense of care; universality at the expense of contextuality; impartiality at the expense of positionality; and model work at the expense of data work. Many of the silenced values we identify sit in opposition with social computing practices. We conclude with suggestions on how to better incorporate silenced values into the dataset creation and curation process.","OAT - Dataset audit papers","Data Audit",379
"284","Other non-ACM","Data Audit","E. Denton, A. Hanna, R. Amironesei, A. Smart, and H. Nicole","On the genealogy of machine learning datasets: A critical history of imagenet","https://journals.sagepub.com/doi/10.1177/20539517211035955",NA,2021,"Machine learning, artificial intelligence, big data, AI ethics, algorithmic fairness , genealogy","In response to growing concerns of bias, discrimination, and unfairness perpetuated by algorithmic systems, the datasets used to train and evaluate machine learning models have come under increased scrutiny. Many of these examinations have focused on the contents of machine learning datasets, finding glaring underrepresentation of minoritized groups. In contrast, relatively little work has been done to examine the norms, values, and assumptions embedded in these datasets. In this work, we conceptualize machine learning datasets as a type of informational infrastructure, and motivate a genealogy as method in examining the histories and modes of constitution at play in their creation. We present a critical history of ImageNet as an exemplar, utilizing critical discourse analysis of major texts around ImageNet’s creation and impact. We find that assumptions around ImageNet and other large computer vision datasets more generally rely on three themes: the aggregation and accumulation of more data, the computational construction of meaning, and making certain types of data labor invisible. By tracing the discourses that surround this influential benchmark, we contribute to the ongoing development of the standards and norms around data development in machine learning and artificial intelligence research.","OAT - Dataset audit papers","Data Audit",381
"285","Other non-ACM","Data Audit","Shazia AfzalIBM Research, India; C Rajmohan; Manish Kesarwani; Sameep Mehta; Hima Patel","Data Readiness Report ","https://ieeexplore.ieee.org/abstract/document/9592479",NA,2021,"Productivity  ,Data integrity ,Conferences ,Pipelines, Documentation ,Machine learning ,Data models","Data exploration and quality analysis is an important yet tedious process in the AI pipeline. Current data cleaning and data readiness assessment practices for machine learning tasks are mostly conducted in an arbitrary manner which limits their reuse and often results in loss of productivity. We introduce the concept of a Data Readiness Report as accompanying documentation to a dataset that allows data consumers to get detailed insights into the quality of data. Data characteristics and challenges on various quality dimensions are identified and documented, keeping in mind the principles of transparency and explainability. The Data Readiness Report also serves as a record of all data assessment operations, including applied transformations. This provides a detailed lineage for data governance and management. In effect, the report captures and documents the actions taken by various personas in a data readiness and assessment workflow. Over time this becomes a repository of best practices and can potentially drive a recommendation system for building automated data readiness workflows on the lines of AutoML [1]. The data readiness report could serve as a valuable asset for organizing and operationalizing data in a Data-as-a-service model as it augments the trust and reliability of the datasets. We anticipate that together with the Datasheets [2], Dataset Nutrition Label [3], FactSheets [4] and Model Cards [5], the Data Readiness Report completes the AI documentation pipeline and increases trust and re-useability of data.","OAT - Dataset audit papers","Data Audit",392
"286","Other non-ACM","Data Audit","Inioluwa Deborah Raji, Emily M. Bender, Amandalynne Paullada, Emily Denton, Alex Hanna","AI and the Everything in the Whole Wide World Benchmark","https://arxiv.org/abs/2111.15366",NA,2021,NA,"There is a tendency across different subfields in AI to valorize a small collection of influential benchmarks. These benchmarks operate as stand-ins for a range of anointed common problems that are frequently framed as foundational milestones on the path towards flexible and generalizable AI systems. State-of-the-art performance on these benchmarks is widely understood as indicative of progress towards these long-term goals. In this position paper, we explore the limits of such benchmarks in order to reveal the construct validity issues in their framing as the functionally ""general"" broad measures of progress they are set up to be.","OAT - Dataset audit papers","Data Audit",393
"287","Other non-ACM","Data Audit","Bernard Koch, Emily Denton, Alex Hanna, Jacob G. Foster","Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research","https://arxiv.org/abs/2112.01716",NA,2021,NA,"Benchmark datasets play a central role in the organization of machine learning research. They coordinate researchers around shared research problems and serve as a measure of progress towards shared goals. Despite the foundational role of benchmarking practices in this field, relatively little attention has been paid to the dynamics of benchmark dataset use and reuse, within or across machine learning subcommunities. In this paper, we dig into these dynamics. We study how dataset usage patterns differ across machine learning subcommunities and across time from 2015-2020. We find increasing concentration on fewer and fewer datasets within task communities, significant adoption of datasets from other tasks, and concentration across the field on datasets that have been introduced by researchers situated within a small number of elite institutions. Our results have implications for scientific evaluation, AI ethics, and equity/access within the field.","OAT - Dataset audit papers","Data Audit",395
"288","Other non-ACM","Data Audit","Ryan Carrier, Merve Hickok, Adam Leon Smith","Bias Mitigation in Data Sets","https://osf.io/preprints/socarxiv/z8qrb/",NA,2021,"artificial intelligence audit bias data quality machine learning","Tackling sample bias, Non-Response Bias, Cognitive Bias, and disparate impact associated with Protected Categories in three parts/papers, data, algorithm construction, and output impact. This paper covers the Data section.","OAT - Dataset audit papers","Data Audit",397
"289","Other non-ACM","Data Audit","Rashida Richardson and Amba Kak","Suspect Development Systems: Databasing Marginality and Enforcing Discipline","https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3868392",NA,2021,"technology policy, databases, criminal justice, welfare, biometrics, citizenship","Algorithmic accountability law — focused on the regulation of data-driven systems like artificial intelligence (AI) or automated decision-making (ADM) tools — is the subject of lively policy debates, heated advocacy, and mainstream media attention. Concerns have moved beyond data protection and individual due process to encompass a broader range of group-level harms such as, discrimination and modes of democratic participation. While this is a welcome and long overdue shift, this discourse has ignored systems like databases, that are viewed as technically ‘rudimentary’ and often siloed from regulatory scrutiny and public attention. Additionally, burgeoning regulatory proposals like algorithmic impact assessments are not structured to surface important yet often overlooked social, organizational and political economy contexts that are critical to evaluating the practical functions and outcomes of technological systems.

This article presents a new categorical lens and analytical framework that aims to address and overcome these limitations. “Suspect Development Systems” (SDS) refers to: (1) information technologies used by government and private actors, (2) to manage vague or often immeasurable social risk based on presumed or real social conditions (e.g. violence, corruption, substance abuse), (3) that subjects targeted individuals or groups to greater suspicion, differential treatment, and more punitive and exclusionary outcomes. This frame includes some of the most recent and egregious examples of data-driven tools (such as predictive policing or risk assessments) but critically, it is also inclusive of a broader range of database systems that are currently at the margins of technology policy discourse. By examining the use of various criminal intelligence databases in India, the United Kingdom, and the United States, we developed a framework of five categories of features (technical, legal, political economy, organizational, and social) that together and separately influence how these technologies function in practice, the ways they are used, and the outcomes they produce. We then apply this analytical framework to welfare system databases, universal or ID number databases, and citizenship databases to demonstrate the value of this framework in both identifying and evaluating emergent or under-examined technologies in other sensitive social domains.

Suspect Development Systems is an intervention in legal scholarship and practice as it provides a much-needed definitional and analytical framework for understanding an ever-evolving ecosystem of technologies embedded and employed in modern governance. Our analysis also helps redirect attention toward important yet often under-examined contexts, conditions, and consequences that are pertinent to the development of meaningful legislative or regulatory interventions in the field of algorithmic accountability. The cross-jurisdictional evidence put forth across this Article illuminates the value of examining commonalities between the Global North and South to inform our understanding of how seemingly disparate technologies and contexts are in fact coaxial, which is the basis for building more global solidarity.","OAT - Dataset audit papers","Data Audit",401
"290","Other non-ACM","Data Audit",NA,"Retiring Adult: New Datasets for Fair Machine Learning","https://papers.nips.cc/paper/2021/hash/32e54441e6382a7fbacbbbaf3c450059-Abstract.html",NA,2021,NA,"Although the fairness community has recognized the importance of data, researchers in the area primarily rely on UCI Adult when it comes to tabular data. Derived from a 1994 US Census survey, this dataset has appeared in hundreds of research papers where it served as the basis for the development and comparison of many algorithmic fairness interventions. We reconstruct a superset of the UCI Adult data from available US Census sources and reveal idiosyncrasies of the UCI Adult dataset that limit its external validity. Our primary contribution is a suite of new datasets derived from US Census surveys that extend the existing data ecosystem for research on fair machine learning. We create prediction tasks relating to income, employment, health, transportation, and housing. The data span multiple years and all states of the United States, allowing researchers to study temporal shift and geographic variation. We highlight a broad initial sweep of new empirical insights relating to trade-offs between fairness criteria, performance of algorithmic interventions, and the role of distribution shift based on our new datasets. Our findings inform ongoing debates, challenge some existing narratives, and point to future research directions.","OAT - Dataset audit papers","Data Audit",404
"291","Other non-ACM","Meta-Commentary","Vecchione et al","Algorithmic Auditing and Social Justice: Lessons from the History of Audit Studies","https://dl.acm.org/doi/pdf/10.1145/3465416.3483294",NA,2021,NA,"A paper that draws from the history of social science audits to examine algorithmic audits ","OAT - Meta-Commentary","Meta-Commentary",411
"292","Other non-ACM","Meta-Commentary","Metaxa et al ","Auditing Algorithms: Understanding Algorithmic Systems from the Outside In","https://www.nowpublishers.com/article/Details/HCI-083",NA,2021,"Computer Supported Cooperative Work,  Computer Supported Cooperative Work: History of CSCW in HCI,  Computer Supported Cooperative Work: Organizational issues,  History of the research community,  Interdisciplinary influence,  Interdisciplinary influence: The role of the social sciences in HCI,  Interdisciplinary influence: Artificial intelligence and the user interface,  Privacy and social implications,  Technology,  Data Mining,  Ethics,  Hypertext/Hypermedia,  Identity,  Privacy,  Search,  Social Networking,  Standards,  The Law and the Web,  Trust and,Provenance,  Web search,  Accountability,  Applications and case studies","an overview of the algorithm audit methodology, including the history of audit studies in the social sciences","OAT - Meta-Commentary","Meta-Commentary",412
"293","Other non-ACM","Meta-Commentary","Ayling et al ","Putting AI ethics to work: are the tools fit for purpose?","https://link.springer.com/article/10.1007/s43681-021-00084-x",NA,2021,"AI, Ethics ,Impact assessment ,Audit","This work provides an assessment of these practical frameworks with the lens of known best practices for impact assessment and audit of technology.","OAT - Meta-Commentary","Meta-Commentary",419
"294","Other non-ACM","Meta-Commentary","Jack Bandy","Problematic Machine Behavior: A Systematic Literature Review of Algorithm Audits ","https://arxiv.org/abs/2102.04256",NA,2021,"algorithm auditing, literature review, ethics, policy, critical algorithm studies, algorithmic bias, algorithmic authority, algorithmic accountability","systematic literature review that [synthesizes prior work and strategize future research in algorithmic audit] ... following PRISMA guidelines in a review of over 500 English articles that yielded 62 algorithm audit studies.","OAT - Meta-Commentary","Meta-Commentary",422
"295","Other non-ACM","Meta-Commentary","Wu et al ","How medical AI devices are evaluated: limitations and recommendations from an analysis of FDA approvals","https://www.nature.com/articles/s41591-021-01312-x",NA,2021,"Business and industry, Medical research, Predictive medicine","""we have created an annotated database of FDA-approved medical AI devices and systematically analyzed how these devices were evaluated before approval. Additionally, we have conducted a case study of pneumothorax-triage devices and found that evaluating deep-learning models at a single site alone, which is often done, can mask weaknesses in the models and lead to worse performance across sites","OAT - Meta-Commentary","Meta-Commentary",426
"296","Other non-ACM","Case Study","Liu et al ","The medical algorithmic audit","https://www.sciencedirect.com/science/article/pii/S2589750022000036",NA,2022,NA,"Artificial intelligence systems for health care, like any other medical device, have the potential to fail. However, specific qualities of artificial intelligence systems, such as the tendency to learn spurious correlates in training data, poor generalisability to new deployment settings, and a paucity of reliable explainability mechanisms, mean they can yield unpredictable errors that might be entirely missed without proactive investigation. We propose a medical algorithmic audit framework that guides the auditor through a process of considering potential algorithmic errors in the context of a clinical task, mapping the components that might contribute to the occurrence of errors, and anticipating their potential consequences. We suggest several approaches for testing algorithmic errors, including exploratory error analysis, subgroup testing, and adversarial testing, and provide examples from our own work and previous studies. The medical algorithmic audit is a tool that can be used to better understand the weaknesses of an artificial intelligence system and put in place mechanisms to mitigate their impact. We propose that safety monitoring and medical algorithmic auditing should be a joint responsibility between users and developers, and encourage the use of feedback mechanisms between these groups to promote learning and maintain safe deployment of artificial intelligence systems.","OAT - Algorithm/Model Audit Case Studies","Case Study",276
"297","Other non-ACM","Case Study","Howard et al","Evaluating Proposed Fairness Models for Face Recognition Algorithms","https://arxiv.org/abs/2203.05051",NA,2022,"Datasets, Face Recognition, Fairness, Socio-technical Policy","The development of face recognition algorithms by academic and commercial organizations is growing rapidly due to the onset of deep learning and the widespread availability of training data. Though tests of face recognition algorithm performance indicate yearly performance gains, error rates for many of these systems differ based on the demographic composition of the test set. These ""demographic differentials"" in algorithm performance can contribute to unequal or unfair outcomes for certain groups of people, raising concerns with increased worldwide adoption of face recognition systems. Consequently, regulatory bodies in both the United States and Europe have proposed new rules requiring audits of biometric systems for ""discriminatory impacts"" (European Union Artificial Intelligence Act) and ""fairness"" (U.S. Federal Trade Commission). However, no standard for measuring fairness in biometric systems yet exists. This paper characterizes two proposed measures of face recognition algorithm fairness (fairness measures) from scientists in the U.S. and Europe. We find that both proposed methods are challenging to interpret when applied to disaggregated face recognition error rates as they are commonly experienced in practice. To address this, we propose a set of interpretability criteria, termed the Functional Fairness Measure Criteria (FFMC), that outlines a set of properties desirable in a face recognition algorithm fairness measure. We further develop a new fairness measure, the Gini Aggregation Rate for Biometric Equitability (GARBE), and show how, in conjunction with the Pareto optimization, this measure can be used to select among alternative algorithms based on the accuracy/fairness trade-space. Finally, we have open-sourced our dataset of machine-readable, demographically disaggregated error rates. We believe this is currently the largest open-source dataset of its kind.","OAT - Algorithm/Model Audit Case Studies","Case Study",277
"298","Other non-ACM","Case Study","Ivoline C. Ngong, Krystal Maughan, Joseph P. Near","Prediction Sensitivity: Continual Audit of Counterfactual Fairness in Deployed Classifiers","https://arxiv.org/pdf/2202.04504.pdf",NA,2022,NA,"As AI-based systems increasingly impact many areas of our lives, auditing these systems for fairness is an increasingly high-stakes problem. Traditional group fairness metrics can miss discrimination against individuals and are difficult to apply after deployment. Counterfactual fairness describes an individualized notion of fairness but is even more challenging to evaluate after deployment. We present prediction sensitivity, an approach for continual audit of counterfactual fairness in deployed classifiers. Prediction sensitivity helps answer the question: would this prediction have been different, if this individual had belonged to a different demographic group -- for every prediction made by the deployed model. Prediction sensitivity can leverage correlations between protected status and other features and does not require protected status information at prediction time. Our empirical results demonstrate that prediction sensitivity is effective for detecting violations of counterfactual fairness.","OAT - Algorithm/Model Audit Case Studies","Case Study",283
"299","Other non-ACM","Case Study","Kollnig et al ","Goodbye Tracking? Impact of iOS App Tracking Transparency and Privacy Labels","https://arxiv.org/pdf/2204.03556.pdf",NA,2022,"mobile apps, Apple, iOS, data protection, privacy, platform policies, gatekeeper power, App Tracking Transparency, Privacy Nutrition Labels","Tracking is a highly privacy-invasive data collection practice that has been ubiquitous in mobile apps for many years due to its role in supporting advertising-based revenue models. In response, Apple introduced two significant changes with iOS 14: App Tracking Transparency (ATT), a mandatory opt-in system for enabling tracking on iOS, and Privacy Nutrition Labels, which disclose what kinds of data each app processes. So far, the impact of these changes on individual privacy and control has not been well understood. This paper addresses this gap by analysing two versions of 1,759 iOS apps from the UK App Store: one version from before iOS 14 and one that has been updated to comply with the new rules.
We find that Apple's new policies, as promised, prevent the collection of the Identifier for Advertisers (IDFA), an identifier for cross-app tracking. Smaller data brokers that engage in invasive data practices will now face higher challenges in tracking users - a positive development for privacy. However, the number of tracking libraries has roughly stayed the same in the studied apps. Many apps still collect device information that can be used to track users at a group level (cohort tracking) or identify individuals probabilistically (fingerprinting). We find real-world evidence of apps computing and agreeing on a fingerprinting-derived identifier through the use of server-side code, thereby violating Apple's policies. We find that Apple itself engages in some forms of tracking and exempts invasive data practices like first-party tracking and credit scoring. We also find that the new Privacy Nutrition Labels are sometimes inaccurate and misleading.
Overall, our findings suggest that, while tracking individual users is more difficult now, the changes reinforce existing market power of gatekeeper companies with access to large troves of first-party data and motivate a countermovement. ","OAT - Algorithm/Model Audit Case Studies","Case Study",285
"300","Other non-ACM","Case Study","Nina Markl","Language variation and algorithmic bias: understanding algorithmic bias in British English automatic speech recognition","https://dl.acm.org/doi/pdf/10.1145/3531146.3533117?casa_token=_Y48EbEjWxEAAAAA:QWCc0fpN5hn_OC8yeJq3DBOu0-Mm1nRK-_7IEQI9nlNdpdf3f2mi1De_0oOVhDtk8cP4w-yRyyIjWg",NA,2022,"algorithmic bias, speech and language technologies, language variation, speech recognition","All language is characterised by variation which language users employ to construct complex social identities and express social meaning. Like other machine learning technologies, speech and language technologies (re)produce structural oppression when they perform worse for marginalised language communities. Using knowledge and theories from sociolinguistics, I explore why commercial automatic speech recognition systems and other language technologies perform significantly worse for already marginalised populations, such as second-language speakers and speakers of stigmatised varieties of English in the British Isles. Situating language technologies within the broader scholarship around algorithmic bias, consider the allocative and representational harms they can cause even (and perhaps especially) in systems which do not exhibit predictive bias, narrowly defined as differential performance between groups. This raises the question whether addressing or “fixing” this “bias” is actually always equivalent to mitigating the harms algorithmic systems can cause, in particular to marginalised communities.","OAT - Algorithm/Model Audit Case Studies","Case Study",286
"301","Other non-ACM","Case Study","Emily Sullivan and Philippe Verreault-Julien","Long-term Dynamics of Fairness Intervention in Connection Recommender Systems","https://arxiv.org/abs/2203.16432",NA,2022,NA,"Recommender system fairness has been studied from the perspectives of a variety of stakeholders including content producers, the content itself and recipients of recommendations. Regardless of which type of stakeholders are considered, most works in this area assess the efficacy of fairness intervention by evaluating a single fixed fairness criterion through the lens of a one-shot, static setting. Yet recommender systems constitute dynamical systems with feedback loops from the recommendations to the underlying population distributions which could lead to unforeseen and adverse consequences if not taken into account. In this paper, we study a connection recommender system patterned after the systems employed by web-scale social networks and analyze the long-term effects of intervening on fairness in the recommendations. We find that, although seemingly fair in aggregate, common exposure and utility parity interventions fail to mitigate amplification of biases in the long term. We theoretically characterize how certain fairness interventions impact the bias amplification dynamics in a stylized Pólya urn model.","OAT - Algorithm/Model Audit Case Studies","Case Study",287
"302","Other non-ACM","Case Study","H Shimao et al 2022","Write It Like You See It: Detectable Differences in Clinical Notes By Race Lead To Differential Model Recommendations","https://arxiv.org/pdf/2205.03931.pdf",NA,2022,NA,"Clinical notes are becoming an increasingly important data source for machine learning (ML) applications in healthcare. Prior research has shown that deploying ML models can perpetuate existing biases against racial minorities, as bias can be implicitly embedded in data. In this study, we investigate the level of implicit race information available to ML models and human experts and the implications of model-detectable differences in clinical notes. Our work makes three key contributions. First, we find that models can identify patient self-reported race from clinical notes even when the notes are stripped of explicit indicators of race. Second, we determine that human experts are not able to accurately predict patient race from the same redacted clinical notes. Finally, we demonstrate the potential harm of this implicit information in a simulation study, and show that models trained on these race-redacted clinical notes can still perpetuate existing biases in clinical treatment decisions.","OAT - Algorithm/Model Audit Case Studies","Case Study",288
"303","Other non-ACM","Case Study",NA,"Understanding Decision Subjects’ Fairness Perceptions and Retention in AI-Based Decision Systems: A Long-Term View","https://doi.org/10.3390/make4020026",NA,2022,"AI explanation; AI fairness; trust; perception of fairness; AI ethics","AI-assisted decision-making that impacts individuals raises critical questions about transparency and fairness in artificial intelligence (AI). Much research has highlighted the reciprocal relationships between the transparency/explanation and fairness in AI-assisted decision-making. Thus, considering their impact on user trust or perceived fairness simultaneously benefits responsible use of socio-technical AI systems, but currently receives little attention. In this paper, we investigate the effects of AI explanations and fairness on human-AI trust and perceived fairness, respectively, in specific AI-based decision-making scenarios. A user study simulating AI-assisted decision-making in two health insurance and medical treatment decision-making scenarios provided important insights. Due to the global pandemic and restrictions thereof, the user studies were conducted as online surveys. From the participant’s trust perspective, fairness was found to affect user trust only under the condition of a low fairness level, with the low fairness level reducing user trust. However, adding explanations helped users increase their trust in AI-assisted decision-making. From the perspective of perceived fairness, our work found that low levels of introduced fairness decreased users’ perceptions of fairness, while high levels of introduced fairness increased users’ perceptions of fairness. The addition of explanations definitely increased the perception of fairness. Furthermore, we found that application scenarios influenced trust and perceptions of fairness. The results show that the use of AI explanations and fairness statements in AI applications is complex: we need to consider not only the type of explanations and the degree of fairness introduced, but also the scenarios in which AI-assisted decision-making is used.","OAT - Algorithm/Model Audit Case Studies","Case Study",289
"304","Other non-ACM","Case Study",NA,"How Cognitive Biases Affect XAI-assisted Decision-making: A Systematic Review","https://dl.acm.org/doi/10.1145/3514094.3534164",NA,2022,"Explainability, explainable AI, cognitive bias, human-centered AI, XAI.","The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to complex AI systems. Although it is usually considered an essentially technical field, effort has been made recently to better understand users' human explanation methods and cognitive constraints. Despite these advances, the community lacks a general vision of what and how cognitive biases affect explainability systems. To address this gap, we present a heuristic map which matches human cognitive biases with explainability techniques from the XAI literature, structured around XAI-aided decision-making. We identify four main ways cognitive biases affect or are affected by XAI systems: 1) cognitive biases affect how XAI methods are designed, 2) they can distort how XAI techniques are evaluated in user studies, 3) some cognitive biases can be successfully mitigated by XAI techniques, and, on the contrary, 4) some cognitive biases can be exacerbated by XAI techniques. We construct this heuristic map through the systematic review of 37 papers-drawn from a corpus of 285-that reveal cognitive biases in XAI systems, including the explainability method and the user and task types in which they arise. We use the findings from our review to structure directions for future XAI systems to better align with people's cognitive processes.","OAT - Algorithm/Model Audit Case Studies","Case Study",290
"305","Other non-ACM","Case Study",NA,"The Cost of Ethical AI Development for AI Startups","https://dl.acm.org/doi/10.1145/3514094.3534195",NA,2022,"artificial intelligence, ethics, data, startup, competition","Artificial Intelligence startups use training data as direct inputs in product development. These firms must balance numerous tradeoffs between ethical issues and data access without substantive guidance from regulators or existing judicial precedence. We survey these startups to determine what actions they have taken to address these ethical issues and the consequences of those actions. We find that 58% of these startups have established a set of AI principles. Startups with data-sharing relationships with high-technology firms or that have prior experience with privacy regulations are more likely to establish ethical AI principles and are more likely to take costly steps, like dropping training data or turning down business, to adhere to their ethical AI policies. Moreover, startups with ethical AI policies are more likely to invest in unconscious bias training, hire ethnic minorities and female programmers, seek expert advice, and search for more diverse training data. Potential costs associated with data-sharing relationships and the adherence to ethical policies may create tradeoffs between increased AI product competition and more ethical AI production.","OAT - Algorithm/Model Audit Case Studies","Case Study",291
"306","Other non-ACM","Case Study",NA,"Fairness in Agreement With European Values: An Interdisciplinary Perspective on AI Regulation","https://arxiv.org/abs/2207.01510",NA,2022,NA,"With increasing digitalization, Artificial Intelligence (AI) is becoming ubiquitous. AI-based systems to identify, optimize, automate, and scale solutions to complex economic and societal problems are being proposed and implemented. This has motivated regulation efforts, including the Proposal of an EU AI Act. This interdisciplinary position paper considers various concerns surrounding fairness and discrimination in AI, and discusses how AI regulations address them, focusing on (but not limited to) the Proposal. We first look at AI and fairness through the lenses of law, (AI) industry, sociotechnology, and (moral) philosophy, and present various perspectives. Then, we map these perspectives along three axes of interests: (i) Standardization vs. Localization, (ii) Utilitarianism vs. Egalitarianism, and (iii) Consequential vs. Deontological ethics which leads us to identify a pattern of common arguments and tensions between these axes. Positioning the discussion within the axes of interest and with a focus on reconciling the key tensions, we identify and propose the roles AI Regulation should take to make the endeavor of the AI Act a success in terms of AI fairness concerns.","OAT - Algorithm/Model Audit Case Studies","Case Study",292
"307","Other non-ACM","Case Study",NA,"Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics","https://arxiv.org/abs/2206.03390",NA,2022,"word embeddings, AI bias, gender bias, psycholinguistics, representation, masculine default","The statistical regularities in language corpora encode well-known social biases into word embeddings. Here, we focus on gender to provide a comprehensive analysis of group-based biases in widely-used static English word embeddings trained on internet corpora (GloVe 2014, fastText 2017). Using the Single-Category Word Embedding Association Test, we demonstrate the widespread prevalence of gender biases that also show differences in: (1) frequencies of words associated with men versus women; (b) part-of-speech tags in gender-associated words; (c) semantic categories in gender-associated words; and (d) valence, arousal, and dominance in gender-associated words.
First, in terms of word frequency: we find that, of the 1,000 most frequent words in the vocabulary, 77% are more associated with men than women, providing direct evidence of a masculine default in the everyday language of the English-speaking world. Second, turning to parts-of-speech: the top male-associated words are typically verbs (e.g., fight, overpower) while the top female-associated words are typically adjectives and adverbs (e.g., giving, emotionally). Gender biases in embeddings also permeate parts-of-speech. Third, for semantic categories: bottom-up, cluster analyses of the top 1,000 words associated with each gender. The top male-associated concepts include roles and domains of big tech, engineering, religion, sports, and violence; in contrast, the top female-associated concepts are less focused on roles, including, instead, female-specific slurs and sexual content, as well as appearance and kitchen terms. Fourth, using human ratings of word valence, arousal, and dominance from a ~20,000 word lexicon, we find that male-associated words are higher on arousal and dominance, while female-associated words are higher on valence. ","OAT - Algorithm/Model Audit Case Studies","Case Study",293
"308","Other non-ACM","Case Study",NA,"Ordinary people as moral heroes and foes: Digital role model narratives propagate social norms in China’s Social Credit System","https://dl.acm.org/doi/10.1145/3514094.3534180",NA,2022,"China, Social Credit System, moral education, narrative study","The Chinese Social Credit System (SCS) is a digital sociotechnical credit system that rewards and sanctions economic and social behaviors of individuals and companies. As a complex and transformative digital credit system, the SCS uses digital communication channels to inform the Chinese public about behaviors that lead to reward or sanction. Since 2017, the Chinese government has been publishing ""blameworthy"" and ""praiseworthy"" role model narratives of ordinary Chinese citizens on its central SCS information platform creditchina.gov.cn. Across many cultures, role model narratives are a known instrument to convey ""appropriate"" and ""inappropriate"" social norms. Using a directed content analysis methodology, we study the SCS-specific social norms embedded in 100 ""praiseworthy"" and 100 ""blameworthy"" role model narratives published on creditchina.gov.cn. ""Blameworthy"" role model narratives stress social norms associated with an ""immoral"" SCS identity label termed ""Lao Lai"" - a ""moral foe"" that fails to repay debt. SCS role model narratives familiarize Chinese society with SCS-specific measures such as digital surveillance, public shaming, and disproportionate punishment. Our study makes progress towards understanding how a state-run sociotechnical credit system combines digital tools with culturally familiar customs to propagate ""blameworthy"" and ""praiseworthy"" identities.","OAT - Algorithm/Model Audit Case Studies","Case Study",294
"309","Other non-ACM","Case Study","Basileal Imana, Aleksandra Korolova, John Heidemann","Auditing for Discrimination in Algorithms Delivering Job Ads","https://dl.acm.org/doi/pdf/10.1145/3442381.3450077",NA,2022,NA,"Ad platforms such as Facebook, Google and LinkedIn promise
value for advertisers through their targeted advertising. How-
ever, multiple studies have shown that ad delivery on such plat-
forms can be skewed by gender or race due to hidden algorith-
mic optimization by the platforms, even when not requested
by the advertisers. Building on prior work measuring skew
in ad delivery, we develop a new methodology for black-box
auditing of algorithms for discrimination in the delivery of job
advertisements. Our first contribution is to identify the distinc-
tion between skew in ad delivery due to protected categories
such as gender or race, from skew due to differences in qualifi-
cation among people in the targeted audience. This distinction
is important in U.S. law, where ads may be targeted based
on qualifications, but not on protected categories. Second, we
develop an auditing methodology that distinguishes between
skew explainable by differences in qualifications from other
factors, such as the ad platform’s optimization for engagement
or training its algorithms on biased data. Our method con-
trols for job qualification by comparing ad delivery of two
concurrent ads for similar jobs, but for a pair of companies
with different de facto gender distributions of employees. We
describe the careful statistical tests that establish evidence
of non-qualification skew in the results. Third, we apply our
proposed methodology to two prominent targeted advertising
platforms for job ads: Facebook and LinkedIn. We confirm
skew by gender in ad delivery on Facebook, and show that
it cannot be justified by differences in qualifications. We fail
to find skew in ad delivery on LinkedIn. Finally, we suggest
improvements to ad platform practices that could make ex-
ternal auditing of their algorithms in the public interest more
feasible and accurate.","OAT - Algorithm/Model Audit Case Studies","Case Study",314
"310","Other non-ACM","Case Study","Yunhe Fang, Chirag Shah","Has CEO Gender Bias Really Been Fixed? Adversarial Attacking and Improving Gender Fairness in Image Search","https://ojs.aaai.org/index.php/AAAI/article/view/21445",NA,2022,"AI For Social Impact","Gender bias is one of the most common and well-studied demographic biases in information retrieval, and in general in AI systems. After discovering and reporting that gender bias for certain professions could change searchers' worldviews, mainstreaming image search engines, such as Google, quickly took action to correct and fix such a bias. However, given the nature of these systems, viz., being opaque, it is unclear if they addressed unequal gender representation and gender stereotypes in image search results systematically and in a sustainable way. In this paper, we propose adversarial attack queries composed of professions and countries (e.g., 'CEO United States') to investigate whether gender bias is thoroughly mitigated by image search engines. Our experiments on Google, Baidu, Naver, and Yandex Image Search show that the proposed attack can trigger high levels of gender bias in image search results very effectively. To defend against such attacks and mitigate gender bias, we design and implement three novel re-ranking algorithms -- epsilon-greedy algorithm, relevance-aware swapping algorithm, and fairness-greedy algorithm, to re-rank returned images for given image queries. Experiments on both simulated (three typical gender distributions) and real-world datasets demonstrate the proposed algorithms can mitigate gender bias effectively.","OAT - Algorithm/Model Audit Case Studies","Case Study",322
"311","Other non-ACM","Case Study","Anjali Gupta, Rahul Yadav, Ashish Nair, Abhijnan Chakraborty, Sayan Ranu, Amitabha Bagchi","FairFoody: Bringing In Fairness in Food Delivery","https://ojs.aaai.org/index.php/AAAI/article/view/21447",NA,2022,"AI For Social Impact","Along with the rapid growth and rise to prominence of food delivery platforms, concerns have also risen about the terms of employment of the ``gig workers'' underpinning this growth. Our analysis on data derived from a real-world food delivery platform across three large cities from India show that there is significant inequality in the money delivery agents earn. In this paper, we formulate the problem of fair income distribution among agents while also ensuring timely food delivery. We establish that the problem is not only NP-hard but also inapproximable in polynomial time. We overcome this computational bottleneck through a novel matching algorithm called FairFoody. Extensive experiments over real-world food delivery datasets show FairFoody imparts up to 10 times improvement in equitable income distribution when compared to baseline strategies, while also ensuring minimal impact on customer experience.","OAT - Algorithm/Model Audit Case Studies","Case Study",323
"312","Other non-ACM","Case Study","Afsaneh Razi et al","Instagram Data Donation: A Case Study on Collecting Ecologically Valid Social Media Data for the Purpose of Adolescent Online Risk Detection","https://dl.acm.org/doi/10.1145/3491101.3503569","CHI",2022,"Dataset, Social Media Data, Data Collection, online safety, Privacy, Adolescents, Teens","In this work, we present a case study on an Instagram Data Donation (IGDD) project, which is a user study and web-based platform for youth (ages 13-21) to donate and annotate their Instagram data with the goal of improving adolescent online safety. We employed human-centered design principles to create an ecologically valid dataset that will be utilized to provide insights from teens’ private social media interactions and train machine learning models to detect online risks. Our work provides practical insights and implications for Human-Computer Interaction (HCI) researchers that collect and study social media data to address sensitive problems relating to societal good.","OAT - Algorithm/Model Audit Case Studies","Case Study",324
"313","Other non-ACM","Case Study","Claude P. R. Heath and Lizzie Coles-Kemp","Drawing Out the Everyday Hyper-[In]Securities of Digital Identity","https://dl.acm.org/doi/abs/10.1145/3491102.3501961",NA,2022,"digital inclusion, security technologies, world building, drawing","In a study of everyday digital identity, a set of primary drawings were made by researchers in online focus group settings as a way to capture our participants’ spoken narratives of hyper-[in]security in the usages of digital identity. In a second stage of work, key extracts from the drawings were collaged using the method described in the paper, allowing an exploratory qualitative cartography of hyper-[in]security to be constructed. These secondary collages group the [in]securities thematically without losing essential contextual information. Samples of our data are given, to illustrate the contribution of the method to experience-centred design, with special reference to security from the perspective of marginalised and underserved communities. We discuss our method as a step towards multidimensional cognitive mapping of the salient features of our participants’ narratives about hyper-[in]security, potentially paving the way for further world building explorations of digital identity futures.","OAT - Algorithm/Model Audit Case Studies","Case Study",325
"314","Other non-ACM","Case Study","Yao Lyu and John Carroll","Cultural Influences on Chinese Citizens' Adoption of Digital Contact Tracing: A Human Infrastructure Perspective","https://dl.acm.org/doi/abs/10.1145/3491102.3517572",NA,2022,"Culture Studies, Infrastructure, Infrastructuring, Human Infrastructure, Crisis Informatics, Contact Tracing, COVID-19 Pandemic","Digital contact tracing is an ICT approach for controlling public health crises. It identifies users’ risk of infection based on their healthcare and travel information. In the COVID-19 pandemic, many countries implemented digital contact tracing to contain the coronavirus outbreak. However, the adoption rates vary significantly across different countries. In this study, we investigate Chinese people’s adoption of digital contact tracing. We aim at finding the influence of Chinese culture on people’s attitudes and behaviors toward the technology. We interviewed 26 Chinese participants and used thematic analysis to interpret the data. Our findings showed that Chinese culture shaped citizens’ interactions with the digital contact tracing at multiple levels; driven by the culture, Chinese citizens accepted digital contact tracing and contributed to making digital contact tracing a socio-technical infrastructure of people’s daily lives. We also discuss such cultural influences with the growing literature of human infrastructure and crisis informatics.","OAT - Algorithm/Model Audit Case Studies","Case Study",326
"315","Other non-ACM","Case Study","Kopo Ramokapane et al","Privacy Design Strategies for Home Energy Management Systems (HEMS)","https://programs.sigchi.org/chi/2022/index/content/72190",NA,2022,NA,"Home energy management systems (HEMS) offer control and the ability to manage energy, generating and collecting energy consumption data at the most detailed level. However, data at this level poses various privacy concerns, including, for instance, profiling consumer behaviors and large-scale surveillance. The question of how utility providers can get value from such data without infringing consumers' privacy has remained under-investigated. We address this gap by exploring the pro-sharing attitudes and privacy perceptions of 30 HEMS users and non-users through an interview study. While participants are concerned about data misuse and stigmatization, our analysis also reveals that incentives, altruism, trust, security and privacy, transparency and accountability encourage data sharing. From this analysis, we derive privacy design strategies for HEMS that can both improve privacy and engender adoption.","OAT - Algorithm/Model Audit Case Studies","Case Study",327
"316","Other non-ACM","Case Study","Alex Bowyer et al","Human-GDPR Interaction: Practical Experiences of Accessing Personal Data","https://dl.acm.org/doi/abs/10.1145/3491102.3501947",NA,2022,"privacy, GDPR, information access, personal data, open data, data portability, human-data interaction, HDI, user empowerment, data collection, digital rights, trust, information literacy, participatory action research","In our data-centric world, most services rely on collecting and using personal data. The EU's General Data Protection Regulation (GDPR) aims to enhance individuals’ control over their data, but its practical impact is not well understood. We present a 10-participant study, where each participant filed 4-5 data access requests. Through interviews accompanying these requests and discussions scrutinising returned data, it appears that GDPR falls short of its goals due to non-compliance and low-quality responses. Participants found their hopes to understand providers’ data practices or harness their own data unmet. This causes increased distrust without any subjective improvement in power, although more transparent providers do earn greater trust. We propose designing more effective, data-inclusive and open policies and data access systems to improve both customer relations and individual agency, and also that wider public use of GDPR rights could help with delivering accountability and motivating providers to improve data practices.","OAT - Algorithm/Model Audit Case Studies","Case Study",328
"317","Other non-ACM","Case Study","Jay Rainey et al","Exploring the Role of Paradata in Digitally Supported Qualitative Co-Research","https://dl.acm.org/doi/abs/10.1145/3491102.3502103",NA,2022,"Paradata, Ethnography, Citizen Research, Co-Research, Qualitative Practices, Research Methods","Academics and community organisations are increasingly adopting co-research practices where participants contribute to qualitative data collection, analysis, and dissemination. These qualitative practices can often lack transparency that can present a problem for stakeholders (such as funding agencies) who seek evidence of the rigour and accountability in these decision-making processes. When qualitative research is done digitally, paradata is available as interaction logs that reveal the underlying processes, such as the time spent engaging with different segments of an interview. In practice, paradata is seldom used to examine the decisions associated with undertaking qualitative research. This paper explores the role of paradata arising from a four-month engagement with a community-led charity that used a digital platform to support their qualitative co-research project. Through observations of platform use and reflective post-deployment interviews, our findings highlight examples of paradata generated through digital tools in qualitative research, e.g., listening coverage, engagement rate, thematic maps and data discards. From this, we contribute a conceptualisation of paradata and discuss its role in qualitative research to improve process transparency, enhance data sharing, and to create feedback loops with research participants.","OAT - Algorithm/Model Audit Case Studies","Case Study",329
"318","Other non-ACM","Case Study","Jing Wei et al","What Could Possibly Go Wrong When Interacting with Proactive Smart Speakers? A Case Study Using an ESM Application","https://dl.acm.org/doi/10.1145/3491102.3517432",NA,2022,"Voice user interface, smart speakers, voice assistants, Google Home, user experience, interaction error","Voice user interfaces (VUIs) have made their way into people’s daily lives, from voice assistants to smart speakers. Although VUIs typically just react to direct user commands, increasingly, they incorporate elements of proactive behaviors. In particular, proactive smart speakers have the potential for many applications, ranging from healthcare to entertainment; however, their usability in everyday life is subject to interaction errors. To systematically investigate the nature of errors, we designed a voice-based Experience Sampling Method (ESM) application to run on proactive speakers. We captured 1,213 user interactions in a 3-week field deployment in 13 participants’ homes. Through auxiliary audio recordings and logs, we identify substantial interaction errors and strategies that users apply to overcome those errors. We further analyze the interaction timings and provide insights into the time cost of errors. We find that, even for answering simple ESMs, interaction errors occur frequently and can hamper the usability of proactive speakers and user experience. Our work also identifies multiple facets of VUIs that can be improved in terms of the timing of speech.","OAT - Algorithm/Model Audit Case Studies","Case Study",330
"319","Other non-ACM","Case Study","Vince Bartle et al","“A Second Voice”: Investigating Opportunities and Challenges for Interactive Voice Assistants to Support Home Health Aides","https://dl.acm.org/doi/abs/10.1145/3491102.3517683",NA,2022,"Voice assistant, AI, intelligent agent, home health care, home health aides, community health, future of work.","Home health aides are a vulnerable group of frontline caregivers who provide personal and medically-oriented care in patients’ homes. Their work is difficult and unpredictable, involving a mix of physical and emotional labor as they adapt to patients’ changing needs. Our paper presents an exploratory, qualitative study with 32 participants, that investigates design opportunities for Interactive Voice Assistants (IVAs) to support aides’ essential care work. We explore challenges and opportunities for IVAs to (1) fill gaps in aides’ access to information and care coordination, (2) assist with decision making and task completion, (3) advocate on behalf of aides, and (4) provide emotional support. We then discuss key implications of our work, including how materiality may impact perceived ownership and usage of IVAs, the need to carefully consider tensions around surveillance, accountability, data collection, and reporting, and the challenges of centering aides as essential workers in complex home health care contexts.","OAT - Algorithm/Model Audit Case Studies","Case Study",331
"320","Other non-ACM","Case Study","Maximiliane Windl et al","Automating Contextual Privacy Policies: Design and Evaluation of a Production Tool for Digital Consumer Privacy Awareness","https://dl.acm.org/doi/abs/10.1145/3491102.3517688",NA,2022,"privacy, privacy policies, online services, contextual privacy","Users avoid engaging with privacy policies because they are lengthy and complex, making it challenging to retrieve relevant information. In response, research proposed contextual privacy policies (CPPs) that embed relevant privacy information directly into their affiliated contexts. To date, CPPs are limited to concept showcases. This work evolves CPPs into a production tool that automatically extracts and displays concise policy information. We first evaluated the technical functionality on the US’s 500 most visited websites with 59 participants. Based on our results, we further revised the tool to deploy it in the wild with 11 participants over ten days. We found that our tool is effective at embedding CPP information on websites. Moreover, we found that the tool’s usage led to more reflective privacy behavior, making CPPs powerful in helping users understand the consequences of their online activities. We contribute design implications around CPP presentation to inform future systems design.","OAT - Algorithm/Model Audit Case Studies","Case Study",332
"321","Other non-ACM","Case Study","Emily Tseng et al","Care Infrastructures for Digital Security in Intimate Partner Violence","https://dl.acm.org/doi/abs/10.1145/3491102.3502038",NA,2022,"intimate partner violence, gender-based violence, computersecurity and privacy, care","Survivors of intimate partner violence (IPV) face complex threats to their digital privacy and security. Prior work has established protocols for directly helping them mitigate these harms; however, there remains a need for flexible and pluralistic systems that can support survivors’ long-term needs. This paper describes the design and development of sociotechnical infrastructure that incorporates feminist notions of care to connect IPV survivors experiencing technology abuse with volunteer computer security consultants. We present findings from a mixed methods study that draws on data from an 8-month, real-world deployment, as well as interviews with 7 volunteer technology consultants and 18 IPV professionals. Our findings illuminate emergent challenges in safely and adaptively providing computer security advice as care. We discuss implications of these findings for feminist approaches to computer security and privacy, and provide broader lessons for interventions that aim to directly assist at-risk and marginalized people experiencing digital insecurity.","OAT - Algorithm/Model Audit Case Studies","Case Study",333
"322","Other non-ACM","Case Study",NA,"Hey Alexa, Who Am I Talking to?: Analyzing Users’ Perception and Awareness Regarding Third-party Alexa Skills","https://dl.acm.org/doi/abs/10.1145/3491102.3517510",NA,2022,"Voice assistant; Third-party skills; Security indicators","The Amazon Alexa voice assistant provides convenience through automation and control of smart home appliances using voice commands. Amazon allows third-party applications known as skills to run on top of Alexa to further extend Alexa’s capability. However, as multiple skills can share the same invocation phrase and request access to sensitive user data, growing security and privacy concerns surround third-party skills. In this paper, we study the availability and effectiveness of existing security indicators or a lack thereof to help users properly comprehend the risk of interacting with different types of skills. We conduct an interactive user study (inviting active users of Amazon Alexa) where participants listen to and interact with real-world skills using the official Alexa app. We find that most participants fail to identify the skill developer correctly (i.e., they assume Amazon also develops the third-party skills) and cannot correctly determine which skills will be automatically activated through the voice interface. We also propose and evaluate a few voice-based skill type indicators, showcasing how users would benefit from such voice-based indicators.","OAT - Algorithm/Model Audit Case Studies","Case Study",334
"323","Other non-ACM","Case Study","Dan Calacci and Alex Pentland","Bargaining With the Black Black-Box: Designing and Deploying Worker-Centric Tools to Audit Algorithmic Management","https://dl.acm.org/doi/pdf/10.1145/3570601","CSCW",2022,"Platform Work; Organizing; Labor; Data Advocacy","The increasing prevalence of large-scale labor aggregation platforms, worker analytics, and algorithmic decision-making by management raises the question of whether workers can use similar technologies to advocate for their own goals. Yet, there are inherent challenges in building worker-centric tools that collect, aggregate, and share data in responsible and ethical ways. In this paper, we present the design and deployment of the Shipt Calculator, a tool developed in collaboration with non-profit worker groups that allows app-based delivery workers to track and share aggregate data about their pay, increasing wage transparency. We first discuss the design challenges inherent to building worker-centric technologies, particularly for informally organized workers, and ground our discussion in the history of worker inquiry and co-research. We then describe some principles from this history and our own lessons in designing the Calculator that can be applied by future researchers and advocates seeking to build technical tools for organizing campaigns. Finally, we share the results of using the Calculator to audit an app's shift to a black-box pay model using data contributed by 140 workers in the Summer of 2020, finding that although the average pay per-order increased under the new payment model, almost half of workers experienced an unannounced pay cut during the shift, and many workers worked shifts that paid under their state's minimum wage. Finally, we discuss how tools like the Calculator demonstrate the important role that aggregate worker data, and a new Digital Workerism, can serve in creating and maintaining a more balanced platform economy.","OAT - Algorithm/Model Audit Case Studies","Case Study",338
"324","Other non-ACM","Case Study","NUHA ALBADI, MARAM KURDI and SHIVAKANT MISHRA","Deradicalizing YouTube: Characterization, Detection, and Personalization of Religiously Intolerant Arabic Videos","https://arxiv.org/pdf/2207.00111.pdf","CSCW",2022,"hate speech, Islamist radicalization, detection, algorithmic audit, radicalization audit, YouTube recommendations, Arab HCI","Growing evidence suggests that YouTube's recommendation algorithm plays a role in online radicalization via surfacing extreme content. Radical Islamist groups, in particular, have been profiting from the global appeal of YouTube to disseminate hate and jihadist propaganda. In this quantitative, data-driven study, we investigate the prevalence of religiously intolerant Arabic YouTube videos, the tendency of the platform to recommend such videos, and how these recommendations are affected by demographics and watch history. Based on our deep learning classifier developed to detect hateful videos and a large-scale dataset of over 350K videos, we find that Arabic videos targeting religious minorities are particularly prevalent in search results (30%) and first-level recommendations (21%), and that 15% of overall captured recommendations point to hateful videos. Our personalized audit experiments suggest that gender and religious identity can substantially affect the extent of exposure to hateful content. Our results contribute vital insights into the phenomenon of online radicalization and facilitate curbing online harmful content.","OAT - Algorithm/Model Audit Case Studies","Case Study",339
"325","Other non-ACM","Case Study","MICHELLE S. LAM,  MITCHELL L. GORDON,DANAË METAXA, JEFFREY T. HANCOCK, JAMES A. LANDAY,  MICHAEL S. BERNSTEIN","End-User Audits: A System Empowering Communities to Lead Large-Scale Investigations of Harmful Algorithmic Behavior","https://dl.acm.org/doi/10.1145/3555625","CSCW",2022,"algorithm auditing, algorithmic fairness, machine learning, human-centered AI, interactive visualization","Because algorithm audits are conducted by technical experts, audits are necessarily limited to the hypotheses that experts think to test. End users hold the promise to expand this purview, as they inhabit spaces and witness algorithmic impacts that auditors do not. In pursuit of this goal, we propose end-user audits-system-scale audits led by non-technical users-and present an approach that scaffolds end users in hypothesis generation, evidence identification, and results communication. Today, performing a system-scale audit requires substantial user effort to label thousands of system outputs, so we introduce a collaborative filtering technique that leverages the algorithmic system's own disaggregated training data to project from a small number of end user labels onto the full test set. Our end-user auditing tool, IndieLabel, employs these predicted labels so that users can rapidly explore where their opinions diverge from the algorithmic system's outputs. By highlighting topic areas where the system is under-performing for the user and surfacing sets of likely error cases, the tool guides the user in authoring an audit report. In an evaluation of end-user audits on a popular comment toxicity model with 17 non-technical participants, participants both replicated issues that formal audits had previously identified and also raised previously underreported issues such as under-flagging on veiled forms of hate that perpetuate stigma and over-flagging of slurs that have been reclaimed by marginalized communities.","OAT - Algorithm/Model Audit Case Studies","Case Study",340
"326","Other non-ACM","Case Study","Arun Dunna et al","Paying Attention to the Algorithm Behind the Curtain Bringing Transparency to YouTube’s Demonetization Algorithms","https://dl.acm.org/doi/10.1145/3555209","CSCW",2022,NA,"YouTube has long been a top-choice destination for independent video content creators to share their work. A large part of YouTube's appeal is owed to its practice of sharing advertising revenue with qualifying content creators through the YouTube Partner Program (YPP). In recent years, changes to the monetization policies and the introduction of algorithmic systems for making monetization decisions have been a source of controversy and tension between content creators and the platform. There have been numerous accusations suggesting that the underlying monetization algorithms engage in preferential treatment of larger channels and effectively censor minority voices by demonetizing their content.

In this paper, we conduct a measurement of the YouTube monetization algorithms. We begin by measuring the incidence rates of different monetization decisions and the time taken to reach them. Next, we analyze the relationships between video content, channel popularity and these decisions. Finally, we explore the relationship between demonetization and a channel's view growth rate. Taken all together, our work suggests that demonetization after a video is publicly listed is not a common occurrence, the characteristics of the process are associated with channel size and (in unexplainable ways) video topic, and demonetization appears to have a harsh influence on the growth rate of smaller channels. We also highlight the challenges associated with conducting large-scale algorithm audits such as ours and make an argument for more transparency in algorithmic decision-making.","OAT - Algorithm/Model Audit Case Studies","Case Study",341
"327","Other non-ACM","Case Study","MATTHEW GROH, CALEB HARRIS, ROXANA DANESHJOU,OMAR BADRI, ARASH KOOCHEK","Towards Transparency in Dermatology Image Datasets with Skin Tone Annotations by Experts, Crowds, and an Algorithm","https://www.researchgate.net/publication/361842897_Towards_Transparency_in_Dermatology_Image_Datasets_with_Skin_Tone_Annotations_by_Experts_Crowds_and_an_Algorithm","CSCW",2022,"crowdsourcing, articial intelligence, healthcare, fairness, accountability, transparency","While artificial intelligence (AI) holds promise for supporting healthcare providers and improving the accuracy of medical diagnoses, a lack of transparency in the composition of datasets exposes AI models to the possibility of unintentional and avoidable mistakes. In particular, public and private image datasets of dermatological conditions rarely include information on skin color. As a start towards increasing transparency, AI researchers have appropriated the use of the Fitzpatrick skin type (FST) from a measure of patient photosensitivity to a measure for estimating skin tone in algorithmic audits of computer vision applications including facial recognition and dermatology diagnosis. In order to understand the variability of estimated FST annotations on images, we compare several FST annotation methods on a diverse set of 460 images of skin conditions from both textbooks and online dermatology atlases. We find the inter-rater reliability between three board-certified dermatologists is comparable to the inter-rater reliability between the board-certified dermatologists and two crowdsourcing methods. In contrast, we find that the Individual Typology Angle converted to FST (ITA-FST) method produces annotations that are significantly less correlated with the experts' annotations than the experts' annotations are correlated with each other. These results demonstrate that algorithms based on ITA-FST are not reliable for annotating large-scale image datasets, but human-centered, crowd-based protocols can reliably add skin type transparency to dermatology datasets. Furthermore, we introduce the concept of d​y​","OAT - Algorithm/Model Audit Case Studies","Case Study",342
"328","Other non-ACM","Case Study","AAFAQ SABIR, EVAN LAFONTAINE, ANUPAM DAS","Analyzing the Impact and Accuracy of Facebook Activity on Facebook’s Ad-Interest Inference Process","https://dl.acm.org/doi/abs/10.1145/3512923","CSCW",2022,"Facebook Ads; Targeted Ads; Causal Inference","Social media platforms like Facebook have become increasingly popular for serving targeted ads to their users. This has led to increased privacy concerns due to the lack of transparency regarding how ads are matched against each user profile. Facebook infers user interests through their activities and targets ads based on those interests. Although Facebook provides explanations for why a particular interest is inferred about a user, there is still a gap in understanding what activities lead to interest inferences and the extent to which the sentiment or context of activities is considered in inferring interests.

To obtain insights into how Facebook generates interests from a user's Facebook activities, we performed controlled experiments by creating new accounts and systematically executing numerous planned activities. This enabled us to make causal inferences about activities that lead to generating specific interests, many of which were not representative of actual user preferences. We also evaluated which activities resulted in interests and found that very naive activities, such as only viewing/scrolling through a page, lead to an interest inference. We found 33.22% of the inferred interests were inaccurate or irrelevant. We further evaluated the interest inference explanations provided by Facebook and found that these explanations were too generalized and, at times, misleading. To understand if our findings hold for a large and diverse sample, we conducted a user study where we recruited 146 participants (through Amazon Mechanical Turk) from different regions of the world to evaluate the accuracy of interests inferred by Facebook. We developed a browser extension to extract data from their own Facebook accounts and ask questions based on such data. Our participants reported a similar range (29%) of inaccuracy as observed in our controlled experiments. We also found that most of our participants were unaware of the availability of Facebook's ad preference manager, interest inference process, and even interest explanations.","OAT - Algorithm/Model Audit Case Studies","Case Study",343
"329","Other non-ACM","Case Study","Hassan Iqbal, Usman Mahmood Khan, Hassan Ali Khan, Muhammad Shahzad","A Peek into the Political Biases in Email Spam Filtering Algorithms During US Election 2020","https://arxiv.org/pdf/2203.16743.pdf","WWW",2022,"US Elections, Emails, Spam, Bias, Political Bias, Algorithm Bias","Email services use spam filtering algorithms (SFAs) to filter emails that are unwanted by the user. However, at times, the emails perceived by an SFA as unwanted may be important to the user. Such incorrect decisions can have significant implications if SFAs treat emails of user interest as spam on a large scale. This is particularly important during national elections. To study whether the SFAs of popular email services have any biases in treating the campaign emails, we conducted a large-scale study of the campaign emails of the US elections 2020 by subscribing to a large number of Presidential, Senate, and House candidates using over a hundred email accounts on Gmail, Outlook, and Yahoo. We analyzed the biases in the SFAs towards the left and the right candidates and further studied the impact of the interactions (such as reading or marking emails as spam) of email recipients on these biases. We observed that the SFAs of different email services indeed exhibit biases towards different political affiliations. We present this and several other important observations in this paper.","OAT - Algorithm/Model Audit Case Studies","Case Study",352
"330","Other non-ACM","Meta-Commentary","Sloane et al ","A Silicon Valley love triangle: Hiring algorithms, pseudo-science, and the quest for auditability","https://www.sciencedirect.com/science/article/pii/S2666389921003081",NA,2022,NA,"This paper outlines a socio-technical approach to audit automated hiring tools. It introduces a matrix that provides a method for inspecting the assumptions that underpin a system and how they are operationalized technically. These assumptions often rest on contradictory or pseudo-scientific theories about job applicants. We offer this matrix to facilitate holistic audits that go beyond technical performance. ""This article contends that audits and assessments of hiring ADSs cannot be limited merely to the degree to which they promote demographic parity within the hiring process but, rather, must also contend with claims that such ADSs can reveal aptitude, future performance, and cultural fit to promote equity and accountability across the entire hiring ecosystem.""","OAT - Critique/Background","Critique",364
"331","Other non-ACM","Meta-Commentary","Ben Gansky, Sean McDonald","CounterFAccTual: How FAccT Undermines Its Organizing Principles","https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533241",NA,2022,"metadata maximalism, algorithmic realism, failures of FAccT, accountability infrastructures","""What these proposals have in common is a focus on the stages of development that occur prior to any meaningful contact with or deployment into real-world contexts of use at scale, and the inequities that arise as a result. As a result, metadata maximalism forces itself into a position in which it produces documentation of how various interests intersect in the production of ML datasets, models, and services (for instance, interests in proportionate representation and culturally sensitive labels/classifications), without ever directly involving the participation of stakeholders who actually hold these interests.""","OAT - Critique/Background","Critique",365
"332","Other non-ACM","Data Audit","K. Yang, J. Yau, L. Fei-Fei, J. Deng, and O. Russakovsky","A study of face obfuscation in imagenet","https://arxiv.org/abs/2103.06191",NA,2022,NA,"Face obfuscation (blurring, mosaicing, etc.) has been shown to be effective for privacy protection; nevertheless, object recognition research typically assumes access to complete, unobfuscated images. In this paper, we explore the effects of face obfuscation on the popular ImageNet challenge visual recognition benchmark. Most categories in the ImageNet challenge are not people categories; however, many incidental people appear in the images, and their privacy is a concern. We first annotate faces in the dataset. Then we demonstrate that face obfuscation has minimal impact on the accuracy of recognition models. Concretely, we benchmark multiple deep neural networks on obfuscated images and observe that the overall recognition accuracy drops only slightly (<= 1.0%). Further, we experiment with transfer learning to 4 downstream tasks (object recognition, scene recognition, face attribute classification, and object detection) and show that features learned on obfuscated images are equally transferable. Our work demonstrates the feasibility of privacy-aware visual recognition, improves the highly-used ImageNet challenge benchmark, and suggests an important path for future visual datasets. Data and code are available at this https URL.","OAT - Dataset audit papers","Data Audit",383
"333","Other non-ACM","Data Audit","Adriano Barbosa-Silva, Simon Ott, Kathrin Blagec, Jan Brauner, Matthias Samwald","Mapping global dynamics of benchmark creation and saturation in artificial intelligence","https://arxiv.org/abs/2203.04592",NA,2022,NA,"Benchmarks are crucial to measuring and steering progress in artificial intelligence (AI). However, recent studies raised concerns over the state of AI benchmarking, reporting issues such as benchmark overfitting, benchmark saturation and increasing centralization of benchmark dataset creation. To facilitate monitoring of the health of the AI benchmarking ecosystem, we introduce methodologies for creating condensed maps of the global dynamics of benchmark creation and saturation. We curated data for 3765 benchmarks covering the entire domains of computer vision and natural language processing, and show that a large fraction of benchmarks quickly trended towards near-saturation, that many benchmarks fail to find widespread utilization, and that benchmark performance gains for different AI tasks were prone to unforeseen bursts. We analyze attributes associated with benchmark popularity, and conclude that future benchmarks should emphasize versatility, breadth and real-world utility.","OAT - Dataset audit papers","Data Audit",394
"334","Other non-ACM","Data Audit","Jacqueline A. Penn,  Denis Newman-Griffis","Half the picture: Word frequencies reveal racial differences in clinical documentation, but not their causes","https://www.medrxiv.org/content/10.1101/2022.02.10.22270631v1",NA,2022,NA,"Clinical notes are the best record of a provider’s perceptions of their patients, but their use in studying racial bias in clinical documentation has typically been limited to manual evaluation of small datasets. We investigated the use of computational methods to scale these insights to large, heterogeneous clinical text data. We found significant differences in negative emotional tone and language implying social dominance in clinical notes between Black and White patients, but identified multiple contributing factors in addition to potential provider bias, including mis-categorization of some healthcare vocabulary as emotion-related. We further found that notes for Black patients were significantly less likely to mention opioids than for White patients, potentially reflecting both inequitable access to medication and provider bias. Our analysis showed that computational tools have significant potential for studying racial bias in large clinical corpora, and identified key challenges to providing a nuanced analysis of bias in clinical documentation.","OAT - Dataset audit papers","Data Audit",399
"335","Other non-ACM","Data Audit","Alessandro Fabris, Stefano Messina, Gianmaria Silvello, Gian Antonio Susto","Algorithmic Fairness Datasets: the Story so Far","https://arxiv.org/abs/2202.01711",NA,2022,"Algorithmic Fairness · Datasets · Documentation Debt","Data-driven algorithms are studied in diverse domains to support critical decisions, directly impacting people's well-being. As a result, a growing community of researchers has been investigating the equity of existing algorithms and proposing novel ones, advancing the understanding of risks and opportunities of automated decision-making for historically disadvantaged populations. Progress in fair Machine Learning hinges on data, which can be appropriately used only if adequately documented. Unfortunately, the algorithmic fairness community suffers from a collective data documentation debt caused by a lack of information on specific resources (opacity) and scatteredness of available information (sparsity). In this work, we target data documentation debt by surveying over two hundred datasets employed in algorithmic fairness research, and producing standardized and searchable documentation for each of them. Moreover we rigorously identify the three most popular fairness datasets, namely Adult, COMPAS and German Credit, for which we compile in-depth documentation.
This unifying documentation effort supports multiple contributions. Firstly, we summarize the merits and limitations of Adult, COMPAS and German Credit, adding to and unifying recent scholarship, calling into question their suitability as general-purpose fairness benchmarks. Secondly, we document and summarize hundreds of available alternatives, annotating their domain and supported fairness tasks, along with additional properties of interest for fairness researchers. Finally, we analyze these datasets from the perspective of five important data curation topics: anonymization, consent, inclusivity, sensitive attributes, and transparency. We discuss different approaches and levels of attention to these topics, making them tangible, and distill them into a set of best practices for the curation of novel resources. ","OAT - Dataset audit papers","Data Audit",402
"336","Other non-ACM","Data Audit","Rostamzadeh et al 2022","Healthsheet: Development of a Transparency Artifact for Health","https://dl.acm.org/doi/pdf/10.1145/3531146.3533239",NA,2022,NA,"Machine learning (ML) approaches have demonstrated promising results in a wide range of healthcare applications. Data plays a crucial role in developing ML-based healthcare systems that directly affect people’s lives. Many of the ethical issues surrounding the use of ML in healthcare stem from structural inequalities underlying the way we collect, use, and handle data. Developing guidelines to improve documentation practices regarding the creation, use, and maintenance of ML healthcare datasets is therefore of critical importance. In this work, we introduce Healthsheet, a contextualized adaptation of the original datasheet questionnaire [22] for health-specific applications. Through a series of semi-structured interviews, we adapt the datasheets for healthcare data documentation. As part of the Healthsheet development process and to understand the obstacles researchers face in creating datasheets, we worked with three publicly-available healthcare datasets as our case studies, each with different types of structured data: Electronic health Records (EHR), clinical trial study data, and smartphone-based performance outcome measures. Our findings from the interviewee study and case studies show 1) that datasheets should be contextualized for healthcare, 2) that despite incentives to adopt accountability practices such as datasheets, there is a lack of consistency in the broader use of these practices 3) how the ML for health community views datasheets and particularly Healthsheets as diagnostic tool to surface the limitations and strength of datasets and 4) the relative importance of different fields in the datasheet to healthcare concerns.","OAT - Dataset audit papers","Data Audit",403
"337","Other non-ACM","Meta-Commentary","Matias, Nathan  and Lucas Wright","Impact Assessment of Human-Algorithm Feedback Loops","https://just-tech.ssrc.org/field-reviews/impact-assessment-of-human-algorithm-feedback-loops/",NA,2022,NA,"a field review that provides an introduction to the challenges of governing  adaptive algorithms, whose interactions with human behavior cannot yet  be reliably predicted or assessed. This review is a practical tool for  regulators, advocates, journalists, scientists, and engineers who are working to assess the impact of these algorithms for social justice.","OAT - Meta-Commentary","Meta-Commentary",414
"338","Other non-ACM","Meta-Commentary","Minkkinen et al ","What about investors? ESG analyses as tools for ethics-based AI auditing","https://link.springer.com/article/10.1007/s00146-022-01415-0",NA,2022,"Artificial intelligence, AI Auditing ,ESG investing, Responsible investment, Ethics","We conducted a series of expert interviews and analyzed the data using thematic analysis.... we contribute to the ethics-based AI auditing literature","OAT - Meta-Commentary","Meta-Commentary",418
"339","Other non-ACM","Meta-Commentary","Yurrita et al ","Towards a multi-stakeholder value-based assessment framework for algorithmic systems","https://dl.acm.org/doi/pdf/10.1145/3531146.3533118",NA,2022,"values, ML development and deployment pipeline, algorithm assessment, multi-stakeholder","""In an effort to regulate Machine Learning-driven (ML) systems, current auditing processes mostly focus on detecting harmful algorithmic biases. While these strategies have proven to be impactful, some values outlined in documents dealing with ethics in ML- driven systems are still underrepresented in auditing processes. [...] s opposed to some other auditing frameworks that merely rely on ML researchers’ and practitioners’ input, we argue that it is necessary to include stake-holders that present diverse standpoints to systematically negotiate and consolidate value and criteria tensions. To that end, we map stakeholders with different insight needs, and assign tailored means for communicating value manifestations to them. "" ","OAT - Meta-Commentary","Meta-Commentary",428
"340","Other non-ACM","Meta-Commentary","Fabris et al.","Tackling Documentation Debt: A Survey on Algorithmic Fairness Datasets","https://www.dei.unipd.it/~fabrisal/papers/2022_eaamo.pdf",NA,2022,"Algorithmic fairness, Data studies, Documentation debt.","A growing community of researchers has been investigating the
equity of algorithms, advancing the understanding of risks and
opportunities of automated decision-making for historically dis-
advantaged populations. Progress in fair Machine Learning (ML)
hinges on data, which can be appropriately used only if adequately
documented. Unfortunately, the research community, as a whole,
suffers from a collective data documentation debt caused by a lack
of information on specific resources (opacity) and scatteredness
of available information (sparsity). In this work, we survey over
two hundred datasets employed in algorithmic fairness research,
producing standardized and searchable documentation for each
of them. Moreover we rigorously identify the three most popular
fairness datasets, namely Adult, COMPAS, and German Credit, for
which we compile in-depth documentation. This unifying documen-
tation effort targets documentation sparsity and supports multiple
contributions. In the first part of this work, we summarize the mer-
its and limitations of Adult, COMPAS, and German Credit, adding
to and unifying recent scholarship, calling into question their suit-
ability as general-purpose fairness benchmarks. To overcome this
limitation, we document hundreds of available alternatives, anno-
tating their domain and the algorithmic fairness tasks they support,
along with additional properties of interest for fairness practition-
ers and researchers, including their format, cardinality, and the
sensitive attributes they encode. In the second part, we summarize
this information, zooming in on the domains and tasks supported
by these resources. Overall, we assemble and summarize sparse
information on hundreds of datasets into a single resource, which
we make available to the community, with the aim of tackling the
data documentation debt.","OAT - Meta-Commentary","Meta-Commentary",431
"341","Other non-ACM","Meta-Commentary","Deng et al.","Exploring How Machine Learning Practitioners (Try To) Use Fairness Toolkits","https://dl.acm.org/doi/10.1145/3531146.3533113",NA,2022,NA,"""In this paper, we conducted the first in-depth empirical exploration of how industry practitioners (try to) work with existing fairness toolkits. In particular, we conducted think-aloud interviews to understand how participants learn about and use fairness toolkits, and explored the generality of our findings through an anonymous online survey. We identified several opportunities for fairness toolkits to better address practitioner needs and scaffold them in using toolkits effectively and responsibly. Based on these findings, we highlight implications for the design of future open-source fairness toolkits that can support practitioners in better contextualizing, communicating, and collaborating around ML fairness efforts. ""","OAT - Meta-Commentary","Meta-Commentary",432
